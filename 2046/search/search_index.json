{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Splunk Connect for Syslog! \u00b6 Splunk Connect for Syslog is an open source packaged solution for getting data into Splunk. It is based on the syslog-ng Open Source Edition (Syslog-NG OSE) and transports data to Splunk via the Splunk HTTP event Collector (HEC) rather than writing events to disk for collection by a Universal Forwarder. Product Goals \u00b6 Bring a tested configuration and build of syslog-ng OSE to the market that will function consistently regardless of the underlying host\u2019s linux distribution Provide a container with the tested configuration for Docker/K8s that can be more easily deployed than upstream packages directly on a customer OS Provide validated (testable and tested) implementations of filter and parse functions for common vendor products Reduce latency and improve scale by balancing event distribution across Splunk Indexers Support \u00b6 Splunk Connect for Syslog is an open source project that is now officially supported by Splunk. That said, the notes below outlining community support are still highly relevant. Splunk Connect for Syslog is an open source product developed by Splunkers with contributions from the community of partners and customers. This unique product will be enhanced, maintained and supported by the community, led by Splunkers with deep subject matter expertise. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace. Post a question to Splunk Answers using the tag \u201cSplunk Connect For Syslog\u201d Join the #splunk-connect-for-syslog room in the splunk-usergroups Slack Workspace. If you don\u2019t yet have an account sign up Please use the GitHub issue tracker to submit bugs or request enhancements: https://github.com/splunk/splunk-connect-for-syslog/issues Get involved, try it out, ask questions, contribute filters, and make new friends! Contributing \u00b6 We welcome feedback and contributions from the community! Please see our contribution guidelines for more information on how to get involved. License \u00b6 Configuration and documentation licensed subject to CC0 Code and scripts licensed subject to BSD-2-Clause Third Party Red Hat Universal Base Image see License Third Party Syslog-NG (OSE) License","title":"Home"},{"location":"#welcome-to-splunk-connect-for-syslog","text":"Splunk Connect for Syslog is an open source packaged solution for getting data into Splunk. It is based on the syslog-ng Open Source Edition (Syslog-NG OSE) and transports data to Splunk via the Splunk HTTP event Collector (HEC) rather than writing events to disk for collection by a Universal Forwarder.","title":"Welcome to Splunk Connect for Syslog!"},{"location":"#product-goals","text":"Bring a tested configuration and build of syslog-ng OSE to the market that will function consistently regardless of the underlying host\u2019s linux distribution Provide a container with the tested configuration for Docker/K8s that can be more easily deployed than upstream packages directly on a customer OS Provide validated (testable and tested) implementations of filter and parse functions for common vendor products Reduce latency and improve scale by balancing event distribution across Splunk Indexers","title":"Product Goals"},{"location":"#support","text":"Splunk Connect for Syslog is an open source project that is now officially supported by Splunk. That said, the notes below outlining community support are still highly relevant. Splunk Connect for Syslog is an open source product developed by Splunkers with contributions from the community of partners and customers. This unique product will be enhanced, maintained and supported by the community, led by Splunkers with deep subject matter expertise. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace. Post a question to Splunk Answers using the tag \u201cSplunk Connect For Syslog\u201d Join the #splunk-connect-for-syslog room in the splunk-usergroups Slack Workspace. If you don\u2019t yet have an account sign up Please use the GitHub issue tracker to submit bugs or request enhancements: https://github.com/splunk/splunk-connect-for-syslog/issues Get involved, try it out, ask questions, contribute filters, and make new friends!","title":"Support"},{"location":"#contributing","text":"We welcome feedback and contributions from the community! Please see our contribution guidelines for more information on how to get involved.","title":"Contributing"},{"location":"#license","text":"Configuration and documentation licensed subject to CC0 Code and scripts licensed subject to BSD-2-Clause Third Party Red Hat Universal Base Image see License Third Party Syslog-NG (OSE) License","title":"License"},{"location":"CONTRIBUTING/","text":"CONTRIBUTING \u00b6 Splunk welcomes contributions from the SC4S community, and your feedback and enhancements are appreciated. There\u2019s always code that can be clarified, functionality that can be extended, and new data filters to develop, and documentation to refine. If you see something you think should be fixed or added, go for it! Data Safety \u00b6 Splunk Connect for Syslog is a community built and maintained product. Anyone with internet access can get a Splunk GitHub account and participate. As with any publicly available repository, care must be taken to never share private data via Issues, Pull Requests or any other mechanisms. Any data that is shared in the Splunk Connect for Syslog GitHub repository is made available to the entire Community without limits. Members of the Community and/or their employers (including Splunk) assume no responsibility or liability for any damages resulting from the sharing of private data via the Splunk GitHub. Any data samples shared in the Splunk GitHub repository must be free of private data. * Working locally, identify potentially sensitive field values in data samples (Public IP address, URL, Hostname, Etc.) * Replace all potentially sensitive field values with synthetic values * Manually review data samples to re-confirm they are free of private data before sharing in the Splunk GitHub Prerequisites \u00b6 When contributing to this repository, please first discuss the change you wish to make via a GitHub issue or Slack message with the owners of this repository. Setup Development Environment \u00b6 For a basic development environment docker and a bash shell is all that is required. For a more complete IDE experience see our wiki (Setup PyCharm)[https://github.com/splunk/splunk-connect-for-syslog/wiki/SC4S-Development-Setup-Using-PyCharm] Feature Requests and Bug Reports \u00b6 Have ideas on improvements or found a problem? While the community encourages everyone to contribute code, it is also appreciated when someone reports an issue. Please report any issues or bugs you find through GitHub\u2019s issue tracker. If you are reporting a bug, please include the following details: Your operating system name and version Any details about your local setup that might be helpful in troubleshooting (ex. container runtime you use, etc.) Data sample (in raw, \u201con the wire\u201d format) Detailed steps to reproduce the bug We want to hear about your enhancements as well. Feel free to submit them as issues: Explain in detail how they should work Keep the scope as narrow as possible. This will make it easier to implement Fixing Issues \u00b6 Look through our issue tracker to find problems to fix! Feel free to comment and tag community members of this project with any questions or concerns. Pull Requests \u00b6 What is a \u201cpull request\u201d? It informs the project\u2019s core developers about the changes you want to review and merge. Once you submit a pull request, it enters a stage of code review where you and others can discuss its potential modifications and even add more commits to it later on. If you want to learn more, please consult this tutorial on how pull requests work in the GitHub Help Center. Here\u2019s an overview of how you can make a pull request against this project: Fork the Splunk-connect-for-syslog GitHub repository Clone your fork using git and create a branch off develop git clone git @github . com : YOUR_GITHUB_USERNAME / splunk - connect - for - syslog . git cd splunk - connect - for - syslog This project uses \u2018develop\u2019 for all development activity, so create your branch off that git checkout -b your-bugfix-branch-name develop Run all the tests to verify your environment cd splunk - connect - for - syslog . / test - with - compose . sh Make your changes, commit and push once your tests have passed git commit -m \"\" git push Submit a pull request through the GitHub website using the changes from your forked codebase Code Review \u00b6 There are two aspects of code review: giving and receiving. To make it easier for your PR to receive reviews, consider the reviewers will need you to: Follow the project coding conventions Write good commit messages Break large changes into a logical series of smaller patches which individually make easily understandable changes, and in aggregate solve a broader issue Reviewers are highly encouraged to revisit the Code of Conduct and must go above and beyond to promote a collaborative, respectful community. When reviewing PRs from others, \u201cThe Gentle Art of Patch Review\u201d suggests an iterative series of focuses which is designed to lead new contributors to positive collaboration without inundating them initially with nuances: Is the idea behind the contribution sound? Is the contribution architected correctly? Is the contribution polished? For this project, we require that at least 2 approvals are given and a build from our continuous integration system is successful off of your branch. Please note that any new changes made with your existing pull request during review will automatically unapprove and retrigger another build/round of tests. Testing \u00b6 Testing is the responsibility of all contributors. In general, we try to adhere to TDD, writing the test first. There are multiple types of tests. The location of the test code varies with type, as do the specifics of the environment needed to successfully run the test. Review existing tests in the tests folder of the repo We could always use improvements to our documentation! Anyone can contribute to these docs - whether you\u2019re new to the project, you\u2019ve been around a long time, and whether you self-identify as a developer, an end user, or someone who just can\u2019t stand seeing typos. What exactly is needed? More complementary documentation. Have you perhaps found something unclear? More examples or generic templates that others can use. Blog posts, articles and such \u2013 they\u2019re all very much appreciated. You can also edit documentation files directly in the GitHub web interface, without creating a local copy. This can be convenient for small typos or grammar fixes. Release Notes \u00b6 To add commit messages to release notes, tag the message in following format [ TYPE ] < commit message > [TYPE] can be among the following * FEATURE * FIX * DOC * TEST * CI * REVERT * FILTERADD * FILTERMOD Sample commit : git commit - m \"[TEST] test-message\"","title":"CONTRIBUTING"},{"location":"CONTRIBUTING/#contributing","text":"Splunk welcomes contributions from the SC4S community, and your feedback and enhancements are appreciated. There\u2019s always code that can be clarified, functionality that can be extended, and new data filters to develop, and documentation to refine. If you see something you think should be fixed or added, go for it!","title":"CONTRIBUTING"},{"location":"CONTRIBUTING/#data-safety","text":"Splunk Connect for Syslog is a community built and maintained product. Anyone with internet access can get a Splunk GitHub account and participate. As with any publicly available repository, care must be taken to never share private data via Issues, Pull Requests or any other mechanisms. Any data that is shared in the Splunk Connect for Syslog GitHub repository is made available to the entire Community without limits. Members of the Community and/or their employers (including Splunk) assume no responsibility or liability for any damages resulting from the sharing of private data via the Splunk GitHub. Any data samples shared in the Splunk GitHub repository must be free of private data. * Working locally, identify potentially sensitive field values in data samples (Public IP address, URL, Hostname, Etc.) * Replace all potentially sensitive field values with synthetic values * Manually review data samples to re-confirm they are free of private data before sharing in the Splunk GitHub","title":"Data Safety"},{"location":"CONTRIBUTING/#prerequisites","text":"When contributing to this repository, please first discuss the change you wish to make via a GitHub issue or Slack message with the owners of this repository.","title":"Prerequisites"},{"location":"CONTRIBUTING/#setup-development-environment","text":"For a basic development environment docker and a bash shell is all that is required. For a more complete IDE experience see our wiki (Setup PyCharm)[https://github.com/splunk/splunk-connect-for-syslog/wiki/SC4S-Development-Setup-Using-PyCharm]","title":"Setup Development Environment"},{"location":"CONTRIBUTING/#feature-requests-and-bug-reports","text":"Have ideas on improvements or found a problem? While the community encourages everyone to contribute code, it is also appreciated when someone reports an issue. Please report any issues or bugs you find through GitHub\u2019s issue tracker. If you are reporting a bug, please include the following details: Your operating system name and version Any details about your local setup that might be helpful in troubleshooting (ex. container runtime you use, etc.) Data sample (in raw, \u201con the wire\u201d format) Detailed steps to reproduce the bug We want to hear about your enhancements as well. Feel free to submit them as issues: Explain in detail how they should work Keep the scope as narrow as possible. This will make it easier to implement","title":"Feature Requests and Bug Reports"},{"location":"CONTRIBUTING/#fixing-issues","text":"Look through our issue tracker to find problems to fix! Feel free to comment and tag community members of this project with any questions or concerns.","title":"Fixing Issues"},{"location":"CONTRIBUTING/#pull-requests","text":"What is a \u201cpull request\u201d? It informs the project\u2019s core developers about the changes you want to review and merge. Once you submit a pull request, it enters a stage of code review where you and others can discuss its potential modifications and even add more commits to it later on. If you want to learn more, please consult this tutorial on how pull requests work in the GitHub Help Center. Here\u2019s an overview of how you can make a pull request against this project: Fork the Splunk-connect-for-syslog GitHub repository Clone your fork using git and create a branch off develop git clone git @github . com : YOUR_GITHUB_USERNAME / splunk - connect - for - syslog . git cd splunk - connect - for - syslog This project uses \u2018develop\u2019 for all development activity, so create your branch off that git checkout -b your-bugfix-branch-name develop Run all the tests to verify your environment cd splunk - connect - for - syslog . / test - with - compose . sh Make your changes, commit and push once your tests have passed git commit -m \"\" git push Submit a pull request through the GitHub website using the changes from your forked codebase","title":"Pull Requests"},{"location":"CONTRIBUTING/#code-review","text":"There are two aspects of code review: giving and receiving. To make it easier for your PR to receive reviews, consider the reviewers will need you to: Follow the project coding conventions Write good commit messages Break large changes into a logical series of smaller patches which individually make easily understandable changes, and in aggregate solve a broader issue Reviewers are highly encouraged to revisit the Code of Conduct and must go above and beyond to promote a collaborative, respectful community. When reviewing PRs from others, \u201cThe Gentle Art of Patch Review\u201d suggests an iterative series of focuses which is designed to lead new contributors to positive collaboration without inundating them initially with nuances: Is the idea behind the contribution sound? Is the contribution architected correctly? Is the contribution polished? For this project, we require that at least 2 approvals are given and a build from our continuous integration system is successful off of your branch. Please note that any new changes made with your existing pull request during review will automatically unapprove and retrigger another build/round of tests.","title":"Code Review"},{"location":"CONTRIBUTING/#testing","text":"Testing is the responsibility of all contributors. In general, we try to adhere to TDD, writing the test first. There are multiple types of tests. The location of the test code varies with type, as do the specifics of the environment needed to successfully run the test. Review existing tests in the tests folder of the repo We could always use improvements to our documentation! Anyone can contribute to these docs - whether you\u2019re new to the project, you\u2019ve been around a long time, and whether you self-identify as a developer, an end user, or someone who just can\u2019t stand seeing typos. What exactly is needed? More complementary documentation. Have you perhaps found something unclear? More examples or generic templates that others can use. Blog posts, articles and such \u2013 they\u2019re all very much appreciated. You can also edit documentation files directly in the GitHub web interface, without creating a local copy. This can be convenient for small typos or grammar fixes.","title":"Testing"},{"location":"CONTRIBUTING/#release-notes","text":"To add commit messages to release notes, tag the message in following format [ TYPE ] < commit message > [TYPE] can be among the following * FEATURE * FIX * DOC * TEST * CI * REVERT * FILTERADD * FILTERMOD Sample commit : git commit - m \"[TEST] test-message\"","title":"Release Notes"},{"location":"architecture/","text":"SC4S Architectural Considerations \u00b6 There are some key architectural considerations and recommendations that will yield extremely performant and reliable syslog data collection while minimizing the \u201cover-engineering\u201d that is common in many syslog data collection designs. These recommendations are not specific to Splunk Connect for Syslog, but rather stem from the syslog protocol itself \u2013 and its age. The syslog Protocol \u00b6 The syslog protocol was designed in the mid 1980s to offer very high-speed, network-based logging for network and security devices that were (especially at the time) starved for CPU and I/O resources. For this reason, the protocol was designed for speed and efficiency at the expense of resiliency/reliability. UDP was chosen due to its ability to \u201csend and forget\u201d the events over the network without regard (or acknowledgment) of receipt. In later years, TCP was added as a transport, as well as TLS/SSL. In spite of these additions, UDP still retains favor as a syslog transport for most data centers, and for the same reasons as originally designed. Because of these tradeoffs selected by the original designers (and retained to this day), traditional methods used to provide scale and resiliency do not necessarily transfer to the syslog world. We will discuss (and reference) some of the salient points below. Collector Location \u00b6 Due to syslog being a \u201csend and forget\u201d protocol, it does not perform well when routed through substantial (and especially WAN) network infrastructure. This includes front-side load balancers. The most reliable way to collect syslog traffic is to provide for edge (not centralized) collection. Resist the urge to centrally locate any syslog server (sc4s included) and expect the UDP and (stateless) TCP traffic to \u201cmake it\u201d. Data loss will undoubtedly occur. syslog Data Collection at Scale \u00b6 In concert with attempts to centralize syslog, many admins will co-locate several syslog-ng servers for horizontal scale, and load balance to them with a front-side load balancer. For many reasons (that go beyond this short discussion) this is not a best practice. Briefly: The attempt to load balance for scale (and HA \u2013 see below) will actually cause more data loss due to normal device operations and attendant buffer loss than would be the case if a simple, robust single server (or shared-IP cluster) were used. Front-side load balancing will also cause inadequate data distribution on the upstream side, leading to data unevenness on the indexers. HA Considerations and Challenges \u00b6 In addition to scale, many opt to load balance for high availability. While a sound approach for stateful, application-level protocols such as http, it does not work well for stateless, unacknowledged syslog traffic. Again, in the attempt to design for HA, more data ends up being lost vs. more simple designs such as vMotioned VMs. With syslog, always remember that the protocol itself is lossy, and there will be data loss (think CD-quality (lossless) vs. MP3). Syslog data collection can be made, at best, \u201cMostly Available\u201d. UDP vs. TCP \u00b6 For running syslog UDP is recommended over TCP. The syslogd daemon was originally configured to use UDP for log forwarding to reduce overhead. While UDP is an unreliable protocol, it\u2019s streaming method does not require the overhead of establishing a network session. This protocol also reduces network load as the network stream with no required receipt verification or window adjustment. While TCP could seem a better choice because it uses ACKS and there should not be data loss, there are some cases when it\u2019s possible: * The TCP session is closed events published while the system is creating a new session will be lost. (Closed Window Case) * The remote side is busy and can not ack fast enough events are lost due to local buffer full * A single ack is lost by the network and the client closes the connection. (local and remote buffer lost) * The remote server restarts for any reason (local buffer lost) * The remote server restarts without closing the connection (local buffer plus timeout time lost) * The client side restarts without closing the connection Additionally as stated before it causes more overhead on the network. TCP should be used in case of the syslog event is larger than the maximum size of the UDP packet on your network typically limited to Web Proxy, DLP and IDs type sources. To decrease drawbacks of TCP you can use TLS over TCP: * The TLS can continue a session over a broken TCP reducing buffer loss conditions * The TLS will fill packets for more efficient use of wire * The TLS will compress in most cases","title":"Architectural Considerations"},{"location":"architecture/#sc4s-architectural-considerations","text":"There are some key architectural considerations and recommendations that will yield extremely performant and reliable syslog data collection while minimizing the \u201cover-engineering\u201d that is common in many syslog data collection designs. These recommendations are not specific to Splunk Connect for Syslog, but rather stem from the syslog protocol itself \u2013 and its age.","title":"SC4S Architectural Considerations"},{"location":"architecture/#the-syslog-protocol","text":"The syslog protocol was designed in the mid 1980s to offer very high-speed, network-based logging for network and security devices that were (especially at the time) starved for CPU and I/O resources. For this reason, the protocol was designed for speed and efficiency at the expense of resiliency/reliability. UDP was chosen due to its ability to \u201csend and forget\u201d the events over the network without regard (or acknowledgment) of receipt. In later years, TCP was added as a transport, as well as TLS/SSL. In spite of these additions, UDP still retains favor as a syslog transport for most data centers, and for the same reasons as originally designed. Because of these tradeoffs selected by the original designers (and retained to this day), traditional methods used to provide scale and resiliency do not necessarily transfer to the syslog world. We will discuss (and reference) some of the salient points below.","title":"The syslog Protocol"},{"location":"architecture/#collector-location","text":"Due to syslog being a \u201csend and forget\u201d protocol, it does not perform well when routed through substantial (and especially WAN) network infrastructure. This includes front-side load balancers. The most reliable way to collect syslog traffic is to provide for edge (not centralized) collection. Resist the urge to centrally locate any syslog server (sc4s included) and expect the UDP and (stateless) TCP traffic to \u201cmake it\u201d. Data loss will undoubtedly occur.","title":"Collector Location"},{"location":"architecture/#syslog-data-collection-at-scale","text":"In concert with attempts to centralize syslog, many admins will co-locate several syslog-ng servers for horizontal scale, and load balance to them with a front-side load balancer. For many reasons (that go beyond this short discussion) this is not a best practice. Briefly: The attempt to load balance for scale (and HA \u2013 see below) will actually cause more data loss due to normal device operations and attendant buffer loss than would be the case if a simple, robust single server (or shared-IP cluster) were used. Front-side load balancing will also cause inadequate data distribution on the upstream side, leading to data unevenness on the indexers.","title":"syslog Data Collection at Scale"},{"location":"architecture/#ha-considerations-and-challenges","text":"In addition to scale, many opt to load balance for high availability. While a sound approach for stateful, application-level protocols such as http, it does not work well for stateless, unacknowledged syslog traffic. Again, in the attempt to design for HA, more data ends up being lost vs. more simple designs such as vMotioned VMs. With syslog, always remember that the protocol itself is lossy, and there will be data loss (think CD-quality (lossless) vs. MP3). Syslog data collection can be made, at best, \u201cMostly Available\u201d.","title":"HA Considerations and Challenges"},{"location":"architecture/#udp-vs-tcp","text":"For running syslog UDP is recommended over TCP. The syslogd daemon was originally configured to use UDP for log forwarding to reduce overhead. While UDP is an unreliable protocol, it\u2019s streaming method does not require the overhead of establishing a network session. This protocol also reduces network load as the network stream with no required receipt verification or window adjustment. While TCP could seem a better choice because it uses ACKS and there should not be data loss, there are some cases when it\u2019s possible: * The TCP session is closed events published while the system is creating a new session will be lost. (Closed Window Case) * The remote side is busy and can not ack fast enough events are lost due to local buffer full * A single ack is lost by the network and the client closes the connection. (local and remote buffer lost) * The remote server restarts for any reason (local buffer lost) * The remote server restarts without closing the connection (local buffer plus timeout time lost) * The client side restarts without closing the connection Additionally as stated before it causes more overhead on the network. TCP should be used in case of the syslog event is larger than the maximum size of the UDP packet on your network typically limited to Web Proxy, DLP and IDs type sources. To decrease drawbacks of TCP you can use TLS over TCP: * The TLS can continue a session over a broken TCP reducing buffer loss conditions * The TLS will fill packets for more efficient use of wire * The TLS will compress in most cases","title":"UDP vs. TCP"},{"location":"configuration/","text":"SC4S Configuration Variables \u00b6 Other than device filter creation, SC4S is almost entirely controlled by environment variables. Here are the categories and variables needed to properly configure SC4S for your environment. Global Configuration \u00b6 Variable Values Description SC4S_USE_REVERSE_DNS yes or no(default) use reverse DNS to identify hosts when HOST is not valid in the syslog header SC4S_CONTAINER_HOST string variable passed to the container to identify the actual log host for container implementations NOTE: Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. NOTE: Use of the SC4S_USE_REVERSE_DNS variable can have a significant impact on performance if the reverse DNS facility (typically a caching nameserver) is not performant. If you notice events being indexed far later than their actual timestamp in the event (latency between _indextime and _time ), this is the first place to check. Configure use of external http proxy \u00b6 Warning: Many http proxies are not provisioned with application traffic in mind. Ensure adequate capacity is available to avoid data loss and or proxy outages. Note: the follow variables are lower case Variable Values Description http_proxy undefined Use libcurl format proxy string \u201chttp://username:password@proxy.server:port\u201d https_proxy undefined Use libcurl format proxy string \u201chttp://username:password@proxy.server:port\u201d Splunk HEC Destination Configuration \u00b6 Variable Values Description SC4S_DEST_SPLUNK_HEC_CIPHER_SUITE comma separated list Open SSL cipher suite list SC4S_DEST_SPLUNK_HEC_SSL_VERSION comma separated list Open SSL version list SC4S_DEST_SPLUNK_HEC_WORKERS numeric Number of destination workers (default: 10 threads). This should rarely need to be changed; consult sc4s community for advice on appropriate setting in extreme high- or low-volume environments. SC4S_DEST_SPLUNK_INDEXED_FIELDS r_unixtime,facility, severity, container, loghost, destport, fromhostip, proto none List of sc4s indexed fields that will be included with each event in Splunk (default is the entire list except \u201cnone\u201d). Two other indexed fields, sc4s_vendor_product and sc4s_syslog_format , will also appear along with the fields selected via the list and cannot be turned on or off individually. If no indexed fields are desired (including the two internal ones), set the value to the single value of \u201cnone\u201d. When setting this variable, separate multiple entries with commas and do not include extra spaces. This list maps to the following indexed fields that will appear in all Splunk events: facility: sc4s_syslog_facility severity: sc4s_syslog_severity container: sc4s_container loghost: sc4s_loghost dport: sc4s_destport fromhostip: sc4s_fromhostip proto: sc4s_proto NOTE: When using alternate HEC destinations, the destination operating parameters outlined above ( CIPHER_SUITE , SSL_VERSION , etc.) can be individually controlled per DESTID (see \u201cConfiguration of Additional Splunk HEC Destinations\u201d immediately below). For example, to set the number of workers for the alternate HEC destination d_hec_FOO to 24, set SC4S_DEST_SPLUNK_HEC_FOO_WORKERS=24 . NOTE2: Configuration files for destinations must have a .conf extension Configure additional PKI Trust Anchors \u00b6 Additional trusted (private) Certificate authorities may be trusted by appending each PEM formatted certificate to /opt/sc4s/tls/trusted.pem Configuration of timezone for legacy sources \u00b6 Legacy sources (those that remain non compliant with RFC5424) often leave the recipient to guess at the actual time zone offset. SC4S uses an advanced feature of syslog-ng to \u201cguess\u201d the correct time zone for real time sources. However, this feature requires the source (device) clock to be synchronized to within +/- 30s of the SC4S system clock. Industry accepted best practice is to set such legacy systems to GMT (sometimes inaccurately called UTC). However, this is not always possible and in such cases two additional methods are available. For a list of time zones see . Only the \u201cTZ Database name\u201d OR \u201coffset\u201d format may be used. Change Global default time zone \u00b6 This setting is used when the container cost is not set for UTC (best practice). Using this setting is often confusing and should be avoided. Set the SC4S_DEFAULT_TIMEZONE variable to a recognized \u201czone info\u201d (Region/City) time zone format such as America/New_York . Setting this value will force SC4S to use the specified timezone (and honor its associated Daylight Savings/Summer Time rules) for all events without a timezone offset in the header or message payload. SC4S Disk Buffer Configuration \u00b6 Disk buffers in SC4S are allocated per destination . Keep this in mind when using additional destinations that have disk buffering configured. By default, when alternate HEC destinations are configured as outlined above disk buffering will be configured identically to that of the main HEC destination (unless overridden individually). Important Notes Regarding Disk Buffering: \u00b6 \u201cReliable\u201d disk buffering offers little advantage over \u201cnormal\u201d disk buffering, at a significant performance penalty. For this reason, normal disk buffering is recommended. If you add destinations locally in your configuration, pay attention to the cumulative buffer requirements when allocating local disk. Disk buffer storage is configured via container volumes and is persistent between restarts of the container. Be sure to account for disk space requirements on the local sc4s host when creating the container volumes in your respective runtime environment (outlined in the \u201cgetting started\u201d runtime docs). These volumes can grow significantly if there is an extended outage to the SC4S destinations (HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. The values for the variables below represent the total sizes of the buffers for the destination. These sizes are divided by the number of workers (threads) when setting the actual syslog-ng buffer options, because the buffer options apply to each worker rather than the entire destination. Pay careful attention to this when using the \u201cBYOE\u201d version of SC4S, where direct access to the syslog-ng config files may hide this nuance. Lastly, be sure to factor in the syslog-ng data structure overhead (approx. 2x raw message size) when calculating the total buffer size needed. To determine the proper size of the disk buffer, consult the \u201cData Resilience\u201d section below. When changing the disk buffering directory, the new directory must exist. If it doesn\u2019t, then syslog-ng will fail to start. When changing the disk buffering directory, if buffering has previously occurred on that instance, a persist file may exist which will prevent syslog-ng from changing the directory. Disk Buffer Variables \u00b6 Variable Values/Default Description SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_ENABLE yes(default) or no Enable local disk buffering SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_RELIABLE yes or no(default) Enable reliable/normal disk buffering (normal recommended) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_MEMBUFSIZE bytes (10241024) Memory buffer size in bytes (used with reliable disk buffering) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_MEMBUFLENGTH messages (15000) Memory buffer size in message count (used with normal disk buffering) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_DISKBUFSIZE bytes (53687091200) Size of local disk buffer in bytes (default 50 GB) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_DIR path Location to store the disk buffer files. This variable should only be set when using BYOE; this location is fixed when using the Container. Archive File Configuration \u00b6 This feature is designed to support compliance or \u201cdiode mode\u201d archival of all messages. Instructions for mounting the appropriate local directory to use this feature are included in each \u201cgetting started\u201d runtime document. The files will be stored in a folder structure at the mount point using the pattern shown in the table below depending on the value of the SC4S_GLOBAL_ARCHIVE_MODE variable. All events for both modes are formatted using syslog-ng\u2019s EWMM template. Variable Value/Default Location/Pattern SC4S_GLOBAL_ARCHIVE_MODE compliance(default) <archive mount>/${.splunk.sourcetype}/${HOST}/$YEAR-$MONTH-$DAY-archive.log SC4S_GLOBAL_ARCHIVE_MODE diode <archive mount>/${YEAR}/${MONTH}/${DAY}/${fields.sc4s_vendor_product}_${YEAR}${MONTH}${DAY}${HOUR}${MIN}.log\" WARNING POTENTIAL OUTAGE CAUSING CONSEQUENCE Use the following variables to select global archiving or per-source archiving. C4S does not prune the files that are created; therefore the administrator must provide a means of log rotation to prune files and/or move them to an archival system to avoid exhaustion of disk space. Variable Values Description SC4S_ARCHIVE_GLOBAL yes or undefined Enable archive of all vendor_products SC4S_ARCHIVE_<VENDOR_PRODUCT> yes(default) or undefined See sources section of documentation enables selective archival Syslog Source Configuration \u00b6 Variable Values/Default Description SC4S_SOURCE_TLS_ENABLE yes or no(default) Enable TLS globally. Be sure to configure the cert as shown immediately below. SC4S_LISTEN_DEFAULT_TLS_PORT undefined or 6514 Enable a TLS listener on port 6514 SC4S_LISTEN_DEFAULT_RFC6425_PORT undefined or 5425 Enable a TLS listener on port 5425 SC4S_SOURCE_TLS_OPTIONS no-sslv2 Comma-separated list of the following options: no-sslv2, no-sslv3, no-tlsv1, no-tlsv11, no-tlsv12, none . See syslog-ng docs for the latest list and defaults SC4S_SOURCE_TLS_CIPHER_SUITE See openssl Colon-delimited list of ciphers to support, e.g. ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384 . See openssl docs for the latest list and defaults SC4S_SOURCE_TCP_MAX_CONNECTIONS 2000 Max number of TCP Connections SC4S_SOURCE_UDP_IW_USE yes or no(default) If we want to change the Initial Window Size for UDP SC4S_SOURCE_UDP_FETCH_LIMIT 1000 Number of events to fetch from server buffer at once SC4S_SOURCE_UDP_IW_SIZE 250000 Initial Window size SC4S_SOURCE_TCP_IW_SIZE 20000000 Initial Window size SC4S_SOURCE_TCP_FETCH_LIMIT 2000 Number of events to fetch from server buffer at once SC4S_SOURCE_UDP_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_TCP_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_TLS_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_RFC5426_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_RFC6587_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_RFC5425_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_LISTEN_UDP_SOCKETS 4 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss. Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_LISTEN_RFC5426_SOCKETS 1 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss. Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_LISTEN_RFC6587_SOCKETS 1 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss. Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_LISTEN_RFC5425_SOCKETS 1 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss. Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_STORE_RAWMSG undefined or \u201cno\u201d Store unprocessed \u201con the wire\u201d raw message in the RAWMSG macro for use with the \u201cfallback\u201d sourcetype. Do not set this in production; substantial memory and disk overhead will result. Use for log path/filter development only. SC4S_IPV6_ENABLE yes or no(default) enable (dual-stack)IPv6 listeners and health checks Syslog Source TLS Certificate Configuration \u00b6 Create a folder /opt/sc4s/tls if not already done as part of the \u201cgetting started\u201d process. Uncomment the appropriate mount line in the unit or yaml file (again, documented in the \u201cgetting started\u201d runtime documents). Save the server private key in PEM format with NO PASSWORD to /opt/sc4s/tls/server.key Save the server certificate in PEM format to /opt/sc4s/tls/server.pem Ensure the entry SC4S_SOURCE_TLS_ENABLE=yes exists in /opt/sc4s/env_file SC4S metadata configuration \u00b6 Log Path overrides of index or metadata \u00b6 A key aspect of SC4S is to properly set Splunk metadata prior to the data arriving in Splunk (and before any TA processing takes place). The filters will apply the proper index, source, sourcetype, host, and timestamp metadata automatically by individual data source. Proper values for this metadata, including a recommended index and output format (template), are included with all \u201cout-of-the-box\u201d log paths included with SC4S and are chosen to properly interface with the corresponding TA in Splunk. The administrator will need to ensure all recommended indexes be created to accept this data if the defaults are not changed. It will be common to override default values in many installations. To accommodate this, each log path consults an internal lookup file that maps Splunk metadata to the specific data source being processed. This file contains the defaults that are used by SC4S to set the appropriate Splunk metadata ( index , host , source , and sourcetype ) for each data source. This file is not directly available to the administrator, but a copy of the file is deposited in the local mounted directory (by default /opt/sc4s/local/context/splunk_metadata.csv.example ) for reference. It is important to note that this copy is not used directly, but is provided solely for reference. To add to the list, or to override default entries, simply create an override file without the example extension (e.g. /opt/sc4s/local/context/splunk_metadata.csv ) and modify it according to the instructions below. splunk_metadata.csv is a CSV file containing a \u201ckey\u201d that is referenced in the log path for each data source. These keys are documented in the individual source files in this section, and allow one to override Splunk metadata either in whole or part. The use of this file is best shown by example. Here is the Netscreen \u201cSourcetype and Index Configuration\u201d table from the Juniper source documentation : key sourcetype index notes juniper_netscreen netscreen:firewall netfw none Here is a line from a typical splunk_metadata.csv override file: juniper_netscreen,index,ns_index The columns in this file are key , metadata , and value . To make a change via the override file, consult the example file (or the source documentation) for the proper key when overriding an existing source and modify and/or add rows in the table, specifying one or more of the following metadata/value pairs for a given key : key which refers to the vendor and product name of the data source, using the vendor_product convention. For overrides, these keys will be listed in the example file. For new (custom) sources, be sure to choose a key that accurately reflects the vendor and product being configured, and that matches what is specified in the log path. index to specify an alternate value for index source to specify an alternate value for source host to specify an alternate value for host sourcetype to specify an alternate value for sourcetype (be very careful when changing this; only do so if an upstream TA is not being used, or a custom TA (built by you) is being used.) sc4s_template to specify an alternate value for the syslog-ng template that will be used to format the event that will be indexed by Splunk. Changing this carries the same warning as the sourcetype above; this will affect the upstream TA. The template choices are documented elsewhere in this Configuration section. In our example above, the juniper_netscreen key references a new index used for that data source called ns_index . In general, for most deployments the index should be the only change needed; other default metadata should almost never be overridden (particularly for the \u201cOut of the Box\u201d data sources). Even then, care should be taken when considering any alternates, as the defaults for SC4S were chosen with best practices in mind. NOTE: The splunk_metadata.csv file is a true override file and the entire example file should not be copied over to the override. In most cases, the override file is just one or two lines, unless an entire index category (e.g. netfw ) needs to be overridden. This is similar in concept to the \u201cdefault\u201d and \u201clocal\u201d conf file precedence in Splunk Enterprise. NOTE The splunk_metadata.csv file should always be appended with an appropriate new key and default for the index when building a custom SC4S log path, as the new key will not exist in the internal lookup (nor the example file). Care should be taken during log path design to choose appropriate index, sourcetype and template defaults so that admins are not compelled to override them. If the custom log path is later added to the list of SC4S-supported sources, this addendum can be removed. NOTE: As noted above, the splunk_metadata.csv.example file is provided for reference only and is not used directly by SC4S. However, it is an exact copy of the internal file, and can therefore change from release to release. Be sure to check the example file first to make sure the keys for any overrides map correctly to the ones in the example file. Override index or metadata based on host, ip, or subnet (compliance overrides) \u00b6 In other cases it is appropriate to provide the same overrides but based on PCI scope, geography, or other criterion rather than globally. This is accomplished by the use of a file that uniquely identifies these source exceptions via syslog-ng filters, which maps to an associated lookup of alternate indexes, sources, or other metadata. In addition, (indexed) fields can also be added to further classify the data. The conf and csv files referenced below will be populated into the /opt/sc4s/local/context directory when SC4S is run for the first time after being set up according to the \u201cgetting started\u201d runtime documents, in a similar fashion to splunk_metadata.csv . After this first-time population of the files takes place, they can be edited (and SC4S restarted) for the changes to take effect. To get started: Edit the file compliance_meta_by_source.conf to supply uniquely named filters to identify events subject to override. Edit the file compliance_meta_by_source.csv to supply appropriate field(s) and values. The three columns in the csv file are filter name , field name , and value . Filter names in the conf file must match one or more corresponding filter name rows in the csv file. The field name column obeys the following convention: .splunk.index to specify an alternate value for index .splunk.source to specify an alternate value for source .splunk.sourcetype to specify an alternate value for sourcetype (be very careful when changing this; only do so if a downstream TA is not being used, or a custom TA (built by you) is being used.) fields.fieldname where fieldname will become the name of an indexed field sent to Splunk with the supplied value This file construct is best shown by an example. Here is a sample compliance_meta_by_source.conf file: filter f_test_test { host(\"something-*\" type(glob)) or netmask(192.168.100.1/24) }; and the corresponding compliance_meta_by_source.csv file: f_test_test,.splunk.index,\"pciindex\" f_test_test,fields.compliance,\"pci\" First off, ensure that the filter name(s) in the conf file match one or more rows in the csv file. In this case, any incoming message with a hostname starting with something- or arriving from a netmask of 192.168.100.1/24 will match the f_test_test filter, and the corresponding entries in the csv file will be checked for overrides. In this case, the new index is pciindex , and an indexed field named compliance will be sent to Splunk, with it\u2019s value set to pci . To add additional overrides, simply add another filter foo_bar {}; stanza to the conf file, and add appropriate entries to the csv file that match the filter name(s) to the overrides you desire. IMPORTANT: The files above are actual syslog-ng config file snippets that get parsed directly by the underlying syslog-ng process. Take care that your syntax is correct; for more information on proper syslog-ng syntax, see the syslog-ng documentation . A syntax error will cause the runtime process to abort in the \u201cpreflight\u201d phase at startup. Finally, to update your changes for the systemd-based runtimes, restart SC4S using the commands: sudo systemctl daemon - reload sudo systemctl restart sc4s For the Docker Swarm runtime, redeploy the updated service using the command: docker stack deploy --compose-file docker-compose.yml sc4s Dropping all data by ip or subnet (deprecated) \u00b6 The use of vendor_product_by_source to null queue is deprecated and will be removed in v3. See Filtering events from output In some cases rogue or port-probing data can be sent to SC4S from misconfigured devices or vulnerability scanners. Update the vendor_product_by_source.conf filter f_null_queue with one or more ip/subnet masks to drop events without logging. Note that drop metrics will be recorded. Fixing (overriding) the host field \u00b6 In some cases the host value is not present in an event (or an IP address is in its place). For administrators who require a true hostname be attached to each event, SC4S provides an optional facility to perform a reverse IP to name lookup. If the variable SC4S_USE_REVERSE_DNS is set to \u201cyes\u201d, SC4S will first check host.csv and replace the value of host with the value specified that matches the incoming IP address. If a value is not found in host.csv then a reverse DNS lookup will be attempted against the configured nameserver. The IP address will only be used as the host value as a last resort. NOTE: Use of this variable can have a significant impact on performance if the reverse DNS facility (typically a caching nameserver) is not performant. If you notice events being indexed far later than their actual timestamp in the event (latency between _indextime and _time ), this is the first place to check. Splunk Connect for Syslog output templates (syslog-ng templates) \u00b6 Splunk Connect for Syslog utilizes the syslog-ng template mechanism to format the output payload (event) that will be sent to Splunk. These templates can format the messages in a number of ways (straight text, JSON, etc.) as well as utilize the many syslog-ng \u201cmacros\u201d (fields) to specify what gets placed in the payload that is delivered to the destination. Here is a list of the templates used in SC4S, which can be used in the metadata override section immediately above. New templates can also be added by the administrator in the \u201clocal\u201d section for local destinations; pay careful attention to the syntax as the templates are \u201clive\u201d syslog-ng config code. Template name Template contents Notes t_standard ${DATE} ${HOST} ${MSGHDR}${MESSAGE} Standard template for most RFC3164 (standard syslog) traffic t_msg_only ${MSGONLY} syslog-ng $MSG is sent, no headers (host, timestamp, etc.) t_msg_trim $(strip $MSGONLY) As above with whitespace stripped t_everything ${ISODATE} ${HOST} ${MSGHDR}${MESSAGE} Standard template with ISO date format t_hdr_msg ${MSGHDR}${MESSAGE} Useful for non-compliant syslog messages t_legacy_hdr_msg ${LEGACY_MSGHDR}${MESSAGE} Useful for non-compliant syslog messages t_hdr_sdata_msg ${MSGHDR}${MSGID} ${SDATA} ${MESSAGE} Useful for non-compliant syslog messages t_program_msg ${PROGRAM}[${PID}]: ${MESSAGE} Useful for non-compliant syslog messages t_program_nopid_msg ${PROGRAM}: ${MESSAGE} Useful for non-compliant syslog messages t_JSON_3164 $(format-json \u2013scope rfc3164 \u2013pair PRI=\u201d<$PRI>\u201d \u2013key LEGACY_MSGHDR \u2013exclude FACILITY \u2013exclude PRIORITY) JSON output of all RFC3164-based syslog-ng macros. Useful with the \u201cfallback\u201d sourcetype to aid in new filter development. t_JSON_5424 $(format-json \u2013scope rfc5424 \u2013pair PRI=\u201d<$PRI>\u201d \u2013key ISODATE \u2013exclude DATE \u2013exclude FACILITY \u2013exclude PRIORITY) JSON output of all RFC5424-based syslog-ng macros; for use with RFC5424-compliant traffic. t_JSON_5424_SDATA $(format-json \u2013scope rfc5424 \u2013pair PRI=\u201d<$PRI>\u201d \u2013key ISODATE \u2013exclude DATE \u2013exclude FACILITY \u2013exclude PRIORITY) \u2013exclude MESSAGE JSON output of all RFC5424-based syslog-ng macros except for MESSAGE; for use with RFC5424-compliant traffic. Data Resilience - Local Disk Buffer Configuration \u00b6 SC4S provides capability to minimize the number of lost events if the connection to all the Splunk Indexers goes down. This capability utilizes the disk buffering feature of Syslog-ng. SC4S receives a response from the Splunk HTTP Event Collector (HEC) when a message is received successfully. If a confirmation message from the HEC endpoint is not received (or a \u201cserver busy\u201d reply, such as a \u201c503\u201d is sent), the load balancer will try the next HEC endpoint in the pool. If all pool members are exhausted (such as would occur if there were a full network outage to the HEC endpoints), events will queue to the local disk buffer on the SC4S Linux host. SC4S will continue attempting to send the failed events while it buffers all new incoming events to disk. If the disk space allocated to disk buffering fills up then SC4S will stop accepting new events and subsequent events will be lost. Once SC4S gets confirmation that events are again being received by one or more indexers, events will then stream from the buffer using FIFO queueing. The number of events in the disk buffer will reduce as long as the incoming event volume is less than the maximum SC4S (with the disk buffer in the path) can handle. When all events have been emptied from the disk buffer, SC4S will resume streaming events directly to Splunk. For more detail on the Syslog-ng behavior the documentation can be found here: https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.22/administration-guide/55#TOPIC-1209280 SC4S has disk buffering enabled by default and it is strongly recommended that you keep it on, however this feature does have a performance cost. Without disk buffering enabled SC4S can handle up to 345K EPS (800 bytes/event avg) With \u201cNormal\u201d disk buffering enabled SC4S can handle up to 60K EPS (800 bytes/event avg) \u2013 This is still a lot of data! To guard against data loss it is important to configure the appropriate type and amount of storage for SC4S disk buffering. To estimate the storage allocation, follow these steps: Start with your estimated maximum events per second that each SC4S server will experience. Based on the maximum throughput of SC4S with disk buffering enabled, the conservative estimate for maximum events per second would be 60K (however, you should use the maximum rate in your environment for this calculation, not the max rate SC4S can handle). Next is your average estimated event size based on your data sources. It is common industry practice to estimate log events as 800 bytes on average. Then, factor in the maximum length of connectivity downtime you want disk buffering to be able to handle. This measure is very much dependent on your risk tolerance. Lastly, syslog-ng imposes significant overhead to maintain its internal data structures (primarily macros) so that the data can be properly \u201cplayed back\u201d upon network restoration. This overhead currently runs at about 1.7x above the total storage size for the raw messages themselves, and can be higher for \u201cfallback\u201d data sources due to the overlap of syslog-ng macros (data fields) containing some or all of the original message. For example, to protect against a full day of lost connectivity from SC4S to all your indexers at maximum throughput the calculation would look like the following: 60,000 EPS * 86400 seconds * 800 bytes * 1.7 = 6.4 TB of storage To configure storage allocation for the SC4S disk buffering, do the following: Edit the file /opt/sc4s/default/env_file Add the SC4S_DEST_SPLUNK_HEC_DISKBUFF_DISKBUFSIZE variable to the file and set the value to the number of bytes based on your estimation (e.g. 7050240000000 in the example above) Splunk does not recommend reducing the disk allocation below 500 GB Restart SC4S Given that in a connectivity outage to the Indexers events will be saved and read from disk until the buffer is emptied, it is ideal to use the fastest type of storage available. For this reason, NVMe storage is recommended for SC4S disk buffering. It is best to design your deployment so that the disk buffer will drain after connectivity is restored to the Splunk Indexers (while incoming data continues at the same general rate). Since \u201cyour mileage may vary\u201d with different combinations of data load, instance type, and disk subsystem performance, it is good practice to provision a box that performs twice as well as is required for your max EPS. This headroom will allow for rapid recovery after a connectivity outage. Misc options \u00b6 SC4S_LISTEN_STATUS_PORT Change the \u201cstatus\u201d port used by the internal health check process default value is 8080","title":"Configuration"},{"location":"configuration/#sc4s-configuration-variables","text":"Other than device filter creation, SC4S is almost entirely controlled by environment variables. Here are the categories and variables needed to properly configure SC4S for your environment.","title":"SC4S Configuration Variables"},{"location":"configuration/#global-configuration","text":"Variable Values Description SC4S_USE_REVERSE_DNS yes or no(default) use reverse DNS to identify hosts when HOST is not valid in the syslog header SC4S_CONTAINER_HOST string variable passed to the container to identify the actual log host for container implementations NOTE: Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. NOTE: Use of the SC4S_USE_REVERSE_DNS variable can have a significant impact on performance if the reverse DNS facility (typically a caching nameserver) is not performant. If you notice events being indexed far later than their actual timestamp in the event (latency between _indextime and _time ), this is the first place to check.","title":"Global Configuration"},{"location":"configuration/#configure-use-of-external-http-proxy","text":"Warning: Many http proxies are not provisioned with application traffic in mind. Ensure adequate capacity is available to avoid data loss and or proxy outages. Note: the follow variables are lower case Variable Values Description http_proxy undefined Use libcurl format proxy string \u201chttp://username:password@proxy.server:port\u201d https_proxy undefined Use libcurl format proxy string \u201chttp://username:password@proxy.server:port\u201d","title":"Configure use of external http proxy"},{"location":"configuration/#splunk-hec-destination-configuration","text":"Variable Values Description SC4S_DEST_SPLUNK_HEC_CIPHER_SUITE comma separated list Open SSL cipher suite list SC4S_DEST_SPLUNK_HEC_SSL_VERSION comma separated list Open SSL version list SC4S_DEST_SPLUNK_HEC_WORKERS numeric Number of destination workers (default: 10 threads). This should rarely need to be changed; consult sc4s community for advice on appropriate setting in extreme high- or low-volume environments. SC4S_DEST_SPLUNK_INDEXED_FIELDS r_unixtime,facility, severity, container, loghost, destport, fromhostip, proto none List of sc4s indexed fields that will be included with each event in Splunk (default is the entire list except \u201cnone\u201d). Two other indexed fields, sc4s_vendor_product and sc4s_syslog_format , will also appear along with the fields selected via the list and cannot be turned on or off individually. If no indexed fields are desired (including the two internal ones), set the value to the single value of \u201cnone\u201d. When setting this variable, separate multiple entries with commas and do not include extra spaces. This list maps to the following indexed fields that will appear in all Splunk events: facility: sc4s_syslog_facility severity: sc4s_syslog_severity container: sc4s_container loghost: sc4s_loghost dport: sc4s_destport fromhostip: sc4s_fromhostip proto: sc4s_proto NOTE: When using alternate HEC destinations, the destination operating parameters outlined above ( CIPHER_SUITE , SSL_VERSION , etc.) can be individually controlled per DESTID (see \u201cConfiguration of Additional Splunk HEC Destinations\u201d immediately below). For example, to set the number of workers for the alternate HEC destination d_hec_FOO to 24, set SC4S_DEST_SPLUNK_HEC_FOO_WORKERS=24 . NOTE2: Configuration files for destinations must have a .conf extension","title":"Splunk HEC Destination Configuration"},{"location":"configuration/#configure-additional-pki-trust-anchors","text":"Additional trusted (private) Certificate authorities may be trusted by appending each PEM formatted certificate to /opt/sc4s/tls/trusted.pem","title":"Configure additional PKI Trust Anchors"},{"location":"configuration/#configuration-of-timezone-for-legacy-sources","text":"Legacy sources (those that remain non compliant with RFC5424) often leave the recipient to guess at the actual time zone offset. SC4S uses an advanced feature of syslog-ng to \u201cguess\u201d the correct time zone for real time sources. However, this feature requires the source (device) clock to be synchronized to within +/- 30s of the SC4S system clock. Industry accepted best practice is to set such legacy systems to GMT (sometimes inaccurately called UTC). However, this is not always possible and in such cases two additional methods are available. For a list of time zones see . Only the \u201cTZ Database name\u201d OR \u201coffset\u201d format may be used.","title":"Configuration of timezone for legacy sources"},{"location":"configuration/#change-global-default-time-zone","text":"This setting is used when the container cost is not set for UTC (best practice). Using this setting is often confusing and should be avoided. Set the SC4S_DEFAULT_TIMEZONE variable to a recognized \u201czone info\u201d (Region/City) time zone format such as America/New_York . Setting this value will force SC4S to use the specified timezone (and honor its associated Daylight Savings/Summer Time rules) for all events without a timezone offset in the header or message payload.","title":"Change Global default time zone"},{"location":"configuration/#sc4s-disk-buffer-configuration","text":"Disk buffers in SC4S are allocated per destination . Keep this in mind when using additional destinations that have disk buffering configured. By default, when alternate HEC destinations are configured as outlined above disk buffering will be configured identically to that of the main HEC destination (unless overridden individually).","title":"SC4S Disk Buffer Configuration"},{"location":"configuration/#important-notes-regarding-disk-buffering","text":"\u201cReliable\u201d disk buffering offers little advantage over \u201cnormal\u201d disk buffering, at a significant performance penalty. For this reason, normal disk buffering is recommended. If you add destinations locally in your configuration, pay attention to the cumulative buffer requirements when allocating local disk. Disk buffer storage is configured via container volumes and is persistent between restarts of the container. Be sure to account for disk space requirements on the local sc4s host when creating the container volumes in your respective runtime environment (outlined in the \u201cgetting started\u201d runtime docs). These volumes can grow significantly if there is an extended outage to the SC4S destinations (HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. The values for the variables below represent the total sizes of the buffers for the destination. These sizes are divided by the number of workers (threads) when setting the actual syslog-ng buffer options, because the buffer options apply to each worker rather than the entire destination. Pay careful attention to this when using the \u201cBYOE\u201d version of SC4S, where direct access to the syslog-ng config files may hide this nuance. Lastly, be sure to factor in the syslog-ng data structure overhead (approx. 2x raw message size) when calculating the total buffer size needed. To determine the proper size of the disk buffer, consult the \u201cData Resilience\u201d section below. When changing the disk buffering directory, the new directory must exist. If it doesn\u2019t, then syslog-ng will fail to start. When changing the disk buffering directory, if buffering has previously occurred on that instance, a persist file may exist which will prevent syslog-ng from changing the directory.","title":"Important Notes Regarding Disk Buffering:"},{"location":"configuration/#disk-buffer-variables","text":"Variable Values/Default Description SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_ENABLE yes(default) or no Enable local disk buffering SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_RELIABLE yes or no(default) Enable reliable/normal disk buffering (normal recommended) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_MEMBUFSIZE bytes (10241024) Memory buffer size in bytes (used with reliable disk buffering) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_MEMBUFLENGTH messages (15000) Memory buffer size in message count (used with normal disk buffering) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_DISKBUFSIZE bytes (53687091200) Size of local disk buffer in bytes (default 50 GB) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_DIR path Location to store the disk buffer files. This variable should only be set when using BYOE; this location is fixed when using the Container.","title":"Disk Buffer Variables"},{"location":"configuration/#archive-file-configuration","text":"This feature is designed to support compliance or \u201cdiode mode\u201d archival of all messages. Instructions for mounting the appropriate local directory to use this feature are included in each \u201cgetting started\u201d runtime document. The files will be stored in a folder structure at the mount point using the pattern shown in the table below depending on the value of the SC4S_GLOBAL_ARCHIVE_MODE variable. All events for both modes are formatted using syslog-ng\u2019s EWMM template. Variable Value/Default Location/Pattern SC4S_GLOBAL_ARCHIVE_MODE compliance(default) <archive mount>/${.splunk.sourcetype}/${HOST}/$YEAR-$MONTH-$DAY-archive.log SC4S_GLOBAL_ARCHIVE_MODE diode <archive mount>/${YEAR}/${MONTH}/${DAY}/${fields.sc4s_vendor_product}_${YEAR}${MONTH}${DAY}${HOUR}${MIN}.log\" WARNING POTENTIAL OUTAGE CAUSING CONSEQUENCE Use the following variables to select global archiving or per-source archiving. C4S does not prune the files that are created; therefore the administrator must provide a means of log rotation to prune files and/or move them to an archival system to avoid exhaustion of disk space. Variable Values Description SC4S_ARCHIVE_GLOBAL yes or undefined Enable archive of all vendor_products SC4S_ARCHIVE_<VENDOR_PRODUCT> yes(default) or undefined See sources section of documentation enables selective archival","title":"Archive File Configuration"},{"location":"configuration/#syslog-source-configuration","text":"Variable Values/Default Description SC4S_SOURCE_TLS_ENABLE yes or no(default) Enable TLS globally. Be sure to configure the cert as shown immediately below. SC4S_LISTEN_DEFAULT_TLS_PORT undefined or 6514 Enable a TLS listener on port 6514 SC4S_LISTEN_DEFAULT_RFC6425_PORT undefined or 5425 Enable a TLS listener on port 5425 SC4S_SOURCE_TLS_OPTIONS no-sslv2 Comma-separated list of the following options: no-sslv2, no-sslv3, no-tlsv1, no-tlsv11, no-tlsv12, none . See syslog-ng docs for the latest list and defaults SC4S_SOURCE_TLS_CIPHER_SUITE See openssl Colon-delimited list of ciphers to support, e.g. ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384 . See openssl docs for the latest list and defaults SC4S_SOURCE_TCP_MAX_CONNECTIONS 2000 Max number of TCP Connections SC4S_SOURCE_UDP_IW_USE yes or no(default) If we want to change the Initial Window Size for UDP SC4S_SOURCE_UDP_FETCH_LIMIT 1000 Number of events to fetch from server buffer at once SC4S_SOURCE_UDP_IW_SIZE 250000 Initial Window size SC4S_SOURCE_TCP_IW_SIZE 20000000 Initial Window size SC4S_SOURCE_TCP_FETCH_LIMIT 2000 Number of events to fetch from server buffer at once SC4S_SOURCE_UDP_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_TCP_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_TLS_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_RFC5426_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_RFC6587_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_RFC5425_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly . SC4S_SOURCE_LISTEN_UDP_SOCKETS 4 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss. Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_LISTEN_RFC5426_SOCKETS 1 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss. Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_LISTEN_RFC6587_SOCKETS 1 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss. Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_LISTEN_RFC5425_SOCKETS 1 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss. Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_STORE_RAWMSG undefined or \u201cno\u201d Store unprocessed \u201con the wire\u201d raw message in the RAWMSG macro for use with the \u201cfallback\u201d sourcetype. Do not set this in production; substantial memory and disk overhead will result. Use for log path/filter development only. SC4S_IPV6_ENABLE yes or no(default) enable (dual-stack)IPv6 listeners and health checks","title":"Syslog Source Configuration"},{"location":"configuration/#syslog-source-tls-certificate-configuration","text":"Create a folder /opt/sc4s/tls if not already done as part of the \u201cgetting started\u201d process. Uncomment the appropriate mount line in the unit or yaml file (again, documented in the \u201cgetting started\u201d runtime documents). Save the server private key in PEM format with NO PASSWORD to /opt/sc4s/tls/server.key Save the server certificate in PEM format to /opt/sc4s/tls/server.pem Ensure the entry SC4S_SOURCE_TLS_ENABLE=yes exists in /opt/sc4s/env_file","title":"Syslog Source TLS Certificate Configuration"},{"location":"configuration/#sc4s-metadata-configuration","text":"","title":"SC4S metadata configuration"},{"location":"configuration/#log-path-overrides-of-index-or-metadata","text":"A key aspect of SC4S is to properly set Splunk metadata prior to the data arriving in Splunk (and before any TA processing takes place). The filters will apply the proper index, source, sourcetype, host, and timestamp metadata automatically by individual data source. Proper values for this metadata, including a recommended index and output format (template), are included with all \u201cout-of-the-box\u201d log paths included with SC4S and are chosen to properly interface with the corresponding TA in Splunk. The administrator will need to ensure all recommended indexes be created to accept this data if the defaults are not changed. It will be common to override default values in many installations. To accommodate this, each log path consults an internal lookup file that maps Splunk metadata to the specific data source being processed. This file contains the defaults that are used by SC4S to set the appropriate Splunk metadata ( index , host , source , and sourcetype ) for each data source. This file is not directly available to the administrator, but a copy of the file is deposited in the local mounted directory (by default /opt/sc4s/local/context/splunk_metadata.csv.example ) for reference. It is important to note that this copy is not used directly, but is provided solely for reference. To add to the list, or to override default entries, simply create an override file without the example extension (e.g. /opt/sc4s/local/context/splunk_metadata.csv ) and modify it according to the instructions below. splunk_metadata.csv is a CSV file containing a \u201ckey\u201d that is referenced in the log path for each data source. These keys are documented in the individual source files in this section, and allow one to override Splunk metadata either in whole or part. The use of this file is best shown by example. Here is the Netscreen \u201cSourcetype and Index Configuration\u201d table from the Juniper source documentation : key sourcetype index notes juniper_netscreen netscreen:firewall netfw none Here is a line from a typical splunk_metadata.csv override file: juniper_netscreen,index,ns_index The columns in this file are key , metadata , and value . To make a change via the override file, consult the example file (or the source documentation) for the proper key when overriding an existing source and modify and/or add rows in the table, specifying one or more of the following metadata/value pairs for a given key : key which refers to the vendor and product name of the data source, using the vendor_product convention. For overrides, these keys will be listed in the example file. For new (custom) sources, be sure to choose a key that accurately reflects the vendor and product being configured, and that matches what is specified in the log path. index to specify an alternate value for index source to specify an alternate value for source host to specify an alternate value for host sourcetype to specify an alternate value for sourcetype (be very careful when changing this; only do so if an upstream TA is not being used, or a custom TA (built by you) is being used.) sc4s_template to specify an alternate value for the syslog-ng template that will be used to format the event that will be indexed by Splunk. Changing this carries the same warning as the sourcetype above; this will affect the upstream TA. The template choices are documented elsewhere in this Configuration section. In our example above, the juniper_netscreen key references a new index used for that data source called ns_index . In general, for most deployments the index should be the only change needed; other default metadata should almost never be overridden (particularly for the \u201cOut of the Box\u201d data sources). Even then, care should be taken when considering any alternates, as the defaults for SC4S were chosen with best practices in mind. NOTE: The splunk_metadata.csv file is a true override file and the entire example file should not be copied over to the override. In most cases, the override file is just one or two lines, unless an entire index category (e.g. netfw ) needs to be overridden. This is similar in concept to the \u201cdefault\u201d and \u201clocal\u201d conf file precedence in Splunk Enterprise. NOTE The splunk_metadata.csv file should always be appended with an appropriate new key and default for the index when building a custom SC4S log path, as the new key will not exist in the internal lookup (nor the example file). Care should be taken during log path design to choose appropriate index, sourcetype and template defaults so that admins are not compelled to override them. If the custom log path is later added to the list of SC4S-supported sources, this addendum can be removed. NOTE: As noted above, the splunk_metadata.csv.example file is provided for reference only and is not used directly by SC4S. However, it is an exact copy of the internal file, and can therefore change from release to release. Be sure to check the example file first to make sure the keys for any overrides map correctly to the ones in the example file.","title":"Log Path overrides of index or metadata"},{"location":"configuration/#override-index-or-metadata-based-on-host-ip-or-subnet-compliance-overrides","text":"In other cases it is appropriate to provide the same overrides but based on PCI scope, geography, or other criterion rather than globally. This is accomplished by the use of a file that uniquely identifies these source exceptions via syslog-ng filters, which maps to an associated lookup of alternate indexes, sources, or other metadata. In addition, (indexed) fields can also be added to further classify the data. The conf and csv files referenced below will be populated into the /opt/sc4s/local/context directory when SC4S is run for the first time after being set up according to the \u201cgetting started\u201d runtime documents, in a similar fashion to splunk_metadata.csv . After this first-time population of the files takes place, they can be edited (and SC4S restarted) for the changes to take effect. To get started: Edit the file compliance_meta_by_source.conf to supply uniquely named filters to identify events subject to override. Edit the file compliance_meta_by_source.csv to supply appropriate field(s) and values. The three columns in the csv file are filter name , field name , and value . Filter names in the conf file must match one or more corresponding filter name rows in the csv file. The field name column obeys the following convention: .splunk.index to specify an alternate value for index .splunk.source to specify an alternate value for source .splunk.sourcetype to specify an alternate value for sourcetype (be very careful when changing this; only do so if a downstream TA is not being used, or a custom TA (built by you) is being used.) fields.fieldname where fieldname will become the name of an indexed field sent to Splunk with the supplied value This file construct is best shown by an example. Here is a sample compliance_meta_by_source.conf file: filter f_test_test { host(\"something-*\" type(glob)) or netmask(192.168.100.1/24) }; and the corresponding compliance_meta_by_source.csv file: f_test_test,.splunk.index,\"pciindex\" f_test_test,fields.compliance,\"pci\" First off, ensure that the filter name(s) in the conf file match one or more rows in the csv file. In this case, any incoming message with a hostname starting with something- or arriving from a netmask of 192.168.100.1/24 will match the f_test_test filter, and the corresponding entries in the csv file will be checked for overrides. In this case, the new index is pciindex , and an indexed field named compliance will be sent to Splunk, with it\u2019s value set to pci . To add additional overrides, simply add another filter foo_bar {}; stanza to the conf file, and add appropriate entries to the csv file that match the filter name(s) to the overrides you desire. IMPORTANT: The files above are actual syslog-ng config file snippets that get parsed directly by the underlying syslog-ng process. Take care that your syntax is correct; for more information on proper syslog-ng syntax, see the syslog-ng documentation . A syntax error will cause the runtime process to abort in the \u201cpreflight\u201d phase at startup. Finally, to update your changes for the systemd-based runtimes, restart SC4S using the commands: sudo systemctl daemon - reload sudo systemctl restart sc4s For the Docker Swarm runtime, redeploy the updated service using the command: docker stack deploy --compose-file docker-compose.yml sc4s","title":"Override index or metadata based on host, ip, or subnet (compliance overrides)"},{"location":"configuration/#dropping-all-data-by-ip-or-subnet-deprecated","text":"The use of vendor_product_by_source to null queue is deprecated and will be removed in v3. See Filtering events from output In some cases rogue or port-probing data can be sent to SC4S from misconfigured devices or vulnerability scanners. Update the vendor_product_by_source.conf filter f_null_queue with one or more ip/subnet masks to drop events without logging. Note that drop metrics will be recorded.","title":"Dropping all data by ip or subnet (deprecated)"},{"location":"configuration/#fixing-overriding-the-host-field","text":"In some cases the host value is not present in an event (or an IP address is in its place). For administrators who require a true hostname be attached to each event, SC4S provides an optional facility to perform a reverse IP to name lookup. If the variable SC4S_USE_REVERSE_DNS is set to \u201cyes\u201d, SC4S will first check host.csv and replace the value of host with the value specified that matches the incoming IP address. If a value is not found in host.csv then a reverse DNS lookup will be attempted against the configured nameserver. The IP address will only be used as the host value as a last resort. NOTE: Use of this variable can have a significant impact on performance if the reverse DNS facility (typically a caching nameserver) is not performant. If you notice events being indexed far later than their actual timestamp in the event (latency between _indextime and _time ), this is the first place to check.","title":"Fixing (overriding) the host field"},{"location":"configuration/#splunk-connect-for-syslog-output-templates-syslog-ng-templates","text":"Splunk Connect for Syslog utilizes the syslog-ng template mechanism to format the output payload (event) that will be sent to Splunk. These templates can format the messages in a number of ways (straight text, JSON, etc.) as well as utilize the many syslog-ng \u201cmacros\u201d (fields) to specify what gets placed in the payload that is delivered to the destination. Here is a list of the templates used in SC4S, which can be used in the metadata override section immediately above. New templates can also be added by the administrator in the \u201clocal\u201d section for local destinations; pay careful attention to the syntax as the templates are \u201clive\u201d syslog-ng config code. Template name Template contents Notes t_standard ${DATE} ${HOST} ${MSGHDR}${MESSAGE} Standard template for most RFC3164 (standard syslog) traffic t_msg_only ${MSGONLY} syslog-ng $MSG is sent, no headers (host, timestamp, etc.) t_msg_trim $(strip $MSGONLY) As above with whitespace stripped t_everything ${ISODATE} ${HOST} ${MSGHDR}${MESSAGE} Standard template with ISO date format t_hdr_msg ${MSGHDR}${MESSAGE} Useful for non-compliant syslog messages t_legacy_hdr_msg ${LEGACY_MSGHDR}${MESSAGE} Useful for non-compliant syslog messages t_hdr_sdata_msg ${MSGHDR}${MSGID} ${SDATA} ${MESSAGE} Useful for non-compliant syslog messages t_program_msg ${PROGRAM}[${PID}]: ${MESSAGE} Useful for non-compliant syslog messages t_program_nopid_msg ${PROGRAM}: ${MESSAGE} Useful for non-compliant syslog messages t_JSON_3164 $(format-json \u2013scope rfc3164 \u2013pair PRI=\u201d<$PRI>\u201d \u2013key LEGACY_MSGHDR \u2013exclude FACILITY \u2013exclude PRIORITY) JSON output of all RFC3164-based syslog-ng macros. Useful with the \u201cfallback\u201d sourcetype to aid in new filter development. t_JSON_5424 $(format-json \u2013scope rfc5424 \u2013pair PRI=\u201d<$PRI>\u201d \u2013key ISODATE \u2013exclude DATE \u2013exclude FACILITY \u2013exclude PRIORITY) JSON output of all RFC5424-based syslog-ng macros; for use with RFC5424-compliant traffic. t_JSON_5424_SDATA $(format-json \u2013scope rfc5424 \u2013pair PRI=\u201d<$PRI>\u201d \u2013key ISODATE \u2013exclude DATE \u2013exclude FACILITY \u2013exclude PRIORITY) \u2013exclude MESSAGE JSON output of all RFC5424-based syslog-ng macros except for MESSAGE; for use with RFC5424-compliant traffic.","title":"Splunk Connect for Syslog output templates (syslog-ng templates)"},{"location":"configuration/#data-resilience-local-disk-buffer-configuration","text":"SC4S provides capability to minimize the number of lost events if the connection to all the Splunk Indexers goes down. This capability utilizes the disk buffering feature of Syslog-ng. SC4S receives a response from the Splunk HTTP Event Collector (HEC) when a message is received successfully. If a confirmation message from the HEC endpoint is not received (or a \u201cserver busy\u201d reply, such as a \u201c503\u201d is sent), the load balancer will try the next HEC endpoint in the pool. If all pool members are exhausted (such as would occur if there were a full network outage to the HEC endpoints), events will queue to the local disk buffer on the SC4S Linux host. SC4S will continue attempting to send the failed events while it buffers all new incoming events to disk. If the disk space allocated to disk buffering fills up then SC4S will stop accepting new events and subsequent events will be lost. Once SC4S gets confirmation that events are again being received by one or more indexers, events will then stream from the buffer using FIFO queueing. The number of events in the disk buffer will reduce as long as the incoming event volume is less than the maximum SC4S (with the disk buffer in the path) can handle. When all events have been emptied from the disk buffer, SC4S will resume streaming events directly to Splunk. For more detail on the Syslog-ng behavior the documentation can be found here: https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.22/administration-guide/55#TOPIC-1209280 SC4S has disk buffering enabled by default and it is strongly recommended that you keep it on, however this feature does have a performance cost. Without disk buffering enabled SC4S can handle up to 345K EPS (800 bytes/event avg) With \u201cNormal\u201d disk buffering enabled SC4S can handle up to 60K EPS (800 bytes/event avg) \u2013 This is still a lot of data! To guard against data loss it is important to configure the appropriate type and amount of storage for SC4S disk buffering. To estimate the storage allocation, follow these steps: Start with your estimated maximum events per second that each SC4S server will experience. Based on the maximum throughput of SC4S with disk buffering enabled, the conservative estimate for maximum events per second would be 60K (however, you should use the maximum rate in your environment for this calculation, not the max rate SC4S can handle). Next is your average estimated event size based on your data sources. It is common industry practice to estimate log events as 800 bytes on average. Then, factor in the maximum length of connectivity downtime you want disk buffering to be able to handle. This measure is very much dependent on your risk tolerance. Lastly, syslog-ng imposes significant overhead to maintain its internal data structures (primarily macros) so that the data can be properly \u201cplayed back\u201d upon network restoration. This overhead currently runs at about 1.7x above the total storage size for the raw messages themselves, and can be higher for \u201cfallback\u201d data sources due to the overlap of syslog-ng macros (data fields) containing some or all of the original message. For example, to protect against a full day of lost connectivity from SC4S to all your indexers at maximum throughput the calculation would look like the following: 60,000 EPS * 86400 seconds * 800 bytes * 1.7 = 6.4 TB of storage To configure storage allocation for the SC4S disk buffering, do the following: Edit the file /opt/sc4s/default/env_file Add the SC4S_DEST_SPLUNK_HEC_DISKBUFF_DISKBUFSIZE variable to the file and set the value to the number of bytes based on your estimation (e.g. 7050240000000 in the example above) Splunk does not recommend reducing the disk allocation below 500 GB Restart SC4S Given that in a connectivity outage to the Indexers events will be saved and read from disk until the buffer is emptied, it is ideal to use the fastest type of storage available. For this reason, NVMe storage is recommended for SC4S disk buffering. It is best to design your deployment so that the disk buffer will drain after connectivity is restored to the Splunk Indexers (while incoming data continues at the same general rate). Since \u201cyour mileage may vary\u201d with different combinations of data load, instance type, and disk subsystem performance, it is good practice to provision a box that performs twice as well as is required for your max EPS. This headroom will allow for rapid recovery after a connectivity outage.","title":"Data Resilience - Local Disk Buffer Configuration"},{"location":"configuration/#misc-options","text":"SC4S_LISTEN_STATUS_PORT Change the \u201cstatus\u201d port used by the internal health check process default value is 8080","title":"Misc options"},{"location":"destinations/","text":"SC4S Destination Configuration \u00b6 Splunk Connect for Syslog can be configured to utilize any destination available in syslog-ng OSE. The configuration system provides ease of use helpers to manage configuration for the three most common destination needs, Splunk HEC, RFC5424 Syslog, and Legacy BSD Syslog. In the getting started guide you configured the Splunk HEC \u201cDEFAULT\u201d destination to receive all traffic by default. The \u201cDEFAULT\u201d destination should be configured to accept all events to ensure that at least one destination has the event to avoid data loss due to misconfiguration. The following example demonstrates configuration of a second HEC destination where only \u201cselected\u201d data will be sent. Example 1 Send all events \u00b6 #Note \"OTHER\" should be a meaningful name SC4S_DEST_SPLUNK_HEC_OTHER_URL = https://splunk:8088 SC4S_DEST_SPLUNK_HEC_OTHER_TOKEN = ${ SPLUNK_HEC_TOKEN } SC4S_DEST_SPLUNK_HEC_OTHER_TLS_VERIFY = no SC4S_DEST_SPLUNK_HEC_OTHER_MODE = GLOBAL Example 2 Send only cisco IOS Events \u00b6 #Note \"OTHER\" should be a meaningful name SC4S_DEST_SPLUNK_HEC_OTHER_URL = https://splunk:8088 SC4S_DEST_SPLUNK_HEC_OTHER_TOKEN = ${ SPLUNK_HEC_TOKEN } SC4S_DEST_SPLUNK_HEC_OTHER_TLS_VERIFY = no SC4S_DEST_SPLUNK_HEC_OTHER_MODE = SELECT SC4S_DEST_CISCO_IOS_ALTERNATES = d_fmt_hec_OTHER Example 3 Send only cisco IOS events that are not debug \u00b6 #Note \"OTHER\" should be a meaningful name SC4S_DEST_SPLUNK_HEC_OTHER_URL = https://splunk:8088 SC4S_DEST_SPLUNK_HEC_OTHER_TOKEN = ${ SPLUNK_HEC_TOKEN } SC4S_DEST_SPLUNK_HEC_OTHER_TLS_VERIFY = no SC4S_DEST_SPLUNK_HEC_OTHER_MODE = SELECT #filename: application sc4s - lp - cisco_ios_dest_fmt_other {{ source }}[ sc4s - lp - dest - select - d_fmt_hec_other ] { filter { ' CISCO_IOS ' eq \"${fields.sc4s_vendor}_${fields.sc4s_product}\" #Match any cisco event that is not like \"%ACL-7-1234\" and not message ( ' ^% [ ^ \\ - ] + -7 - ' ); }; }; Example 4 Mcafee EPO send RFC5424 events without frames to third party system \u00b6 Note in most cases when a destination requires syslog the requirement is referring to legacy BSD syslog (RFC3194) not standard syslog RFC5424 The destination name is taken from the env var each destination must have a unique name regardless of type. This value should be short and meaningful. #env_file SC4S_DEST_SYSLOG_MYSYS_HOST = 172 .17.0.1 SC4S_DEST_SYSLOG_MYSYS_PORT = 514 SC4S_DEST_SYSLOG_MYSYS_MODE = SELECT # set to #yes for ietf frames SC4S_DEST_SYSLOG_MYSYS_IETF = no #filename: /opt/sc4s/local/config/app_parsers/selectors/sc4s-lp-mcafee_epo_d_syslog_msys.conf application sc4s - lp - mcafee_epo_d_syslog_msys [ sc4s - lp - dest - select - d_syslog_msys ] { filter { ' mcafee ' eq \"${fields.sc4s_vendor}\" and ' epo ' eq \"${fields.sc4s_product}\" }; }; Example 5 Cisco ASA send to a third party SIEM \u00b6 The destination name is taken from the env var each destination must have a unique name regardless of type. This value should be short and meaningful In most cases when a third party system needs \u201csyslog\u201d the requirement is to send \u201clegacy BSD\u201d as follows This is often refereed to as RFC3194 #env_file SC4S_DEST_BSD_OLDSIEM_HOST = 172 .17.0.1 SC4S_DEST_BSD_OLDSIEM_PORT = 514 SC4S_DEST_BSD_OLDSIEM_MODE = SELECT # set to #yes for ietf frames #filename: /opt/sc4s/local/config/app_parsers/selectors/sc4s-lp-mcafee_epo_d_bsd_oldsiem.conf application sc4s - lp - mcafee_epo_d_bsd_oldsiem [ sc4s - lp - dest - select - d_bsd_oldsiem ] { filter { ' mcafee ' eq \"${fields.sc4s_vendor}\" and ' epo ' eq \"${fields.sc4s_product}\" }; }; Example 6 Mcafee EPO send RFC5424 events without frames to third party system \u00b6 The destination name is taken from the env var each destination must have a unique name regardless of type. This value should be short and meaningful #env_file SC4S_DEST_SYSLOG_MYSYS_HOST = 172 .17.0.1 SC4S_DEST_SYSLOG_MYSYS_PORT = 514 SC4S_DEST_SYSLOG_MYSYS_MODE = SELECT # set to #yes for ietf frames SC4S_DEST_SYSLOG_MYSYS_IETF = no #filename: /opt/sc4s/local/config/app_parsers/selectors/sc4s-lp-mcafee_epo_d_syslog_msys.conf application sc4s - lp - mcafee_epo_d_syslog_msys [ sc4s - lp - dest - select - d_syslog_msys ] { filter { ' cisco ' eq \"${fields.sc4s_vendor}\" and ' asa ' eq \"${fields.sc4s_product}\" }; }; Supported Simple Destination configurations \u00b6 SC4S Supports the following destination configurations via configuration. Any custom destination can be supported (defined by syslog-ng OSE) Splunk HTTP Event Collector (HEC) RFC5424 format without frames i.e. <166>1 2022-02-02T14:59:55.000+00:00 kinetic-charlie - - - - %FTD-6-430003: DeviceUUID: RFC5424 format with frames also known as RFC6587 123 <166>1 2022-02-02T14:59:55.000+00:00 kinetic-charlie - - - - %FTD-6-430003: DeviceUUID: RFC3164 (BSD format) <134>Feb 2 13:43:05.000 horse-ammonia CheckPoint[26203]: HEC Destination Configuration \u00b6 Variable Values Description SC4S_DEST_SPLUNK_HEC_<ID>_URL url URL(s) of the Splunk endpoint, can be a single URL space separated list SC4S_DEST_SPLUNK_HEC_<ID>_TOKEN string Splunk HTTP Event Collector Token SC4S_DEST_SPLUNK_HEC_<ID>_MODE string \u201cGLOBAL\u201d or \u201cSELECT\u201d SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY yes(default) or no verify HTTP(s) certificate Syslog Standard destination. \u00b6 Note: in many cases destinations incorrectly assert \u201csyslog\u201d support. IETF standards RFC5424, RFC5425, RFC6587 define the use of \u201csyslog\u201d as a network protocol. Often the actual configuration required is Legacy BSD syslog which is NOT a standard and was documented \u201chistorically\u201d in RFC3164 see BSD Destination section. Variable Values Description SC4S_DEST_SYSLOG_<ID>_HOST fqdn or ip the FQDN or IP of the target SC4S_DEST_SYSLOG_<ID>_PORT number 601 (default when framed) 514 (default when not framed) SC4S_DEST_SYSLOG_<ID>_IETF yes,no default \u201cyes\u201d use IETF Standard frames SC4S_DEST_SYSLOG_<ID>_TRANSPORT tcp,udp,tls default tcp SC4S_DEST_SYSLOG_<ID>_MODE string \u201cGLOBAL\u201d or \u201cSELECT\u201d BSD legacy destination (Non standard) \u00b6 Note: in many cases destinations incorrectly assert \u201csyslog\u201d support. IETF standards RFC5424, RFC5425, RFC6587 define the use of \u201csyslog\u201d as a network protocol. Often the actual configuration required is Legacy BSD syslog which is NOT a standard and was documented \u201chistorically\u201d in RFC3164 see BSD Destination section. Variable Values Description SC4S_DEST_BSD_<ID>_HOST fqdn or ip the FQDN or IP of the target SC4S_DEST_BSD_<ID>_PORT number default 514 SC4S_DEST_BSD_<ID>_TRANSPORT tcp,udp,tls default tcp SC4S_DEST_BSD_<ID>_MODE string \u201cGLOBAL\u201d or \u201cSELECT\u201d Configuration of Filtered Alternate Destinations (Advanced) \u00b6 Though source-specific forms of the variables configured above will limit configured alternate destinations to a specific data source, there are cases where even more granularity is desired within a specific data source (e.g. to send all Cisco ASA \u201cdebug\u201d traffic to Cisco Prime for analysis). This extra traffic may or may not be needed in Splunk. To accommodate this use case, Filtered Alternate Destinations allow a filter to be supplied to redirect a portion of a given source\u2019s traffic to a list of alternate destinations (and, optionally, to prevent matching events from being sent to Splunk). Again, these are configured through environment variables similar to the ones above: Variable Values Description SC4S_DEST_<VENDOR_PRODUCT>_ALT_FILTER syslog-ng filter Filter to determine which events are sent to alternate destination(s) SC4S_DEST_<VENDOR_PRODUCT>_FILTERED_ALTERNATES Comma or space-separated list of syslog-ng destinations Send filtered events to alternate syslog-ng destinations using the VENDOR_PRODUCT syntax, e.g. SC4S_DEST_CISCO_ASA_FILTERED_ALTERNATES NOTE: This is an advanced capability, and filters and destinations using proper syslog-ng syntax must be constructed prior to utilizing this feature. NOTE: Unlike the standard alternate destinations configured above, the regular \u201cmainline\u201d destinations (including the primary HEC destination or configured archive destination ( d_hec or d_archive )) are not included for events matching the configured alternate destination filter. If an event matches the filter, the list of filtered alternate destinations completely replaces any mainline destinations including defaults and global or source-based standard alternate destinations. Be sure to include them in the filtered destination list if desired. HINT: Since the filtered alternate destinations completely replace the mainline destinations (including HEC to Splunk), a filter that matches all traffic can be used with a destination list that does not include the standard HEC destination to effectively turn off HEC for a given data source.","title":"Destinations"},{"location":"destinations/#sc4s-destination-configuration","text":"Splunk Connect for Syslog can be configured to utilize any destination available in syslog-ng OSE. The configuration system provides ease of use helpers to manage configuration for the three most common destination needs, Splunk HEC, RFC5424 Syslog, and Legacy BSD Syslog. In the getting started guide you configured the Splunk HEC \u201cDEFAULT\u201d destination to receive all traffic by default. The \u201cDEFAULT\u201d destination should be configured to accept all events to ensure that at least one destination has the event to avoid data loss due to misconfiguration. The following example demonstrates configuration of a second HEC destination where only \u201cselected\u201d data will be sent.","title":"SC4S Destination Configuration"},{"location":"destinations/#example-1-send-all-events","text":"#Note \"OTHER\" should be a meaningful name SC4S_DEST_SPLUNK_HEC_OTHER_URL = https://splunk:8088 SC4S_DEST_SPLUNK_HEC_OTHER_TOKEN = ${ SPLUNK_HEC_TOKEN } SC4S_DEST_SPLUNK_HEC_OTHER_TLS_VERIFY = no SC4S_DEST_SPLUNK_HEC_OTHER_MODE = GLOBAL","title":"Example 1 Send all events"},{"location":"destinations/#example-2-send-only-cisco-ios-events","text":"#Note \"OTHER\" should be a meaningful name SC4S_DEST_SPLUNK_HEC_OTHER_URL = https://splunk:8088 SC4S_DEST_SPLUNK_HEC_OTHER_TOKEN = ${ SPLUNK_HEC_TOKEN } SC4S_DEST_SPLUNK_HEC_OTHER_TLS_VERIFY = no SC4S_DEST_SPLUNK_HEC_OTHER_MODE = SELECT SC4S_DEST_CISCO_IOS_ALTERNATES = d_fmt_hec_OTHER","title":"Example 2 Send only cisco IOS Events"},{"location":"destinations/#example-3-send-only-cisco-ios-events-that-are-not-debug","text":"#Note \"OTHER\" should be a meaningful name SC4S_DEST_SPLUNK_HEC_OTHER_URL = https://splunk:8088 SC4S_DEST_SPLUNK_HEC_OTHER_TOKEN = ${ SPLUNK_HEC_TOKEN } SC4S_DEST_SPLUNK_HEC_OTHER_TLS_VERIFY = no SC4S_DEST_SPLUNK_HEC_OTHER_MODE = SELECT #filename: application sc4s - lp - cisco_ios_dest_fmt_other {{ source }}[ sc4s - lp - dest - select - d_fmt_hec_other ] { filter { ' CISCO_IOS ' eq \"${fields.sc4s_vendor}_${fields.sc4s_product}\" #Match any cisco event that is not like \"%ACL-7-1234\" and not message ( ' ^% [ ^ \\ - ] + -7 - ' ); }; };","title":"Example 3 Send only cisco IOS events that are not debug"},{"location":"destinations/#example-4-mcafee-epo-send-rfc5424-events-without-frames-to-third-party-system","text":"Note in most cases when a destination requires syslog the requirement is referring to legacy BSD syslog (RFC3194) not standard syslog RFC5424 The destination name is taken from the env var each destination must have a unique name regardless of type. This value should be short and meaningful. #env_file SC4S_DEST_SYSLOG_MYSYS_HOST = 172 .17.0.1 SC4S_DEST_SYSLOG_MYSYS_PORT = 514 SC4S_DEST_SYSLOG_MYSYS_MODE = SELECT # set to #yes for ietf frames SC4S_DEST_SYSLOG_MYSYS_IETF = no #filename: /opt/sc4s/local/config/app_parsers/selectors/sc4s-lp-mcafee_epo_d_syslog_msys.conf application sc4s - lp - mcafee_epo_d_syslog_msys [ sc4s - lp - dest - select - d_syslog_msys ] { filter { ' mcafee ' eq \"${fields.sc4s_vendor}\" and ' epo ' eq \"${fields.sc4s_product}\" }; };","title":"Example 4 Mcafee EPO send RFC5424 events without frames to third party system"},{"location":"destinations/#example-5-cisco-asa-send-to-a-third-party-siem","text":"The destination name is taken from the env var each destination must have a unique name regardless of type. This value should be short and meaningful In most cases when a third party system needs \u201csyslog\u201d the requirement is to send \u201clegacy BSD\u201d as follows This is often refereed to as RFC3194 #env_file SC4S_DEST_BSD_OLDSIEM_HOST = 172 .17.0.1 SC4S_DEST_BSD_OLDSIEM_PORT = 514 SC4S_DEST_BSD_OLDSIEM_MODE = SELECT # set to #yes for ietf frames #filename: /opt/sc4s/local/config/app_parsers/selectors/sc4s-lp-mcafee_epo_d_bsd_oldsiem.conf application sc4s - lp - mcafee_epo_d_bsd_oldsiem [ sc4s - lp - dest - select - d_bsd_oldsiem ] { filter { ' mcafee ' eq \"${fields.sc4s_vendor}\" and ' epo ' eq \"${fields.sc4s_product}\" }; };","title":"Example 5 Cisco ASA send to a third party SIEM"},{"location":"destinations/#example-6-mcafee-epo-send-rfc5424-events-without-frames-to-third-party-system","text":"The destination name is taken from the env var each destination must have a unique name regardless of type. This value should be short and meaningful #env_file SC4S_DEST_SYSLOG_MYSYS_HOST = 172 .17.0.1 SC4S_DEST_SYSLOG_MYSYS_PORT = 514 SC4S_DEST_SYSLOG_MYSYS_MODE = SELECT # set to #yes for ietf frames SC4S_DEST_SYSLOG_MYSYS_IETF = no #filename: /opt/sc4s/local/config/app_parsers/selectors/sc4s-lp-mcafee_epo_d_syslog_msys.conf application sc4s - lp - mcafee_epo_d_syslog_msys [ sc4s - lp - dest - select - d_syslog_msys ] { filter { ' cisco ' eq \"${fields.sc4s_vendor}\" and ' asa ' eq \"${fields.sc4s_product}\" }; };","title":"Example 6 Mcafee EPO send RFC5424 events without frames to third party system"},{"location":"destinations/#supported-simple-destination-configurations","text":"SC4S Supports the following destination configurations via configuration. Any custom destination can be supported (defined by syslog-ng OSE) Splunk HTTP Event Collector (HEC) RFC5424 format without frames i.e. <166>1 2022-02-02T14:59:55.000+00:00 kinetic-charlie - - - - %FTD-6-430003: DeviceUUID: RFC5424 format with frames also known as RFC6587 123 <166>1 2022-02-02T14:59:55.000+00:00 kinetic-charlie - - - - %FTD-6-430003: DeviceUUID: RFC3164 (BSD format) <134>Feb 2 13:43:05.000 horse-ammonia CheckPoint[26203]:","title":"Supported Simple Destination configurations"},{"location":"destinations/#hec-destination-configuration","text":"Variable Values Description SC4S_DEST_SPLUNK_HEC_<ID>_URL url URL(s) of the Splunk endpoint, can be a single URL space separated list SC4S_DEST_SPLUNK_HEC_<ID>_TOKEN string Splunk HTTP Event Collector Token SC4S_DEST_SPLUNK_HEC_<ID>_MODE string \u201cGLOBAL\u201d or \u201cSELECT\u201d SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY yes(default) or no verify HTTP(s) certificate","title":"HEC Destination Configuration"},{"location":"destinations/#syslog-standard-destination","text":"Note: in many cases destinations incorrectly assert \u201csyslog\u201d support. IETF standards RFC5424, RFC5425, RFC6587 define the use of \u201csyslog\u201d as a network protocol. Often the actual configuration required is Legacy BSD syslog which is NOT a standard and was documented \u201chistorically\u201d in RFC3164 see BSD Destination section. Variable Values Description SC4S_DEST_SYSLOG_<ID>_HOST fqdn or ip the FQDN or IP of the target SC4S_DEST_SYSLOG_<ID>_PORT number 601 (default when framed) 514 (default when not framed) SC4S_DEST_SYSLOG_<ID>_IETF yes,no default \u201cyes\u201d use IETF Standard frames SC4S_DEST_SYSLOG_<ID>_TRANSPORT tcp,udp,tls default tcp SC4S_DEST_SYSLOG_<ID>_MODE string \u201cGLOBAL\u201d or \u201cSELECT\u201d","title":"Syslog Standard destination."},{"location":"destinations/#bsd-legacy-destination-non-standard","text":"Note: in many cases destinations incorrectly assert \u201csyslog\u201d support. IETF standards RFC5424, RFC5425, RFC6587 define the use of \u201csyslog\u201d as a network protocol. Often the actual configuration required is Legacy BSD syslog which is NOT a standard and was documented \u201chistorically\u201d in RFC3164 see BSD Destination section. Variable Values Description SC4S_DEST_BSD_<ID>_HOST fqdn or ip the FQDN or IP of the target SC4S_DEST_BSD_<ID>_PORT number default 514 SC4S_DEST_BSD_<ID>_TRANSPORT tcp,udp,tls default tcp SC4S_DEST_BSD_<ID>_MODE string \u201cGLOBAL\u201d or \u201cSELECT\u201d","title":"BSD legacy destination (Non standard)"},{"location":"destinations/#configuration-of-filtered-alternate-destinations-advanced","text":"Though source-specific forms of the variables configured above will limit configured alternate destinations to a specific data source, there are cases where even more granularity is desired within a specific data source (e.g. to send all Cisco ASA \u201cdebug\u201d traffic to Cisco Prime for analysis). This extra traffic may or may not be needed in Splunk. To accommodate this use case, Filtered Alternate Destinations allow a filter to be supplied to redirect a portion of a given source\u2019s traffic to a list of alternate destinations (and, optionally, to prevent matching events from being sent to Splunk). Again, these are configured through environment variables similar to the ones above: Variable Values Description SC4S_DEST_<VENDOR_PRODUCT>_ALT_FILTER syslog-ng filter Filter to determine which events are sent to alternate destination(s) SC4S_DEST_<VENDOR_PRODUCT>_FILTERED_ALTERNATES Comma or space-separated list of syslog-ng destinations Send filtered events to alternate syslog-ng destinations using the VENDOR_PRODUCT syntax, e.g. SC4S_DEST_CISCO_ASA_FILTERED_ALTERNATES NOTE: This is an advanced capability, and filters and destinations using proper syslog-ng syntax must be constructed prior to utilizing this feature. NOTE: Unlike the standard alternate destinations configured above, the regular \u201cmainline\u201d destinations (including the primary HEC destination or configured archive destination ( d_hec or d_archive )) are not included for events matching the configured alternate destination filter. If an event matches the filter, the list of filtered alternate destinations completely replaces any mainline destinations including defaults and global or source-based standard alternate destinations. Be sure to include them in the filtered destination list if desired. HINT: Since the filtered alternate destinations completely replace the mainline destinations (including HEC to Splunk), a filter that matches all traffic can be used with a destination list that does not include the standard HEC destination to effectively turn off HEC for a given data source.","title":"Configuration of Filtered Alternate Destinations (Advanced)"},{"location":"experiments/","text":"Current Experimental Features \u00b6 > 2.13.0 \u00b6 In env_file set SC4S_USE_NAME_CACHE=yes to enable caching last valid host string and replacing nill, null, or ipv4 with last good value. Benefit: More correct host name values in Splunk when source vendor fails to provide valid syslog message Risk: Potential disk I/O usage (space, iops) Potential reduction in throughput when a high proportion of events are incomplete. In env_file set SC4S_SOURCE_VMWARE_VSPHERE_GROUPMSG=yes To enable additional post processing to merge multiline vmware events. Recommend also enabling SC4S_USE_NAME_CACHE=yes as many events can be malformed or missing host name In env_file set SC4S_USE_VPS_CACHE=yes To enable automatic configuration of vendor_product by source where possible. This feature caches \u201cvendor\u201d and \u201cproduct\u201d fields from to use in determination of the best values for generic linux events for example without this feature the \u201cvendor product by host\u201d app parser must be configured to identify esx hosts so that esx SSHD events can be routed using the meta key vmware_vsphere_nix_syslog with this feature enabled a common event such containing \u201cprogram=vpxa\u201d will cache this value. Benefit: Less config interaction Risk: Potential disk I/O usage (space, iops) Potential reduction in throughput when a high proportion of events are incomplete. Risk: misidentification due to load balancers and relay sources. SC4S_SOURCE_PROXYCONNECT=yes for TCP and TLS connection expect \u201cPROXY CONNECT\u201d to provide the original client IP in SNAT load balancing","title":"Experiments"},{"location":"experiments/#current-experimental-features","text":"","title":"Current Experimental Features"},{"location":"experiments/#2130","text":"In env_file set SC4S_USE_NAME_CACHE=yes to enable caching last valid host string and replacing nill, null, or ipv4 with last good value. Benefit: More correct host name values in Splunk when source vendor fails to provide valid syslog message Risk: Potential disk I/O usage (space, iops) Potential reduction in throughput when a high proportion of events are incomplete. In env_file set SC4S_SOURCE_VMWARE_VSPHERE_GROUPMSG=yes To enable additional post processing to merge multiline vmware events. Recommend also enabling SC4S_USE_NAME_CACHE=yes as many events can be malformed or missing host name In env_file set SC4S_USE_VPS_CACHE=yes To enable automatic configuration of vendor_product by source where possible. This feature caches \u201cvendor\u201d and \u201cproduct\u201d fields from to use in determination of the best values for generic linux events for example without this feature the \u201cvendor product by host\u201d app parser must be configured to identify esx hosts so that esx SSHD events can be routed using the meta key vmware_vsphere_nix_syslog with this feature enabled a common event such containing \u201cprogram=vpxa\u201d will cache this value. Benefit: Less config interaction Risk: Potential disk I/O usage (space, iops) Potential reduction in throughput when a high proportion of events are incomplete. Risk: misidentification due to load balancers and relay sources. SC4S_SOURCE_PROXYCONNECT=yes for TCP and TLS connection expect \u201cPROXY CONNECT\u201d to provide the original client IP in SNAT load balancing","title":"&gt; 2.13.0"},{"location":"faq/","text":"Splunk Connect for Syslog (SC4S) Frequently Asked Questions \u00b6 Q: The Universal Forwarder/files based architecture has been the documented Splunk best practice for a long time. Why switch to a HTTP Event Collector (HEC) based architecture? A: Using HEC to stream events directly to the Indexers provides superior load balancing which has shown to produce dramatically more even data distribution across the Indexers. This even distribution results in significantly enhanced search performance. This benefit is especially valuable in large Splunk deployments. The HEC architecture designed into SC4S is also far easier to administer with newer versions of syslog-ng, which SC4S takes advantage of. There are far fewer opportunities for mis-configuration, resulting in higher overall performance and customer adoption. Lastly, HEC (and in particular, the \u201c/event\u201d endpoint) offers the opportunity for a far richer data stream to Splunk, with lower resource utilization at ingest. This rich data stream can be taken advantage of in next-generation TAs. Q: Is the Splunk HTTP Event Collector (HEC) as reliable as the Splunk Universal Forwarder? A: HEC utilizes standard HTTP mechanisms to confirm that the endpoint is responsive before sending data. The HEC architecture allows for the use of an industry standard load balancer between SC4S and the Indexer, or the included load balancing capability built into SC4S itself. Q: What if my team doesn\u2019t know how to manage containers? A: SC4S supports both container-based and \u201cbring-your-own-environment\u201d (BYOE) deployment methods. That said, using a runtime like Podman to deploy and manage SC4S containers is exceptionally easy even for those with no prior \u201ccontainer experience\u201d. Our application of container technology behaves much like a packaging system. The interaction is mostly via \u201csystemctl\u201d commands a Linux admin would use for other common administration activities. The best approach is to try it out in a lab to see what the experience is like for yourself! BYOE is intended for advanced deployments that can not use the Splunk container for some reason. One possible reason is a need to \u201cfork\u201d SC4S in order to implement heavy bespoke customization. Though many will initially gravitate toward BYOE because managing config files and syslog-ng directly is \u201cwhat they know\u201d, most enterprises will have the best experience using the container approach. Q: Can my team use SC4S if we are Windows only shop? A: You can now run Docker on Windows! Microsoft has introduced public preview technology for Linux containers on Windows. Alternatively, a minimal Centos/Ubuntu Linux VM running on Windows hyper-v is a reliable production-grade choice. Q: My company has the traditional UF/files based syslog architecture deployed and running, should I rip/replace a working installation with SC4S? A: Generally speaking, if a deployment is working and you are happy with it, it\u2019s best to leave it as is until there is need for major deployment changes such as higher scale. That said, the search performance gains realized from better data distribution is a benefit not to be overlooked. If Splunk users have complained about search performance or you are curious about the possible performance gains, we recommend doing an analysis of the data distribution across the indexers. It may make sense to upgrade to SC4S if there is a change in administration as well. Properly architecting a performant UF/files syslog-ng deployment is difficult, and an administrative personnel change offers the opportunity to \u201cmake a break\u201d to SC4S, where a new set of administrators would otherwise be tasked with understanding the existing (likely complicated) architecture. Q: What is the best way to migrate to SC4S from an existing syslog architecture? A: When exploring migration to SC4S we strongly recommend experimentation in a lab prior to deployment to production. There are a couple of approaches to consider: One option is to stand up and configure the new SC4S infrastructure for all your sources, then confirm all the sourcetypes are being indexed as expected, and finally stop the existing syslog servers. This big bang approach may result in the fewest duplicate events in Splunk vs other options. In some large or complex environments this may not be feasible however. A second option is to start with the sources currently sending events on port 514 (the default). In this case you would stand up the new SC4S infrastructure in its default configuration, confirm all the sourcetypes are being indexed as expected, then retire the old syslog servers listening on port 514. Once the 514 sources are complete you can move on to migrating any other sources one by one. To migrate these other sources you would configure SC4S filters to explicitly identify them either via unique port, hostID or CIDR block. Again, once you confirm that each sourcetype is successfully being indexed then you may disable the old syslog configurations for that source. Q: How can SC4S be deployed to provide high availability? A: It is challenging to provide HA for syslog because the syslog protocol itself was not designed with HA as a goal. See Performant AND Reliable Syslog UDP is best for an excellent overview of this topic. The gist is that the protocol itself limits the extent to which you can make any syslog collection architecture HA; at best it can be made \u201cmostly available\u201d. Think of syslog as MP3 \u2013 it is a \u201clossy\u201d protocol and there is nothing you can do to restore it to CD quality (lossless). Some have attempted to implement HA via front-side load balancers; please don\u2019t! This is the most common architectural mistake folks make when architecting large-scale syslog data collection. So \u2013 how to make it \u201cmostly available\u201d? Keep it simple, and use OS clustering (shared IP) or even just VMs with vMotion. This simple architecture will encounter far less data loss over time than more complicated schemes. Another possible option being evaluated is containerization HA schemes for SC4S (centered around microk8s) that will take some of the admin burden of clustering away \u2013 but it is still OS clustering under the hood. Q: I\u2019m worried about data loss if SC4S goes down. Could I feed syslog to redundant SC4S servers to provide HA, without creating duplicate events in Splunk? A: In many/most system design decisions there is some level of compromise. Any network protocol that doesn\u2019t have an application level ack will lose data, as speed was selected over reliability in the design, this is the case with syslog. Use of a clustered IP with an active/passive node will however offer a level of resilience while keeping complexity to a minimum. It could be possible to implement a far more complex solution utilizing an additional intermediary technology like Kafka, however the costs may outweigh the real world benefits. Q: Can the SC4S container be deployed using OpenShift or K8s? A: There are a number of reasons that OpenShift/K8s are not a good fit for syslog, SNMP or SIP. They can\u2019t use UDP and TCP on the same port which breaks multiple Bluecoat and Cisco feeds among others. Layered networking shrinks the maximum UDP message which causes data loss due to truncation and drops Long lived TCP connections cause well known problems OpenShift doesn\u2019t actually use Podman, it uses a library to wrap OCI that Podman also uses. this wrapper around the wrapper has some shortcomings that prevent the service definitions SC4S requires. Basically, K8s was built for a very different set of problems than syslog Q: If the XL reference HW can handle just under 1 TB/day how can SC4S be scaled to handle large deployments of many TB/day? A: SC4S is a distributed architecture. SC4S instances should be deployed in the same VLAN as the source devices. This means that each SC4S instance will only see a subset of the total syslog traffic in a large deployment. Even in a 100+ TB deployment the individual SC4S instances will see loads in GB/day not TB/day. Q: How are security vulnerabilities handled with SC4S? A: SC4S is comprised of several components including RHL, Syslog-ng and temporized configurations. If a vulnerability is found in the SC4S configurations, they will be given a critical priority in the Development queue. If vulnerabilities are identified in the third party components (RHL, Syslog-ng, etc.) the fixed versions will be pulled in upon the next SC4S release. Fixed security issues are identified by \u201c[security]\u201d in SC4S release notes. Q: SC4S is being blocked by fapolicyd , how do I fix that? Create a rule that allows running sc4s in fapolicyd configuration: * Create the file /etc/fapolicyd/rules.d/15-sc4s.rules . * Put this into the file: allow perm=open exe=/ : dir=/usr/lib64/ all trust=1 . * Run fagenrules --load to load the new rule. * Run systemctl restart fapolicyd to restart the process. * Start sc4s systemctl start sc4s and verify there are no errors systemctl status sc4s. Q: I am facing a unique issue that my postfilter configuration is not working although i don\u2019t have any postfilter for the mentioned source? A: There is a possibility that there is OOB postfilter for the source which will be applied , the same can be validated by checking the value of sc4s_tags in splunk UI, to fix this Please use a new topic called [sc4s-finalfilter] please don\u2019t use it in any other case as it can add the cost of the processing of data Q: Where the config for the vendors should be placed? There are folders of app-parsers and its directories. Which one to use? Does this also mean that csv files for metadata are no longer required? A: It should be placed inside /opt/sc4s/local/config/*/.conf . Most of the folders are placeholder and it will work in any of these folders if it has .conf extension. It is required but it should be placed in local/context/*.csv . Using splunk_metadata.csv is good for metadata override but it is recommended to use .conf file for everything else in place of other csv files. Q: Can we have a file using which we can create all default indexes in one go? A: Refer this file which contains all indexes being created in one go. Also, above file has lastChanceIndex configured, please use it only if it fits your requirement. If not, then please discard the use of lastChanceIndex. For more information on this file, please refer Splunk docs .","title":"SC4S FAQ"},{"location":"faq/#splunk-connect-for-syslog-sc4s-frequently-asked-questions","text":"Q: The Universal Forwarder/files based architecture has been the documented Splunk best practice for a long time. Why switch to a HTTP Event Collector (HEC) based architecture? A: Using HEC to stream events directly to the Indexers provides superior load balancing which has shown to produce dramatically more even data distribution across the Indexers. This even distribution results in significantly enhanced search performance. This benefit is especially valuable in large Splunk deployments. The HEC architecture designed into SC4S is also far easier to administer with newer versions of syslog-ng, which SC4S takes advantage of. There are far fewer opportunities for mis-configuration, resulting in higher overall performance and customer adoption. Lastly, HEC (and in particular, the \u201c/event\u201d endpoint) offers the opportunity for a far richer data stream to Splunk, with lower resource utilization at ingest. This rich data stream can be taken advantage of in next-generation TAs. Q: Is the Splunk HTTP Event Collector (HEC) as reliable as the Splunk Universal Forwarder? A: HEC utilizes standard HTTP mechanisms to confirm that the endpoint is responsive before sending data. The HEC architecture allows for the use of an industry standard load balancer between SC4S and the Indexer, or the included load balancing capability built into SC4S itself. Q: What if my team doesn\u2019t know how to manage containers? A: SC4S supports both container-based and \u201cbring-your-own-environment\u201d (BYOE) deployment methods. That said, using a runtime like Podman to deploy and manage SC4S containers is exceptionally easy even for those with no prior \u201ccontainer experience\u201d. Our application of container technology behaves much like a packaging system. The interaction is mostly via \u201csystemctl\u201d commands a Linux admin would use for other common administration activities. The best approach is to try it out in a lab to see what the experience is like for yourself! BYOE is intended for advanced deployments that can not use the Splunk container for some reason. One possible reason is a need to \u201cfork\u201d SC4S in order to implement heavy bespoke customization. Though many will initially gravitate toward BYOE because managing config files and syslog-ng directly is \u201cwhat they know\u201d, most enterprises will have the best experience using the container approach. Q: Can my team use SC4S if we are Windows only shop? A: You can now run Docker on Windows! Microsoft has introduced public preview technology for Linux containers on Windows. Alternatively, a minimal Centos/Ubuntu Linux VM running on Windows hyper-v is a reliable production-grade choice. Q: My company has the traditional UF/files based syslog architecture deployed and running, should I rip/replace a working installation with SC4S? A: Generally speaking, if a deployment is working and you are happy with it, it\u2019s best to leave it as is until there is need for major deployment changes such as higher scale. That said, the search performance gains realized from better data distribution is a benefit not to be overlooked. If Splunk users have complained about search performance or you are curious about the possible performance gains, we recommend doing an analysis of the data distribution across the indexers. It may make sense to upgrade to SC4S if there is a change in administration as well. Properly architecting a performant UF/files syslog-ng deployment is difficult, and an administrative personnel change offers the opportunity to \u201cmake a break\u201d to SC4S, where a new set of administrators would otherwise be tasked with understanding the existing (likely complicated) architecture. Q: What is the best way to migrate to SC4S from an existing syslog architecture? A: When exploring migration to SC4S we strongly recommend experimentation in a lab prior to deployment to production. There are a couple of approaches to consider: One option is to stand up and configure the new SC4S infrastructure for all your sources, then confirm all the sourcetypes are being indexed as expected, and finally stop the existing syslog servers. This big bang approach may result in the fewest duplicate events in Splunk vs other options. In some large or complex environments this may not be feasible however. A second option is to start with the sources currently sending events on port 514 (the default). In this case you would stand up the new SC4S infrastructure in its default configuration, confirm all the sourcetypes are being indexed as expected, then retire the old syslog servers listening on port 514. Once the 514 sources are complete you can move on to migrating any other sources one by one. To migrate these other sources you would configure SC4S filters to explicitly identify them either via unique port, hostID or CIDR block. Again, once you confirm that each sourcetype is successfully being indexed then you may disable the old syslog configurations for that source. Q: How can SC4S be deployed to provide high availability? A: It is challenging to provide HA for syslog because the syslog protocol itself was not designed with HA as a goal. See Performant AND Reliable Syslog UDP is best for an excellent overview of this topic. The gist is that the protocol itself limits the extent to which you can make any syslog collection architecture HA; at best it can be made \u201cmostly available\u201d. Think of syslog as MP3 \u2013 it is a \u201clossy\u201d protocol and there is nothing you can do to restore it to CD quality (lossless). Some have attempted to implement HA via front-side load balancers; please don\u2019t! This is the most common architectural mistake folks make when architecting large-scale syslog data collection. So \u2013 how to make it \u201cmostly available\u201d? Keep it simple, and use OS clustering (shared IP) or even just VMs with vMotion. This simple architecture will encounter far less data loss over time than more complicated schemes. Another possible option being evaluated is containerization HA schemes for SC4S (centered around microk8s) that will take some of the admin burden of clustering away \u2013 but it is still OS clustering under the hood. Q: I\u2019m worried about data loss if SC4S goes down. Could I feed syslog to redundant SC4S servers to provide HA, without creating duplicate events in Splunk? A: In many/most system design decisions there is some level of compromise. Any network protocol that doesn\u2019t have an application level ack will lose data, as speed was selected over reliability in the design, this is the case with syslog. Use of a clustered IP with an active/passive node will however offer a level of resilience while keeping complexity to a minimum. It could be possible to implement a far more complex solution utilizing an additional intermediary technology like Kafka, however the costs may outweigh the real world benefits. Q: Can the SC4S container be deployed using OpenShift or K8s? A: There are a number of reasons that OpenShift/K8s are not a good fit for syslog, SNMP or SIP. They can\u2019t use UDP and TCP on the same port which breaks multiple Bluecoat and Cisco feeds among others. Layered networking shrinks the maximum UDP message which causes data loss due to truncation and drops Long lived TCP connections cause well known problems OpenShift doesn\u2019t actually use Podman, it uses a library to wrap OCI that Podman also uses. this wrapper around the wrapper has some shortcomings that prevent the service definitions SC4S requires. Basically, K8s was built for a very different set of problems than syslog Q: If the XL reference HW can handle just under 1 TB/day how can SC4S be scaled to handle large deployments of many TB/day? A: SC4S is a distributed architecture. SC4S instances should be deployed in the same VLAN as the source devices. This means that each SC4S instance will only see a subset of the total syslog traffic in a large deployment. Even in a 100+ TB deployment the individual SC4S instances will see loads in GB/day not TB/day. Q: How are security vulnerabilities handled with SC4S? A: SC4S is comprised of several components including RHL, Syslog-ng and temporized configurations. If a vulnerability is found in the SC4S configurations, they will be given a critical priority in the Development queue. If vulnerabilities are identified in the third party components (RHL, Syslog-ng, etc.) the fixed versions will be pulled in upon the next SC4S release. Fixed security issues are identified by \u201c[security]\u201d in SC4S release notes. Q: SC4S is being blocked by fapolicyd , how do I fix that? Create a rule that allows running sc4s in fapolicyd configuration: * Create the file /etc/fapolicyd/rules.d/15-sc4s.rules . * Put this into the file: allow perm=open exe=/ : dir=/usr/lib64/ all trust=1 . * Run fagenrules --load to load the new rule. * Run systemctl restart fapolicyd to restart the process. * Start sc4s systemctl start sc4s and verify there are no errors systemctl status sc4s. Q: I am facing a unique issue that my postfilter configuration is not working although i don\u2019t have any postfilter for the mentioned source? A: There is a possibility that there is OOB postfilter for the source which will be applied , the same can be validated by checking the value of sc4s_tags in splunk UI, to fix this Please use a new topic called [sc4s-finalfilter] please don\u2019t use it in any other case as it can add the cost of the processing of data Q: Where the config for the vendors should be placed? There are folders of app-parsers and its directories. Which one to use? Does this also mean that csv files for metadata are no longer required? A: It should be placed inside /opt/sc4s/local/config/*/.conf . Most of the folders are placeholder and it will work in any of these folders if it has .conf extension. It is required but it should be placed in local/context/*.csv . Using splunk_metadata.csv is good for metadata override but it is recommended to use .conf file for everything else in place of other csv files. Q: Can we have a file using which we can create all default indexes in one go? A: Refer this file which contains all indexes being created in one go. Also, above file has lastChanceIndex configured, please use it only if it fits your requirement. If not, then please discard the use of lastChanceIndex. For more information on this file, please refer Splunk docs .","title":"Splunk Connect for Syslog (SC4S) Frequently Asked Questions"},{"location":"lb/","text":"A word about load balancers \u00b6 Customers often \u201crequire\u201d the use of load balancers incorrectly attempting to meet a business requirement for availability. In general load balancers are not recommended with the exception of a narrow use case where the Syslog Server must be exposed to untrusted clients on the internet such as Palo Alto Cortex. Considerations \u00b6 UDP MUST only pass a load balancer using DNAT. Source IP must be preserved. Note in this configuration a Load Balancer becomes a new single point of failure TCP/TLS May use a DNAT configuration OR SNAT with \u201cPROXY\u201d Protocol enabled SC4S_SOURCE_PROXYCONNECT=yes (Experimental) TCP/TLS load balancers do not consider the weight of individual connection load is frequently biased to one instance; all members in a single resource pool should be vertically scaled to accommodate the full workload. Alternatives \u00b6 The best deployment model for high availability is a Microk8s based deployment with MetalLB in BGP mode. This model uses a special class of load balancer that is implemented as destination network translation.","title":"Load Balancers"},{"location":"lb/#a-word-about-load-balancers","text":"Customers often \u201crequire\u201d the use of load balancers incorrectly attempting to meet a business requirement for availability. In general load balancers are not recommended with the exception of a narrow use case where the Syslog Server must be exposed to untrusted clients on the internet such as Palo Alto Cortex.","title":"A word about load balancers"},{"location":"lb/#considerations","text":"UDP MUST only pass a load balancer using DNAT. Source IP must be preserved. Note in this configuration a Load Balancer becomes a new single point of failure TCP/TLS May use a DNAT configuration OR SNAT with \u201cPROXY\u201d Protocol enabled SC4S_SOURCE_PROXYCONNECT=yes (Experimental) TCP/TLS load balancers do not consider the weight of individual connection load is frequently biased to one instance; all members in a single resource pool should be vertically scaled to accommodate the full workload.","title":"Considerations"},{"location":"lb/#alternatives","text":"The best deployment model for high availability is a Microk8s based deployment with MetalLB in BGP mode. This model uses a special class of load balancer that is implemented as destination network translation.","title":"Alternatives"},{"location":"performance/","text":"Performance and Sizing \u00b6 Performance testing against our lab configuration produces the following results and limitations. Tested Configurations \u00b6 Splunk Cloud Noah \u00b6 Environment \u00b6 Loggen (syslog-ng 3.25.1) - m5zn.3xlarge SC4S(2.30.0) + podman (4.0.2) - m5zn family SC4S_DEST_SPLUNK_HEC_WORKERS=10 (default) Splunk Cloud Noah 8.2.2203.2 - 3SH + 3IDX /opt/syslog-ng/bin/loggen -i --rate = 100000 --interval = 1800 -P -F --sdata = \"[test name=\\\"stress17\\\"]\" -s 800 --active-connections = 10 <local_hostmane> <sc4s_external_tcp514_port> Result \u00b6 SC4S instance root networking slirp4netns networking m5zn.large average rate = 21109.66 msg/sec, count=38023708, time=1801.25, (average) msg size=800, bandwidth=16491.92 kB/sec average rate = 20738.39 msg/sec, count=37344765, time=1800.75, (average) msg size=800, bandwidth=16201.87 kB/sec m5zn.xlarge average rate = 34820.94 msg/sec, count=62687563, time=1800.28, (average) msg size=800, bandwidth=27203.86 kB/sec average rate = 35329.28 msg/sec, count=63619825, time=1800.77, (average) msg size=800, bandwidth=27601.00 kB/sec m5zn.2xlarge average rate = 71929.91 msg/sec, count=129492418, time=1800.26, (average) msg size=800, bandwidth=56195.24 kB/sec average rate = 70894.84 msg/sec, count=127630166, time=1800.27, (average) msg size=800, bandwidth=55386.60 kB/sec m5zn.2xlarge average rate = 85419.09 msg/sec, count=153778825, time=1800.29, (average) msg size=800, bandwidth=66733.66 kB/sec average rate = 84733.71 msg/sec, count=152542466, time=1800.26, (average) msg size=800, bandwidth=66198.21 kB/sec Splunk Enterprise \u00b6 Environment \u00b6 Loggen (syslog-ng 3.25.1) - m5zn.large SC4S(2.30.0) + podman (4.0.2) - m5zn family SC4S_DEST_SPLUNK_HEC_WORKERS=10 (default) Splunk Enterprise 9.0.0 Standalone /opt/syslog-ng/bin/loggen -i --rate = 100000 --interval = 600 -P -F --sdata = \"[test name=\\\"stress17\\\"]\" -s 800 --active-connections = 10 <local_hostmane> <sc4s_external_tcp514_port> Result \u00b6 SC4S instance root networking slirp4netns networking m5zn.large average rate = 21511.69 msg/sec, count=12930565, time=601.095, (average) msg size=800, bandwidth=16806.01 kB/sec average rate = 21583.13 msg/sec, count=12973491, time=601.094, (average) msg size=800, bandwidth=16861.82 kB/sec average rate = 20738.39 msg/sec, count=37344765, time=1800.75, (average) msg size=800, bandwidth=16201.87 kB/sec m5zn.xlarge average rate = 37514.29 msg/sec, count=22530855, time=600.594, (average) msg size=800, bandwidth=29308.04 kB/sec average rate = 37549.86 msg/sec, count=22552210, time=600.594, (average) msg size=800, bandwidth=29335.83 kB/sec average rate = 35329.28 msg/sec, count=63619825, time=1800.77, (average) msg size=800, bandwidth=27601.00 kB/sec m5zn.2xlarge average rate = 98580.10 msg/sec, count=59157495, time=600.096, (average) msg size=800, bandwidth=77015.70 kB/sec average rate = 99463.10 msg/sec, count=59687310, time=600.095, (average) msg size=800, bandwidth=77705.55 kB/sec average rate = 84733.71 msg/sec, count=152542466, time=1800.26, (average) msg size=800, bandwidth=66198.21 kB/sec Guidance on sizing hardware \u00b6 Though vCPU (hyper threading) was used, syslog processing is a CPU intensive task and oversubscription (sharing) of resources is not advised The size of the instance must be larger than the absolute peek to prevent data loss; most sources can not buffer during times of congestion CPU Speed is critical; slower or faster CPUs will impact throughput Not all sources are equal in resource utilization. Well-formed \u201clegacy BSD\u201d syslog messages were used in this test, but many sources are not syslog compliant and will require additional resources to process.","title":"Performance"},{"location":"performance/#performance-and-sizing","text":"Performance testing against our lab configuration produces the following results and limitations.","title":"Performance and Sizing"},{"location":"performance/#tested-configurations","text":"","title":"Tested Configurations"},{"location":"performance/#splunk-cloud-noah","text":"","title":"Splunk Cloud Noah"},{"location":"performance/#environment","text":"Loggen (syslog-ng 3.25.1) - m5zn.3xlarge SC4S(2.30.0) + podman (4.0.2) - m5zn family SC4S_DEST_SPLUNK_HEC_WORKERS=10 (default) Splunk Cloud Noah 8.2.2203.2 - 3SH + 3IDX /opt/syslog-ng/bin/loggen -i --rate = 100000 --interval = 1800 -P -F --sdata = \"[test name=\\\"stress17\\\"]\" -s 800 --active-connections = 10 <local_hostmane> <sc4s_external_tcp514_port>","title":"Environment"},{"location":"performance/#result","text":"SC4S instance root networking slirp4netns networking m5zn.large average rate = 21109.66 msg/sec, count=38023708, time=1801.25, (average) msg size=800, bandwidth=16491.92 kB/sec average rate = 20738.39 msg/sec, count=37344765, time=1800.75, (average) msg size=800, bandwidth=16201.87 kB/sec m5zn.xlarge average rate = 34820.94 msg/sec, count=62687563, time=1800.28, (average) msg size=800, bandwidth=27203.86 kB/sec average rate = 35329.28 msg/sec, count=63619825, time=1800.77, (average) msg size=800, bandwidth=27601.00 kB/sec m5zn.2xlarge average rate = 71929.91 msg/sec, count=129492418, time=1800.26, (average) msg size=800, bandwidth=56195.24 kB/sec average rate = 70894.84 msg/sec, count=127630166, time=1800.27, (average) msg size=800, bandwidth=55386.60 kB/sec m5zn.2xlarge average rate = 85419.09 msg/sec, count=153778825, time=1800.29, (average) msg size=800, bandwidth=66733.66 kB/sec average rate = 84733.71 msg/sec, count=152542466, time=1800.26, (average) msg size=800, bandwidth=66198.21 kB/sec","title":"Result"},{"location":"performance/#splunk-enterprise","text":"","title":"Splunk Enterprise"},{"location":"performance/#environment_1","text":"Loggen (syslog-ng 3.25.1) - m5zn.large SC4S(2.30.0) + podman (4.0.2) - m5zn family SC4S_DEST_SPLUNK_HEC_WORKERS=10 (default) Splunk Enterprise 9.0.0 Standalone /opt/syslog-ng/bin/loggen -i --rate = 100000 --interval = 600 -P -F --sdata = \"[test name=\\\"stress17\\\"]\" -s 800 --active-connections = 10 <local_hostmane> <sc4s_external_tcp514_port>","title":"Environment"},{"location":"performance/#result_1","text":"SC4S instance root networking slirp4netns networking m5zn.large average rate = 21511.69 msg/sec, count=12930565, time=601.095, (average) msg size=800, bandwidth=16806.01 kB/sec average rate = 21583.13 msg/sec, count=12973491, time=601.094, (average) msg size=800, bandwidth=16861.82 kB/sec average rate = 20738.39 msg/sec, count=37344765, time=1800.75, (average) msg size=800, bandwidth=16201.87 kB/sec m5zn.xlarge average rate = 37514.29 msg/sec, count=22530855, time=600.594, (average) msg size=800, bandwidth=29308.04 kB/sec average rate = 37549.86 msg/sec, count=22552210, time=600.594, (average) msg size=800, bandwidth=29335.83 kB/sec average rate = 35329.28 msg/sec, count=63619825, time=1800.77, (average) msg size=800, bandwidth=27601.00 kB/sec m5zn.2xlarge average rate = 98580.10 msg/sec, count=59157495, time=600.096, (average) msg size=800, bandwidth=77015.70 kB/sec average rate = 99463.10 msg/sec, count=59687310, time=600.095, (average) msg size=800, bandwidth=77705.55 kB/sec average rate = 84733.71 msg/sec, count=152542466, time=1800.26, (average) msg size=800, bandwidth=66198.21 kB/sec","title":"Result"},{"location":"performance/#guidance-on-sizing-hardware","text":"Though vCPU (hyper threading) was used, syslog processing is a CPU intensive task and oversubscription (sharing) of resources is not advised The size of the instance must be larger than the absolute peek to prevent data loss; most sources can not buffer during times of congestion CPU Speed is critical; slower or faster CPUs will impact throughput Not all sources are equal in resource utilization. Well-formed \u201clegacy BSD\u201d syslog messages were used in this test, but many sources are not syslog compliant and will require additional resources to process.","title":"Guidance on sizing hardware"},{"location":"upgrade/","text":"Upgrading Splunk Connect for Syslog \u00b6 Splunk Connect for Syslog is updated regularly using a CI/CD development process. The notes below outline significant changes that must be taken into account prior and after an upgrade. Ensure to follow specific instructions below to ensure a smooth transition to a new version of SC4S in production. Upgrade process \u00b6 Check the current version of SC4S by running sudo <docker or podman> logs SC4S . For the latest version, use the latest tag for the SC4S image in the sc4s.service unit file: [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2\" Restart the service sudo systemctl restart sc4s Using the \u201c1\u201d version is recommended, but a specific version can be specified in the unit file if desired: [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2.91.0\" See the release information for more detail. Upgrade Notes \u00b6 Need up migrating legacy \u201clog paths\u201d or v1 app-parsers for v2. Open an issue with the original config attached and a compressed pcap of sample data for testing and we will evaluate inclusion of the source in an upcoming release. Upgrade from <2.23.0 \u00b6 Vmware vsphere fix esx and vcenter sourcetype for TA compatibility Upgrade from <2 \u00b6 Before upgrading to 2.x review sc4s.service and manually update differences compared to current doc EXPERIMENTAL SNMP Trap feature has been removed migrate to Splunk Connect for SNMP Legacy \u201cgomplate\u201d log path template support was deprecated in 1.x and has been removed in 2.x log paths must be migrated to app-parser style config prior to upgrade Check env_file for \u201cMICROFOCUS_ARCSIGHT\u201d variables and replace with CEF variables see source doc Remove old style \u201cCISCO_*_LEGACY\u201d from env_file and replace per docs New images will no longer be published to docker.io please review current getting started docs and update the sc4s.service file accordingly Internal metrics will now use \u201cmulti\u201d format by default if using unsupported versions of Splunk <8.1 see configuration doc to revert to \u201cevent\u201d or \u201csingle\u201d format. Internal metrics will now use the _metrics index by default update vendor_product key \u2018sc4s_metrics\u2019 to change the index Deprecated use of vendor_product_by_source for null queue or dropping events see See Filtering events from output this use will be removed in v3 Deprecated use of vendor_product_by_source for identification of source by host/ip see new app-parser syntax documented per applicable product Deprecated use of SPLUNK_HEC_ALT_DESTS this variable is no longer used and will be ignored Deprecated use of SC4S_DEST_GLOBAL_ALTERNATES this variable will be removed in future major versions see Destinations section in configuration Corrected Vendor/Product keys BREAKING Please see source doc pages and revise configuration as part of upgrade Zscaler (multiple changes) dell_emc_powerswitch_n F5_BIGIP INFOBLOX Dell RSA SecureID ubiquiti SC4S will now use \u201csplunk as the vendor value, \u201csc4s\u201d as the product Fireye HX Juniper ossec Palo Alto Networks Pulse Connect ricoh tanium tintri Vmware esx,vcenter,nsx,horizon Wallix Bastion Internal Changes .dest_key field is no longer used sc4s_vendor_product is read only and will be removed sc4s_vendor new contains \u201cvendor\u201d portion of vendor_product sc4s_vendor_product new contains \u201cproduct\u201d portion of vendor product sc4s_class new contains additional data previously concatenated to vendor_product removed meta_key Custom \u201capp-parsers\u201d Critical Change #Current app parsers contain one or more lines vendor_product ( ' value_here ' ) #This must change to failure to make this change will prevent sc4s from starting vendor ( ' value ' ) product ( ' here ' )","title":"Upgrading SC4S"},{"location":"upgrade/#upgrading-splunk-connect-for-syslog","text":"Splunk Connect for Syslog is updated regularly using a CI/CD development process. The notes below outline significant changes that must be taken into account prior and after an upgrade. Ensure to follow specific instructions below to ensure a smooth transition to a new version of SC4S in production.","title":"Upgrading Splunk Connect for Syslog"},{"location":"upgrade/#upgrade-process","text":"Check the current version of SC4S by running sudo <docker or podman> logs SC4S . For the latest version, use the latest tag for the SC4S image in the sc4s.service unit file: [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2\" Restart the service sudo systemctl restart sc4s Using the \u201c1\u201d version is recommended, but a specific version can be specified in the unit file if desired: [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2.91.0\" See the release information for more detail.","title":"Upgrade process"},{"location":"upgrade/#upgrade-notes","text":"Need up migrating legacy \u201clog paths\u201d or v1 app-parsers for v2. Open an issue with the original config attached and a compressed pcap of sample data for testing and we will evaluate inclusion of the source in an upcoming release.","title":"Upgrade Notes"},{"location":"upgrade/#upgrade-from-2230","text":"Vmware vsphere fix esx and vcenter sourcetype for TA compatibility","title":"Upgrade from &lt;2.23.0"},{"location":"upgrade/#upgrade-from-2","text":"Before upgrading to 2.x review sc4s.service and manually update differences compared to current doc EXPERIMENTAL SNMP Trap feature has been removed migrate to Splunk Connect for SNMP Legacy \u201cgomplate\u201d log path template support was deprecated in 1.x and has been removed in 2.x log paths must be migrated to app-parser style config prior to upgrade Check env_file for \u201cMICROFOCUS_ARCSIGHT\u201d variables and replace with CEF variables see source doc Remove old style \u201cCISCO_*_LEGACY\u201d from env_file and replace per docs New images will no longer be published to docker.io please review current getting started docs and update the sc4s.service file accordingly Internal metrics will now use \u201cmulti\u201d format by default if using unsupported versions of Splunk <8.1 see configuration doc to revert to \u201cevent\u201d or \u201csingle\u201d format. Internal metrics will now use the _metrics index by default update vendor_product key \u2018sc4s_metrics\u2019 to change the index Deprecated use of vendor_product_by_source for null queue or dropping events see See Filtering events from output this use will be removed in v3 Deprecated use of vendor_product_by_source for identification of source by host/ip see new app-parser syntax documented per applicable product Deprecated use of SPLUNK_HEC_ALT_DESTS this variable is no longer used and will be ignored Deprecated use of SC4S_DEST_GLOBAL_ALTERNATES this variable will be removed in future major versions see Destinations section in configuration Corrected Vendor/Product keys BREAKING Please see source doc pages and revise configuration as part of upgrade Zscaler (multiple changes) dell_emc_powerswitch_n F5_BIGIP INFOBLOX Dell RSA SecureID ubiquiti SC4S will now use \u201csplunk as the vendor value, \u201csc4s\u201d as the product Fireye HX Juniper ossec Palo Alto Networks Pulse Connect ricoh tanium tintri Vmware esx,vcenter,nsx,horizon Wallix Bastion Internal Changes .dest_key field is no longer used sc4s_vendor_product is read only and will be removed sc4s_vendor new contains \u201cvendor\u201d portion of vendor_product sc4s_vendor_product new contains \u201cproduct\u201d portion of vendor product sc4s_class new contains additional data previously concatenated to vendor_product removed meta_key Custom \u201capp-parsers\u201d Critical Change #Current app parsers contain one or more lines vendor_product ( ' value_here ' ) #This must change to failure to make this change will prevent sc4s from starting vendor ( ' value ' ) product ( ' here ' )","title":"Upgrade from &lt;2"},{"location":"developing/","text":"Development setup (BETA) \u00b6 Get Docker \u00b6 Development requires Docker desktop available for windows + and mac or Docker CE available for Linux. Visit (Docker)[https://www.docker.com/get-started] for download instructions Setup VS Code IDE \u00b6 VS Code provides a free IDE experience that is effective for daily development with SC4S. Visit (Microsoft)[https://code.visualstudio.com/docs/introvideos/basics] to download and install for your platform (windows/mac/linux) Fork and Clone the github repository \u00b6 Visit our repository at (Github)[https://github.com/splunk/splunk-connect-for-syslog] and \u201cfork\u201d our repository. This will allow you to make changes and submit pull requests. Click the clone icon and select the location Setup the project and install requirements \u00b6 The following steps are required only on the first time run. Install VS Code Extensions S Python Test Explorer \u201cPython Test Explorer\u201d From the terminal menu select \u201cRun Task\u201d Select \u201cSetup step 1: python venv\u201d then \u201cgo without scanning output\u201d From the terminal menu select \u201cRun Task\u201d Select \u201cSetup step 2: python requirements\u201d then \u201cgo without scanning output\u201d Click the test lab icon \u00b6 Run all tests. Icons on each test will turn green or red to indicate pass or fail. Though VS Code does not show the status of any given test until all tests complete in the test tree, you can select \u201cShow test output\u201d near the top of the test directory tree to see the terminal output of each test as it runs in the \u201cOutput\u201d pane.","title":"Development"},{"location":"developing/#development-setup-beta","text":"","title":"Development setup (BETA)"},{"location":"developing/#get-docker","text":"Development requires Docker desktop available for windows + and mac or Docker CE available for Linux. Visit (Docker)[https://www.docker.com/get-started] for download instructions","title":"Get Docker"},{"location":"developing/#setup-vs-code-ide","text":"VS Code provides a free IDE experience that is effective for daily development with SC4S. Visit (Microsoft)[https://code.visualstudio.com/docs/introvideos/basics] to download and install for your platform (windows/mac/linux)","title":"Setup VS Code IDE"},{"location":"developing/#fork-and-clone-the-github-repository","text":"Visit our repository at (Github)[https://github.com/splunk/splunk-connect-for-syslog] and \u201cfork\u201d our repository. This will allow you to make changes and submit pull requests. Click the clone icon and select the location","title":"Fork and Clone the github repository"},{"location":"developing/#setup-the-project-and-install-requirements","text":"The following steps are required only on the first time run. Install VS Code Extensions S Python Test Explorer \u201cPython Test Explorer\u201d From the terminal menu select \u201cRun Task\u201d Select \u201cSetup step 1: python venv\u201d then \u201cgo without scanning output\u201d From the terminal menu select \u201cRun Task\u201d Select \u201cSetup step 2: python requirements\u201d then \u201cgo without scanning output\u201d","title":"Setup the project and install requirements"},{"location":"developing/#click-the-test-lab-icon","text":"Run all tests. Icons on each test will turn green or red to indicate pass or fail. Though VS Code does not show the status of any given test until all tests complete in the test tree, you can select \u201cShow test output\u201d near the top of the test directory tree to see the terminal output of each test as it runs in the \u201cOutput\u201d pane.","title":"Click the test lab icon"},{"location":"gettingstarted/","text":"Before you start \u00b6 Getting Started \u00b6 Splunk Connect for Syslog is a containerized distribution of syslog-ng with a configuration framework designed to simplify getting syslog data into Splunk Enterprise and Splunk Cloud. Our approach is to provide a runtime-agnostic solution allowing customers to deploy using the container runtime environment of choice. Planning Deployment \u00b6 Syslog is an overloaded term that refers to multiple message formats AND optionally a wire protocol for transmission of events between computer systems over UDP, TCP, or TLS. The protocol is designed to minimize overhead on the sender favoring performance over reliability. This fundamental choice means any instability or resource constraint will cause data to be lost in transmission. When practical and cost-effective (considering the importance of completeness as a requirement), place the sc4s instance in the same VLAN as the source device. Avoid crossing a Wireless network, WAN, Firewall, Load Balancer, or inline IDS. When High Availability of a single instance of SC4S is required, implement multi node clustering of the container environment. Avoid TCP except where the source is unable to contain the event to a single UDP packet. Avoid TLS except where the event may cross untrusted network. Plan for appropriately sized hardware Implementation \u00b6 Splunk Setup \u00b6 Runtime configuration \u00b6","title":"Read First"},{"location":"gettingstarted/#before-you-start","text":"","title":"Before you start"},{"location":"gettingstarted/#getting-started","text":"Splunk Connect for Syslog is a containerized distribution of syslog-ng with a configuration framework designed to simplify getting syslog data into Splunk Enterprise and Splunk Cloud. Our approach is to provide a runtime-agnostic solution allowing customers to deploy using the container runtime environment of choice.","title":"Getting Started"},{"location":"gettingstarted/#planning-deployment","text":"Syslog is an overloaded term that refers to multiple message formats AND optionally a wire protocol for transmission of events between computer systems over UDP, TCP, or TLS. The protocol is designed to minimize overhead on the sender favoring performance over reliability. This fundamental choice means any instability or resource constraint will cause data to be lost in transmission. When practical and cost-effective (considering the importance of completeness as a requirement), place the sc4s instance in the same VLAN as the source device. Avoid crossing a Wireless network, WAN, Firewall, Load Balancer, or inline IDS. When High Availability of a single instance of SC4S is required, implement multi node clustering of the container environment. Avoid TCP except where the source is unable to contain the event to a single UDP packet. Avoid TLS except where the event may cross untrusted network. Plan for appropriately sized hardware","title":"Planning Deployment"},{"location":"gettingstarted/#implementation","text":"","title":"Implementation"},{"location":"gettingstarted/#splunk-setup","text":"","title":"Splunk Setup"},{"location":"gettingstarted/#runtime-configuration","text":"","title":"Runtime configuration"},{"location":"gettingstarted/ansible-docker-podman/","text":"Notice \u00b6 SC4S installation can now be automated with Ansible. All you need to do now is provide a host on which you want to run SC4S and basic configuration (Splunk endpoint, HEC token, TLS configuration, etc.). Initial Configuration \u00b6 All you need to do before running sc4s with Ansible is providing env_file . In the env file provide at least proper Splunk endpoint and HEC token. Create a file in ansible/resources catalog or edit example file . SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = http : // xxx . xxx . xxx . xxx : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Next provide a host on which you want to run Docker Swarm cluster and host application in inventory file: all : hosts : children : node : hosts : node_1 : ansible_host : Deploy SC4S \u00b6 Now you can run ansible playbook to deploy the application if you have ansible installed on your host or use docker ansible image provided in the package: # From repository root docker-compose -f ansible/docker-compose.yml build docker-compose -f ansible/docker-compose.yml up -d docker exec -it ansible_sc4s /bin/bash Once you are in containers remote shell you can run Docker Swam ansible playbook. If you are authenticating via username/password: ansible-playbook -i path/to/inventory.yaml -u <username> --ask-pass path/to/playbooks/docker.yml or ansible-playbook -i path/to/inventory.yaml -u <username> --ask-pass path/to/playbooks/podman.yml or using key pair: ansible-playbook -i path/to/inventory.yaml -u <username> --key-file <key_file> path/to/playbooks/docker.yml or ansible-playbook -i path/to/inventory.yaml -u <username> --key-file <key_file> path/to/playbooks/podman.yml Verify Proper Operation \u00b6 SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' You can verify if all services in swarm cluster are working by checking sc4s_container field in splunk- each service should be recognized by different container id. All other fields should be the same. When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container (on the node that is hosting sc4s service). sudo docker ps You will get an ID and , next: docker logs <ID | image name> or: sudo systemctl status sc4s You should see events similar to those below in the output: SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:fallback... SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:events... syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng","title":"Podman/Docker"},{"location":"gettingstarted/ansible-docker-podman/#notice","text":"SC4S installation can now be automated with Ansible. All you need to do now is provide a host on which you want to run SC4S and basic configuration (Splunk endpoint, HEC token, TLS configuration, etc.).","title":"Notice"},{"location":"gettingstarted/ansible-docker-podman/#initial-configuration","text":"All you need to do before running sc4s with Ansible is providing env_file . In the env file provide at least proper Splunk endpoint and HEC token. Create a file in ansible/resources catalog or edit example file . SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = http : // xxx . xxx . xxx . xxx : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Next provide a host on which you want to run Docker Swarm cluster and host application in inventory file: all : hosts : children : node : hosts : node_1 : ansible_host :","title":"Initial Configuration"},{"location":"gettingstarted/ansible-docker-podman/#deploy-sc4s","text":"Now you can run ansible playbook to deploy the application if you have ansible installed on your host or use docker ansible image provided in the package: # From repository root docker-compose -f ansible/docker-compose.yml build docker-compose -f ansible/docker-compose.yml up -d docker exec -it ansible_sc4s /bin/bash Once you are in containers remote shell you can run Docker Swam ansible playbook. If you are authenticating via username/password: ansible-playbook -i path/to/inventory.yaml -u <username> --ask-pass path/to/playbooks/docker.yml or ansible-playbook -i path/to/inventory.yaml -u <username> --ask-pass path/to/playbooks/podman.yml or using key pair: ansible-playbook -i path/to/inventory.yaml -u <username> --key-file <key_file> path/to/playbooks/docker.yml or ansible-playbook -i path/to/inventory.yaml -u <username> --key-file <key_file> path/to/playbooks/podman.yml","title":"Deploy SC4S"},{"location":"gettingstarted/ansible-docker-podman/#verify-proper-operation","text":"SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' You can verify if all services in swarm cluster are working by checking sc4s_container field in splunk- each service should be recognized by different container id. All other fields should be the same. When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container (on the node that is hosting sc4s service). sudo docker ps You will get an ID and , next: docker logs <ID | image name> or: sudo systemctl status sc4s You should see events similar to those below in the output: SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:fallback... SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:events... syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng","title":"Verify Proper Operation"},{"location":"gettingstarted/ansible-docker-swarm/","text":"Notice \u00b6 SC4S installation can now be automated with Ansible. All you need to do now is provide list of hosts on which you want to run SC4S and basic configuration (Splunk endpoint, HEC token, TLS configuration, etc.). This manual assumes that you have proper knowledge of Docker Swarm as setting up proper Swarm architecture/configuration is users duty. Initial Configuration \u00b6 All you need to do before running sc4s with Ansible is providing env_file . In the env file provide at least proper Splunk endpoint and HEC token. Create a file in ansible/resources catalog or edit example file . SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = http : // xxx . xxx . xxx . xxx : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Next provide a list of hosts on which you want to run Docker Swarm cluster and host application in inventory file: all : hosts : children : manager : hosts : manager_node_1 : ansible_host : worker : hosts : worker_node_1 : ansible_host : worker_node_2 : ansible_host : You can run your cluster with one or more manager nodes for more info about setting up a swarm refer to official docker documentation . NOTICE: One of biggest advantages of using Docker Swarm for hosting SC4S is Swarm internal load balancer (routing mesh). To get to know the details refer to docker documentation . Additionally, you can provide extra service configurations (ex. number of replicas) in /ansible/app/docker-compose.yml file: version : \"3.7\" services : sc4s : deploy : replicas : 2 ... Deploy SC4S \u00b6 Now you can run ansible playbook to deploy the application if you have ansible installed on your host or use docker ansible image provided in the package: # From repository root docker-compose -f ansible/docker-compose.yml build docker-compose -f ansible/docker-compose.yml up -d docker exec -it ansible_sc4s /bin/bash Once you are in containers remote shell you can run Docker Swam ansible playbook. If you are authenticating via username/password: ansible-playbook -i path/to/inventory_swarm.yaml -u <username> --ask-pass path/to/playbooks/docker_swarm.yml or using key pair: ansible-playbook -i path/to/inventory_swarm.yaml -u <username> --key-file <key_file> path/to/playbooks/docker_swarm.yml If the process was finished properly you should be able to check state of Swarm cluster and deployed stack from manager\u2019s node remote shell: Verify if stack was created: sudo docker stack ls NAME SERVICES ORCHESTRATOR sc4s 1 Swarm You can scale your number of services: sudo docker service update --replicas 2 sc4s_sc4s See services running in a given stack: sudo docker stack services sc4s ID NAME MODE REPLICAS IMAGE PORTS 1xv9vvbizf3m sc4s_sc4s replicated 2/2 ghcr.io/splunk/splunk-connect-for-syslog/container2:2 :514->514/tcp, :601->601/tcp, :6514->6514/tcp, :514->514/udp Verify Proper Operation \u00b6 SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' You can verify if all services in swarm cluster are working by checking sc4s_container field in splunk- each service should be recognized by different container id. All other fields should be the same. When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container (on the node that is hosting sc4s service). sudo docker | podman ps You will get an ID and , next: docker | podman logs <ID | image name> You should see events similar to those below in the output: SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:fallback... SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:events... syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng","title":"Docker Swarm"},{"location":"gettingstarted/ansible-docker-swarm/#notice","text":"SC4S installation can now be automated with Ansible. All you need to do now is provide list of hosts on which you want to run SC4S and basic configuration (Splunk endpoint, HEC token, TLS configuration, etc.). This manual assumes that you have proper knowledge of Docker Swarm as setting up proper Swarm architecture/configuration is users duty.","title":"Notice"},{"location":"gettingstarted/ansible-docker-swarm/#initial-configuration","text":"All you need to do before running sc4s with Ansible is providing env_file . In the env file provide at least proper Splunk endpoint and HEC token. Create a file in ansible/resources catalog or edit example file . SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = http : // xxx . xxx . xxx . xxx : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Next provide a list of hosts on which you want to run Docker Swarm cluster and host application in inventory file: all : hosts : children : manager : hosts : manager_node_1 : ansible_host : worker : hosts : worker_node_1 : ansible_host : worker_node_2 : ansible_host : You can run your cluster with one or more manager nodes for more info about setting up a swarm refer to official docker documentation . NOTICE: One of biggest advantages of using Docker Swarm for hosting SC4S is Swarm internal load balancer (routing mesh). To get to know the details refer to docker documentation . Additionally, you can provide extra service configurations (ex. number of replicas) in /ansible/app/docker-compose.yml file: version : \"3.7\" services : sc4s : deploy : replicas : 2 ...","title":"Initial Configuration"},{"location":"gettingstarted/ansible-docker-swarm/#deploy-sc4s","text":"Now you can run ansible playbook to deploy the application if you have ansible installed on your host or use docker ansible image provided in the package: # From repository root docker-compose -f ansible/docker-compose.yml build docker-compose -f ansible/docker-compose.yml up -d docker exec -it ansible_sc4s /bin/bash Once you are in containers remote shell you can run Docker Swam ansible playbook. If you are authenticating via username/password: ansible-playbook -i path/to/inventory_swarm.yaml -u <username> --ask-pass path/to/playbooks/docker_swarm.yml or using key pair: ansible-playbook -i path/to/inventory_swarm.yaml -u <username> --key-file <key_file> path/to/playbooks/docker_swarm.yml If the process was finished properly you should be able to check state of Swarm cluster and deployed stack from manager\u2019s node remote shell: Verify if stack was created: sudo docker stack ls NAME SERVICES ORCHESTRATOR sc4s 1 Swarm You can scale your number of services: sudo docker service update --replicas 2 sc4s_sc4s See services running in a given stack: sudo docker stack services sc4s ID NAME MODE REPLICAS IMAGE PORTS 1xv9vvbizf3m sc4s_sc4s replicated 2/2 ghcr.io/splunk/splunk-connect-for-syslog/container2:2 :514->514/tcp, :601->601/tcp, :6514->6514/tcp, :514->514/udp","title":"Deploy SC4S"},{"location":"gettingstarted/ansible-docker-swarm/#verify-proper-operation","text":"SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' You can verify if all services in swarm cluster are working by checking sc4s_container field in splunk- each service should be recognized by different container id. All other fields should be the same. When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container (on the node that is hosting sc4s service). sudo docker | podman ps You will get an ID and , next: docker | podman logs <ID | image name> You should see events similar to those below in the output: SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:fallback... SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:events... syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng","title":"Verify Proper Operation"},{"location":"gettingstarted/ansible-mk8s/","text":"Notice \u00b6 SC4S installation can now be automated with Ansible. All you need to do now is provide list of hosts on which you want to run SC4S and basic configuration (Splunk endpoint, HEC token, TLS configuration, etc.). This manual assumes that you have proper knowledge of microk8s as setting up proper kubernetes cluster architecture/configuration is users duty. Initial Configuration \u00b6 All you need to do before running sc4s with Ansible is providing proper values in values.yaml file (Splunk endpoint and HEC token). Edit example file . Next provide a list of hosts on which you want to run Docker Swarm cluster and host application in inventory file: all : hosts : children : node : hosts : node_1 : ansible_host : or if you want to spin up HA cluster: all : hosts : children : manager : hosts : manager : ansible_host : workers : hosts : worker1 : ansible_host : worker2 : ansible_host : Deploy SC4S \u00b6 Now you can run ansible playbook to deploy the application if you have ansible installed on your host or use docker ansible image provided in the package: # From repository root docker-compose -f ansible/docker-compose.yml build docker-compose -f ansible/docker-compose.yml up -d docker exec -it ansible_sc4s /bin/bash Once you are in containers remote shell you can run Docker Swam ansible playbook. If you are authenticating via username/password: ansible-playbook -i path/to/inventory_mk8s.yaml -u <username> --ask-pass path/to/playbooks/microk8s.yml or if you are running HA cluster: ansible-playbook -i path/to/inventory_mk8s_ha.yaml -u <username> --ask-pass path/to/playbooks/microk8s_ha.yml or using key pair: ansible-playbook -i path/to/inventory_mk8s.yaml -u <username> --key-file <key_file> path/to/playbooks/microk8s.yml Verify Proper Operation \u00b6 SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' You can verify if all services in swarm cluster are working by checking sc4s_container field in splunk- each service should be recognized by different container id. All other fields should be the same. When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container (on the node that is hosting sc4s service). sudo microk8s kubectl get pods sudo microk8s kubectl logs <podname> You should see events similar to those below in the output: SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:fallback... SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:events... syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng","title":"mk8s"},{"location":"gettingstarted/ansible-mk8s/#notice","text":"SC4S installation can now be automated with Ansible. All you need to do now is provide list of hosts on which you want to run SC4S and basic configuration (Splunk endpoint, HEC token, TLS configuration, etc.). This manual assumes that you have proper knowledge of microk8s as setting up proper kubernetes cluster architecture/configuration is users duty.","title":"Notice"},{"location":"gettingstarted/ansible-mk8s/#initial-configuration","text":"All you need to do before running sc4s with Ansible is providing proper values in values.yaml file (Splunk endpoint and HEC token). Edit example file . Next provide a list of hosts on which you want to run Docker Swarm cluster and host application in inventory file: all : hosts : children : node : hosts : node_1 : ansible_host : or if you want to spin up HA cluster: all : hosts : children : manager : hosts : manager : ansible_host : workers : hosts : worker1 : ansible_host : worker2 : ansible_host :","title":"Initial Configuration"},{"location":"gettingstarted/ansible-mk8s/#deploy-sc4s","text":"Now you can run ansible playbook to deploy the application if you have ansible installed on your host or use docker ansible image provided in the package: # From repository root docker-compose -f ansible/docker-compose.yml build docker-compose -f ansible/docker-compose.yml up -d docker exec -it ansible_sc4s /bin/bash Once you are in containers remote shell you can run Docker Swam ansible playbook. If you are authenticating via username/password: ansible-playbook -i path/to/inventory_mk8s.yaml -u <username> --ask-pass path/to/playbooks/microk8s.yml or if you are running HA cluster: ansible-playbook -i path/to/inventory_mk8s_ha.yaml -u <username> --ask-pass path/to/playbooks/microk8s_ha.yml or using key pair: ansible-playbook -i path/to/inventory_mk8s.yaml -u <username> --key-file <key_file> path/to/playbooks/microk8s.yml","title":"Deploy SC4S"},{"location":"gettingstarted/ansible-mk8s/#verify-proper-operation","text":"SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' You can verify if all services in swarm cluster are working by checking sc4s_container field in splunk- each service should be recognized by different container id. All other fields should be the same. When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container (on the node that is hosting sc4s service). sudo microk8s kubectl get pods sudo microk8s kubectl logs <podname> You should see events similar to those below in the output: SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:fallback... SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index = main for sourcetype=sc4s:events... syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng","title":"Verify Proper Operation"},{"location":"gettingstarted/byoe-rhel8/","text":"SC4S \u201cBring Your Own Environment\u201d \u00b6 FOREWORD: The BYOE SC4S deliverable should be considered as a self/community supported option for SC4S deployment, and should be considered only by those with specific needs based on advanced understanding of syslog-ng architectures and linux/syslog-ng system administration and the ability to develop and automate testing in non-production environments. The container deliverable is the most often correct deliverable of SC4S for almost all enterprises. If you are simply trying to \u201cget syslog working\u201d, the turnkey, container approach described in the other runtime documents will be the fastest route to success. The \u201cBring Your Own Environment\u201d instructions that follow allow expert administrators to utilize the SC4S syslog-ng config files directly on the host OS running on a hardware server or virtual machine. Administrators must provide an appropriate host OS (RHEL 8 used in this document) as well as an up-to-date syslog-ng installation either built from source (not documented here) or installed from community-built RPMs. Modification of the base configuration will be required for most customer environments due to enterprise infrastructure variations. Once installed preparing an upgrade requires evaluation of the current environment compared to this reference then developing and testing a installation specific install plan. This activity is the responsibility of the administrator. NOTE: Installing or modifying system configurations can have unexpected consequences, and advanced linux system administration and syslog-ng configuration experience is assumed when using the BYOE version of SC4S. NOTE: Do not depend on the distribution-supplied version of syslog-ng, as it will likely be far too old. Read this explanation for the reason why syslog-ng builds are so dated in almost all RHEL/Debian distributions. BYOE Installation Instructions \u00b6 These installation instructions assume a recent RHEL or CentOS-based release. Minor adjustments may have to be made for Debian/Ubuntu. In addition, almost all pre-compiled binaries for syslog-ng assume installation in /etc/syslog-ng ; these instructions will reflect that. The following installation instructions are summarized from a blog maintained by a developer at One Identity (formerly Balabit), who is the owner of the syslog-ng Open Source project. It is always advisable to review the blog for the latest changes to the repo(s), as changes here are quite dynamic. Install CentOS or RHEL 8.0 Enable EPEL (Centos 8) dnf install 'dnf-command(copr)' -y dnf install epel-release -y dnf copr enable czanik/syslog-ng336 -y dnf install syslog-ng syslog-ng-python syslog-ng-http python3-pip gcc python3-devel -y Disable the distro-supplied syslog-ng unit file, as the syslog-ng process configured here will run as the sc4s service. rsyslog will continue to be the system logger, but should be left enabled only if it is configured to not listen on the same ports as sc4s. sc4s BYOE can be configured to provide local logging as well if desired. sudo systemctl stop syslog-ng sudo systemctl disable syslog-ng Download the latest bare_metal.tar from releases on github and untar the package in /etc/syslog-ng using the command example below. NOTE: The wget process below will unpack a tarball with the sc4s version of the syslog-ng config files in the standard /etc/syslog-ng location, and will overwrite existing content. Ensure that any previous configurations of syslog-ng are saved if needed prior to executing the download step. NOTE: At the time of writing, the latest major release is v1.33 . The latest release is typically listed first on the page above, unless there is an -alpha , -beta , or -rc release that is newer (which will be clearly indicated). For production use, select the latest that does not have an -rc , -alpha , or -beta suffix. sudo wget -c https://github.com/splunk/splunk-connect-for-syslog/releases/download/<latest release>/baremetal.tar -O - | sudo tar -x -C /etc/syslog-ng Install python requirements sudo pip3 install -r /etc/syslog-ng/requirements.txt (Optional, for monitoring): Install goss and confirm that the version is v0.3.16 or newer. goss installs in /usr/local/bin by default, so ensure that 1) entrypoint.sh is modified to include /usr/local/bin in the full path, or 2) move the goss binary to /bin or /usr/bin . curl - L https : // github . com / aelsabbahy / goss / releases / latest / download / goss - linux - amd64 - o / usr / local / bin / goss chmod + rx / usr / local / bin / goss curl - L https : // github . com / aelsabbahy / goss / releases / latest / download / dgoss - o / usr / local / bin / dgoss # Alternatively, using the latest # curl -L https://raw.githubusercontent.com/aelsabbahy/goss/latest/extras/dgoss/dgoss -o /usr/local/bin/dgoss chmod + rx / usr / local / bin / dgoss There are two main options for running SC4S via systemd, the choice of which largely depends on administrator preference and orchestration methodology: 1) the entrypoint.sh script (identical to that used in the container) can be run directly via systemd, or 2) the script can be altered to preconfigure SC4S (after which only the syslog-ng are run via systemd). These are by no means the only ways to run BYOE \u2013 as the name implies, the method you choose will be based on your custom needs. To run the entrypoint.sh script directly in systemd, create the sc4s unit file /lib/systemd/system/sc4s.service and add the following content: [Unit] Description = SC4S Syslog Daemon Documentation = https://splunk-connect-for-syslog.readthedocs.io/en/latest/ Wants = network.target network-online.target After = network.target network-online.target [Service] Type = simple ExecStart = /etc/syslog-ng/entrypoint.sh ExecReload = /bin/kill -HUP $MAINPID EnvironmentFile = /etc/syslog-ng/env_file StandardOutput = journal StandardError = journal Restart = on-abnormal [Install] WantedBy = multi-user.target To run entrypoint.sh as a \u201cpreconfigure\u201d script, modify the script by commenting out or removing the stanzas following the OPTIONAL for BYOE comments in the script. This will prevent syslog-ng from being launched by the script. Then create the sc4s unit file /lib/systemd/system/syslog-ng.service and add the following content: [Unit] Description = System Logger Daemon Documentation = man:syslog-ng(8) After = network.target [Service] Type = notify ExecStart = /usr/sbin/syslog-ng -F $SYSLOGNG_OPTS -p /var/run/syslogd.pid ExecReload = /bin/kill -HUP $MAINPID EnvironmentFile = -/etc/default/syslog-ng EnvironmentFile = -/etc/sysconfig/syslog-ng StandardOutput = journal StandardError = journal Restart = on-failure [Install] WantedBy = multi-user.target Create the file /etc/syslog-ng/env_file and add the following environment variables (adjusting the URL/TOKEN appropriately): # The following \"path\" variables can differ from the container defaults specified in the entrypoint.sh script. # These are *optional* for most BYOE installations, which do not differ from the install location used. # in the container version of SC4S. Failure to properly set these will cause startup failure. #SC4S_ETC=/etc/syslog-ng #SC4S_VAR=/etc/syslog-ng/var #SC4S_BIN=/bin #SC4S_SBIN=/usr/sbin #SC4S_TLS=/etc/syslog-ng/tls # General Options SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // splunk . smg . aws : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = a778f63a - 5 dff - 4e3 c - a72c - a03183659e94 # Uncomment the following line if using untrusted (self-signed) SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no Reload systemctl and restart syslog-ng (example here is shown for systemd option (1) above) sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s Configure SC4S Listening Ports \u00b6 Most enterprises use UDP/TCP port 514 as the default as their main listening port for syslog \u201csoup\u201d traffic, and TCP port 6514 for TLS. The standard SC4S configuration reflect these defaults. These defaults can be changed by adding the following additional environment variables with appropriate values to the env_file above: SC4S_LISTEN_DEFAULT_TCP_PORT=514 SC4S_LISTEN_DEFAULT_UDP_PORT=514 SC4S_LISTEN_DEFAULT_RFC6587_PORT=601 SC4S_LISTEN_DEFAULT_RFC5426_PORT=601 SC4S_LISTEN_DEFAULT_RFC5425_PORT=5425 SC4S_LISTEN_DEFAULT_TLS_PORT=6514 Dedicated (Unique) Listening Ports \u00b6 For certain source technologies, categorization by message content is impossible due to the lack of a unique \u201cfingerprint\u201d in the data. In other cases, a unique listening port is required for certain devices due to network requirements in the enterprise. For collection of such sources we provide a means of dedicating a unique listening port to a specific source. Refer to the \u201cSources\u201d documentation to identify the specific environment variables used to enable unique listening ports for the technology in use.","title":"Bring your own Envionment"},{"location":"gettingstarted/byoe-rhel8/#sc4s-bring-your-own-environment","text":"FOREWORD: The BYOE SC4S deliverable should be considered as a self/community supported option for SC4S deployment, and should be considered only by those with specific needs based on advanced understanding of syslog-ng architectures and linux/syslog-ng system administration and the ability to develop and automate testing in non-production environments. The container deliverable is the most often correct deliverable of SC4S for almost all enterprises. If you are simply trying to \u201cget syslog working\u201d, the turnkey, container approach described in the other runtime documents will be the fastest route to success. The \u201cBring Your Own Environment\u201d instructions that follow allow expert administrators to utilize the SC4S syslog-ng config files directly on the host OS running on a hardware server or virtual machine. Administrators must provide an appropriate host OS (RHEL 8 used in this document) as well as an up-to-date syslog-ng installation either built from source (not documented here) or installed from community-built RPMs. Modification of the base configuration will be required for most customer environments due to enterprise infrastructure variations. Once installed preparing an upgrade requires evaluation of the current environment compared to this reference then developing and testing a installation specific install plan. This activity is the responsibility of the administrator. NOTE: Installing or modifying system configurations can have unexpected consequences, and advanced linux system administration and syslog-ng configuration experience is assumed when using the BYOE version of SC4S. NOTE: Do not depend on the distribution-supplied version of syslog-ng, as it will likely be far too old. Read this explanation for the reason why syslog-ng builds are so dated in almost all RHEL/Debian distributions.","title":"SC4S \"Bring Your Own Environment\""},{"location":"gettingstarted/byoe-rhel8/#byoe-installation-instructions","text":"These installation instructions assume a recent RHEL or CentOS-based release. Minor adjustments may have to be made for Debian/Ubuntu. In addition, almost all pre-compiled binaries for syslog-ng assume installation in /etc/syslog-ng ; these instructions will reflect that. The following installation instructions are summarized from a blog maintained by a developer at One Identity (formerly Balabit), who is the owner of the syslog-ng Open Source project. It is always advisable to review the blog for the latest changes to the repo(s), as changes here are quite dynamic. Install CentOS or RHEL 8.0 Enable EPEL (Centos 8) dnf install 'dnf-command(copr)' -y dnf install epel-release -y dnf copr enable czanik/syslog-ng336 -y dnf install syslog-ng syslog-ng-python syslog-ng-http python3-pip gcc python3-devel -y Disable the distro-supplied syslog-ng unit file, as the syslog-ng process configured here will run as the sc4s service. rsyslog will continue to be the system logger, but should be left enabled only if it is configured to not listen on the same ports as sc4s. sc4s BYOE can be configured to provide local logging as well if desired. sudo systemctl stop syslog-ng sudo systemctl disable syslog-ng Download the latest bare_metal.tar from releases on github and untar the package in /etc/syslog-ng using the command example below. NOTE: The wget process below will unpack a tarball with the sc4s version of the syslog-ng config files in the standard /etc/syslog-ng location, and will overwrite existing content. Ensure that any previous configurations of syslog-ng are saved if needed prior to executing the download step. NOTE: At the time of writing, the latest major release is v1.33 . The latest release is typically listed first on the page above, unless there is an -alpha , -beta , or -rc release that is newer (which will be clearly indicated). For production use, select the latest that does not have an -rc , -alpha , or -beta suffix. sudo wget -c https://github.com/splunk/splunk-connect-for-syslog/releases/download/<latest release>/baremetal.tar -O - | sudo tar -x -C /etc/syslog-ng Install python requirements sudo pip3 install -r /etc/syslog-ng/requirements.txt (Optional, for monitoring): Install goss and confirm that the version is v0.3.16 or newer. goss installs in /usr/local/bin by default, so ensure that 1) entrypoint.sh is modified to include /usr/local/bin in the full path, or 2) move the goss binary to /bin or /usr/bin . curl - L https : // github . com / aelsabbahy / goss / releases / latest / download / goss - linux - amd64 - o / usr / local / bin / goss chmod + rx / usr / local / bin / goss curl - L https : // github . com / aelsabbahy / goss / releases / latest / download / dgoss - o / usr / local / bin / dgoss # Alternatively, using the latest # curl -L https://raw.githubusercontent.com/aelsabbahy/goss/latest/extras/dgoss/dgoss -o /usr/local/bin/dgoss chmod + rx / usr / local / bin / dgoss There are two main options for running SC4S via systemd, the choice of which largely depends on administrator preference and orchestration methodology: 1) the entrypoint.sh script (identical to that used in the container) can be run directly via systemd, or 2) the script can be altered to preconfigure SC4S (after which only the syslog-ng are run via systemd). These are by no means the only ways to run BYOE \u2013 as the name implies, the method you choose will be based on your custom needs. To run the entrypoint.sh script directly in systemd, create the sc4s unit file /lib/systemd/system/sc4s.service and add the following content: [Unit] Description = SC4S Syslog Daemon Documentation = https://splunk-connect-for-syslog.readthedocs.io/en/latest/ Wants = network.target network-online.target After = network.target network-online.target [Service] Type = simple ExecStart = /etc/syslog-ng/entrypoint.sh ExecReload = /bin/kill -HUP $MAINPID EnvironmentFile = /etc/syslog-ng/env_file StandardOutput = journal StandardError = journal Restart = on-abnormal [Install] WantedBy = multi-user.target To run entrypoint.sh as a \u201cpreconfigure\u201d script, modify the script by commenting out or removing the stanzas following the OPTIONAL for BYOE comments in the script. This will prevent syslog-ng from being launched by the script. Then create the sc4s unit file /lib/systemd/system/syslog-ng.service and add the following content: [Unit] Description = System Logger Daemon Documentation = man:syslog-ng(8) After = network.target [Service] Type = notify ExecStart = /usr/sbin/syslog-ng -F $SYSLOGNG_OPTS -p /var/run/syslogd.pid ExecReload = /bin/kill -HUP $MAINPID EnvironmentFile = -/etc/default/syslog-ng EnvironmentFile = -/etc/sysconfig/syslog-ng StandardOutput = journal StandardError = journal Restart = on-failure [Install] WantedBy = multi-user.target Create the file /etc/syslog-ng/env_file and add the following environment variables (adjusting the URL/TOKEN appropriately): # The following \"path\" variables can differ from the container defaults specified in the entrypoint.sh script. # These are *optional* for most BYOE installations, which do not differ from the install location used. # in the container version of SC4S. Failure to properly set these will cause startup failure. #SC4S_ETC=/etc/syslog-ng #SC4S_VAR=/etc/syslog-ng/var #SC4S_BIN=/bin #SC4S_SBIN=/usr/sbin #SC4S_TLS=/etc/syslog-ng/tls # General Options SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // splunk . smg . aws : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = a778f63a - 5 dff - 4e3 c - a72c - a03183659e94 # Uncomment the following line if using untrusted (self-signed) SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no Reload systemctl and restart syslog-ng (example here is shown for systemd option (1) above) sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s","title":"BYOE Installation Instructions"},{"location":"gettingstarted/byoe-rhel8/#configure-sc4s-listening-ports","text":"Most enterprises use UDP/TCP port 514 as the default as their main listening port for syslog \u201csoup\u201d traffic, and TCP port 6514 for TLS. The standard SC4S configuration reflect these defaults. These defaults can be changed by adding the following additional environment variables with appropriate values to the env_file above: SC4S_LISTEN_DEFAULT_TCP_PORT=514 SC4S_LISTEN_DEFAULT_UDP_PORT=514 SC4S_LISTEN_DEFAULT_RFC6587_PORT=601 SC4S_LISTEN_DEFAULT_RFC5426_PORT=601 SC4S_LISTEN_DEFAULT_RFC5425_PORT=5425 SC4S_LISTEN_DEFAULT_TLS_PORT=6514","title":"Configure SC4S Listening Ports"},{"location":"gettingstarted/byoe-rhel8/#dedicated-unique-listening-ports","text":"For certain source technologies, categorization by message content is impossible due to the lack of a unique \u201cfingerprint\u201d in the data. In other cases, a unique listening port is required for certain devices due to network requirements in the enterprise. For collection of such sources we provide a means of dedicating a unique listening port to a specific source. Refer to the \u201cSources\u201d documentation to identify the specific environment variables used to enable unique listening ports for the technology in use.","title":"Dedicated (Unique) Listening Ports"},{"location":"gettingstarted/create-parser/","text":"Create a parser \u00b6 The following is a step-by-step guide for adding new parsers. Why create a parser? \u00b6 Splunk Connect for Syslog can offload Splunk Indexers by performing operations that normally would have been done during index time, including linebreaking, source/sourcetype setting, and timestamping. Creating a parser also reduces the need of using corresponding add-ons on indexers. Before you start \u00b6 Make sure you have read contribution standards . For more background information on how filters and parser work, and what suits you best, read about sources onboarding . Prepare your environment . Create a new branch in the repository where you will apply your changes. Start with a raw log message \u00b6 If you already have a raw log message, you can skip this step. Otherwise, you need to extract one to have something to work with. You can do this in multiple ways; here is a brief description of two of them: tcpdump \u00b6 You can use the tcpdump command to get incoming raw messages on a given port of your server. tcpdump -n -s 0 -S -i any -v port 8088 tcpdump: listening on any, link-type LINUX_SLL ( Linux cooked ) , capture size 262144 bytes 09 :54:26.051644 IP ( tos 0x0, ttl 64 , id 29465 , offset 0 , flags [ DF ] , proto UDP ( 17 ) , length 466 ) 10 .202.22.239.41151 > 10 .202.33.242.syslog: SYSLOG, length: 438 Facility local0 ( 16 ) , Severity info ( 6 ) Msg: 2022 -04-28T16:16:15.466731-04:00 NTNX-21SM6M510425-B-CVM audispd [ 32075 ] : node = ntnx-21sm6m510425-b-cvm type = SYSCALL msg = audit ( 1651176975 .464:2828209 ) : arch = c000003e syscall = 2 success = yes exit = 6 a0 = 7f2955ac932e a1 = 2 a2 = 3e8 a3 = 3 items = 1 ppid = 29680 pid = 4684 auid = 1000 uid = 0 gid = 0 euid = 0 suid = 0 fsuid = 0 egid = 0 sgid = 0 fsgid = 0 tty =( none ) ses = 964698 comm = \u201csshd\u201d exe = \u201c/usr/sbin/sshd\u201d subj = system_u:system_r:sshd_t:s0-s0:c0.c1023 key = \u201clogins\u201d \\0 x0a Wireshark \u00b6 Or you can read the logs using Wireshark from the .pcap file. From Wireshark go to Statistics->Conversations, then click on \u2018Follow Stream\u2019. Once you get your stream of messages, copy one of them. NOTE: In UDP there usually will not be any message separators. Create a unit test \u00b6 It is recommended to use the existing test case that is the most similar to your use case. The naming convention is test_vendor_product.py Afterwards, you need to make sure that your log is being parsed correctly by creating a test case. Assuming you have a raw message like this: < 14 >1 2022 -03-30T11:17:11.900862-04:00 host - - - - Carbon Black App Control event: text = \"File 'c:\\program files\\azure advanced threat protection sensor\\2.175.15073.51407\\winpcap\\x86\\packet.dll' [c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363] would have blocked if the rule was not in Report Only mode.\" type = \"Policy Enforcement\" subtype = \"Execution block (unapproved file)\" hostname = \"CORP\\USER\" username = \"NT AUTHORITY\\SYSTEM\" date = \"3/30/2022 3:16:40 PM\" ip_address = \"10.0.0.3\" process = \"c:\\program files\\azure advanced threat protection sensor\\2.175.15073.51407\\microsoft.tri.sensor.updater.exe\" file_path = \"c:\\program files\\azure advanced threat protection sensor\\2.175.15073.51407\\winpcap\\x86\\packet.dll\" file_name = \"packet.dll\" file_hash = \"c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363\" policy = \"High Enforcement - Domain Controllers\" rule_name = \"Report read-only memory map operations on unapproved executables by .NET applications\" process_key = \"00000433-0000-23d8-01d8-44491b26f203\" server_version = \"8.5.4.3\" file_trust = \"-2\" file_threat = \"-2\" process_trust = \"-2\" process_threat = \"-2\" prevalence = \"50\" You need to: * make sure that the message is a valid python string, where escape characters are placed correctly. * anonymize the data. * rename functions. * update index, and sourcetype fields. * extract replace values with field names in test string. Here you can see proper test case for Vmware Carbonblack Protect device: # Copyright 2019 Splunk, Inc. # # Use of this source code is governed by a BSD-2-clause-style # license that can be found in the LICENSE-BSD2 file or at # https://opensource.org/licenses/BSD-2-Clause import random from jinja2 import Environment from .sendmessage import * from .splunkutils import * from .timeutils import * env = Environment () # Below is a raw message # <14>1 2022-03-30T11:17:11.900862-04:00 host - - - - Carbon Black App Control event: text=\"File 'c:\\program files\\azure advanced threat protection sensor\\0.0.0.0\\winpcap\\x86\\packet.dll' [c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363] would have blocked if the rule was not in Report Only mode.\" type=\"Policy Enforcement\" subtype=\"Execution block (unapproved file)\" hostname=\"CORP\\USER\" username=\"NT AUTHORITY\\SYSTEM\" date=\"3/30/2022 3:16:40 PM\" ip_address=\"0.0.0.0\" process=\"c:\\program files\\azure advanced threat protection sensor\\0.0.0.0\\microsoft.tri.sensor.updater.exe\" file_path=\"c:\\program files\\azure advanced threat protection sensor\\0.0.0.0\\winpcap\\x86\\packet.dll\" file_name=\"packet.dll\" file_hash=\"c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363\" policy=\"High Enforcement - Domain Controllers\" rule_name=\"Report read-only memory map operations on unapproved executables by .NET applications\" process_key=\"00000433-0000-23d8-01d8-44491b26f203\" server_version=\"0.0.0.0\" file_trust=\"-2\" file_threat=\"-2\" process_trust=\"-2\" process_threat=\"-2\" prevalence=\"50\" # Don't forget to rename the function def test_vmware_carbonblack_protect ( record_property , setup_wordlist , setup_splunk , setup_sc4s ): host = \" {} - {} \" . format ( random . choice ( setup_wordlist ), random . choice ( setup_wordlist )) dt = datetime . datetime . now () iso , bsd , time , date , tzoffset , tzname , epoch = time_operations ( dt ) # Tune time functions for Checkpoint epoch = epoch [: - 3 ] mt = env . from_string ( # Extract mark, iso timestamp and host fields # Make sure all needed characters are escaped # If string contains single quotes wrap it in double qutes '{{ mark }} {{ iso }} {{ host }} - - - - Carbon Black App Control event: text=\"File \\' c: \\\\ program files \\\\ azure advanced threat protection sensor \\\\ 0.0.0.0 \\\\ winpcap \\\\ x86 \\\\ packet.dll \\' [c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363] would have blocked if the rule was not in Report Only mode.\" type=\"Policy Enforcement\" subtype=\"Execution block (unapproved file)\" hostname=\"CORP \\\\ USER\" username=\"NT AUTHORITY \\\\ SYSTEM\" date=\"3/30/2022 3:16:40 PM\" ip_address=\"0.0.0.0\" process=\"c: \\\\ program files \\\\ azure advanced threat protection sensor \\\\ 0.0.0.0 \\\\ microsoft.tri.sensor.updater.exe\" file_path=\"c: \\\\ program files \\\\ azure advanced threat protection sensor \\\\ 0.0.0.0 \\\\ winpcap \\\\ x86 \\\\ packet.dll\" file_name=\"packet.dll\" file_hash=\"c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363\" policy=\"High Enforcement - Domain Controllers\" rule_name=\"Report read-only memory map operations on unapproved executables by .NET applications\" process_key=\"00000433-0000-23d8-01d8-44491b26f203\" server_version=\"0.0.0.0\" file_trust=\"-2\" file_threat=\"-2\" process_trust=\"-2\" process_threat=\"-2\" prevalence=\"50\"' ) message = mt . render ( mark = \"<134>1\" , host = host , bsd = bsd , iso = iso , epoch = epoch ) sendsingle ( message , setup_sc4s [ 0 ], setup_sc4s [ 1 ][ 514 ]) st = env . from_string ( # Make sure you changed index and sourcetype properly 'search _time={{ epoch }} index=epintel host=\"{{ host }}\" sourcetype=\"vmware:cb:protect\"' ) search = st . render ( epoch = epoch , bsd = bsd , host = host ) resultCount , eventCount = splunk_single ( setup_splunk , search ) record_property ( \"host\" , host ) record_property ( \"resultCount\" , resultCount ) record_property ( \"message\" , message ) assert resultCount == 1 NOTE: It is a known issue that the test case will timeout when it starts. When it fails, just re-run it. Now run the test: poetry run pytest test/test_vendor_product.py This test will spin up a Splunk instance on your localhost and forward the parsed message there. Now the parsed log should appear in Splunk: As you can see, at this moment, the message is being parsed as a generic *nix:syslog sourcetype. To assign it to the proper index and sourcetype you will need an actual parser. So far we have ensured that the fields in the messages are properly recognized. Create a parser \u00b6 Your parser needs to be declared in package/etc/conf.d/conflib . The naming convention is app-type-vendor_product.conf . If there is a similar parser existing already you can use it as a reference. In the parser, make sure you assign the proper sourcetype, index, vendor, product, and template. The template tells how your message should be parsed before sending it to Splunk. The most basic configuration will only forward raw log with correct metadata. Here is an example: block parser app-syslog-vmware_cb-protect () { channel { rewrite { r_set_splunk_dest_default ( index ( \"epintel\" ) sourcetype ( 'vmware:cb:protect' ) vendor ( \"vmware\" ) product ( \"cb-protect\" ) template ( \"t_msg_only\" ) ) ; } ; } ; } ; application app-syslog-vmware_cb-protect [ sc4s-syslog ] { filter { message ( 'Carbon Black App Control event: ' type ( string ) flags ( prefix )) ; } ; parser { app-syslog-vmware_cb-protect () ; } ; } ; Now all messages that start with the string Carbon Black App Control event: will be routed to the proper index and assigned the given sourcetype: For more info about using message filtering go to sources documentation. If you wish to apply more transformations you will need to add the parser: block parser app-syslog-vmware_cb-protect () { channel { rewrite { r_set_splunk_dest_default ( index ( \"epintel\" ) sourcetype ( 'vmware:cb:protect' ) vendor ( \"vmware\" ) product ( \"cb-protect\" ) template ( \"t_kv_values\" ) ) ; } ; parser { csv-parser ( delimiters ( chars ( '' ) strings ( ': ' )) columns ( 'header' , 'message' ) prefix ( '.tmp.' ) flags ( greedy, drop-invalid )) ; kv-parser ( prefix ( \".values.\" ) pair-separator ( \" \" ) template ( '${.tmp.message}' ) ) ; } ; } ; } ; application app-syslog-vmware_cb-protect [ sc4s-syslog ] { filter { message ( 'Carbon Black App Control event: ' type ( string ) flags ( prefix )) ; } ; parser { app-syslog-vmware_cb-protect () ; } ; } ; In this case, we will extract all fields that are nested in the raw log message first by using csv-parser to split Carbon Black App Control event and the rest of message as a two separate fields named header and message . On top of that, we will use kv-parser to extract all key-value pairs in the message field. The best way to test your parser is to run a previously created test case. If you need more debugging, use docker ps to see running containers, and docker logs to see what\u2019s happening to the parsed message. Once you are content with the results, you can commit your changes and open pull request.","title":"Create a parser"},{"location":"gettingstarted/create-parser/#create-a-parser","text":"The following is a step-by-step guide for adding new parsers.","title":"Create a parser"},{"location":"gettingstarted/create-parser/#why-create-a-parser","text":"Splunk Connect for Syslog can offload Splunk Indexers by performing operations that normally would have been done during index time, including linebreaking, source/sourcetype setting, and timestamping. Creating a parser also reduces the need of using corresponding add-ons on indexers.","title":"Why create a parser?"},{"location":"gettingstarted/create-parser/#before-you-start","text":"Make sure you have read contribution standards . For more background information on how filters and parser work, and what suits you best, read about sources onboarding . Prepare your environment . Create a new branch in the repository where you will apply your changes.","title":"Before you start"},{"location":"gettingstarted/create-parser/#start-with-a-raw-log-message","text":"If you already have a raw log message, you can skip this step. Otherwise, you need to extract one to have something to work with. You can do this in multiple ways; here is a brief description of two of them:","title":"Start with a raw log message"},{"location":"gettingstarted/create-parser/#tcpdump","text":"You can use the tcpdump command to get incoming raw messages on a given port of your server. tcpdump -n -s 0 -S -i any -v port 8088 tcpdump: listening on any, link-type LINUX_SLL ( Linux cooked ) , capture size 262144 bytes 09 :54:26.051644 IP ( tos 0x0, ttl 64 , id 29465 , offset 0 , flags [ DF ] , proto UDP ( 17 ) , length 466 ) 10 .202.22.239.41151 > 10 .202.33.242.syslog: SYSLOG, length: 438 Facility local0 ( 16 ) , Severity info ( 6 ) Msg: 2022 -04-28T16:16:15.466731-04:00 NTNX-21SM6M510425-B-CVM audispd [ 32075 ] : node = ntnx-21sm6m510425-b-cvm type = SYSCALL msg = audit ( 1651176975 .464:2828209 ) : arch = c000003e syscall = 2 success = yes exit = 6 a0 = 7f2955ac932e a1 = 2 a2 = 3e8 a3 = 3 items = 1 ppid = 29680 pid = 4684 auid = 1000 uid = 0 gid = 0 euid = 0 suid = 0 fsuid = 0 egid = 0 sgid = 0 fsgid = 0 tty =( none ) ses = 964698 comm = \u201csshd\u201d exe = \u201c/usr/sbin/sshd\u201d subj = system_u:system_r:sshd_t:s0-s0:c0.c1023 key = \u201clogins\u201d \\0 x0a","title":"tcpdump"},{"location":"gettingstarted/create-parser/#wireshark","text":"Or you can read the logs using Wireshark from the .pcap file. From Wireshark go to Statistics->Conversations, then click on \u2018Follow Stream\u2019. Once you get your stream of messages, copy one of them. NOTE: In UDP there usually will not be any message separators.","title":"Wireshark"},{"location":"gettingstarted/create-parser/#create-a-unit-test","text":"It is recommended to use the existing test case that is the most similar to your use case. The naming convention is test_vendor_product.py Afterwards, you need to make sure that your log is being parsed correctly by creating a test case. Assuming you have a raw message like this: < 14 >1 2022 -03-30T11:17:11.900862-04:00 host - - - - Carbon Black App Control event: text = \"File 'c:\\program files\\azure advanced threat protection sensor\\2.175.15073.51407\\winpcap\\x86\\packet.dll' [c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363] would have blocked if the rule was not in Report Only mode.\" type = \"Policy Enforcement\" subtype = \"Execution block (unapproved file)\" hostname = \"CORP\\USER\" username = \"NT AUTHORITY\\SYSTEM\" date = \"3/30/2022 3:16:40 PM\" ip_address = \"10.0.0.3\" process = \"c:\\program files\\azure advanced threat protection sensor\\2.175.15073.51407\\microsoft.tri.sensor.updater.exe\" file_path = \"c:\\program files\\azure advanced threat protection sensor\\2.175.15073.51407\\winpcap\\x86\\packet.dll\" file_name = \"packet.dll\" file_hash = \"c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363\" policy = \"High Enforcement - Domain Controllers\" rule_name = \"Report read-only memory map operations on unapproved executables by .NET applications\" process_key = \"00000433-0000-23d8-01d8-44491b26f203\" server_version = \"8.5.4.3\" file_trust = \"-2\" file_threat = \"-2\" process_trust = \"-2\" process_threat = \"-2\" prevalence = \"50\" You need to: * make sure that the message is a valid python string, where escape characters are placed correctly. * anonymize the data. * rename functions. * update index, and sourcetype fields. * extract replace values with field names in test string. Here you can see proper test case for Vmware Carbonblack Protect device: # Copyright 2019 Splunk, Inc. # # Use of this source code is governed by a BSD-2-clause-style # license that can be found in the LICENSE-BSD2 file or at # https://opensource.org/licenses/BSD-2-Clause import random from jinja2 import Environment from .sendmessage import * from .splunkutils import * from .timeutils import * env = Environment () # Below is a raw message # <14>1 2022-03-30T11:17:11.900862-04:00 host - - - - Carbon Black App Control event: text=\"File 'c:\\program files\\azure advanced threat protection sensor\\0.0.0.0\\winpcap\\x86\\packet.dll' [c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363] would have blocked if the rule was not in Report Only mode.\" type=\"Policy Enforcement\" subtype=\"Execution block (unapproved file)\" hostname=\"CORP\\USER\" username=\"NT AUTHORITY\\SYSTEM\" date=\"3/30/2022 3:16:40 PM\" ip_address=\"0.0.0.0\" process=\"c:\\program files\\azure advanced threat protection sensor\\0.0.0.0\\microsoft.tri.sensor.updater.exe\" file_path=\"c:\\program files\\azure advanced threat protection sensor\\0.0.0.0\\winpcap\\x86\\packet.dll\" file_name=\"packet.dll\" file_hash=\"c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363\" policy=\"High Enforcement - Domain Controllers\" rule_name=\"Report read-only memory map operations on unapproved executables by .NET applications\" process_key=\"00000433-0000-23d8-01d8-44491b26f203\" server_version=\"0.0.0.0\" file_trust=\"-2\" file_threat=\"-2\" process_trust=\"-2\" process_threat=\"-2\" prevalence=\"50\" # Don't forget to rename the function def test_vmware_carbonblack_protect ( record_property , setup_wordlist , setup_splunk , setup_sc4s ): host = \" {} - {} \" . format ( random . choice ( setup_wordlist ), random . choice ( setup_wordlist )) dt = datetime . datetime . now () iso , bsd , time , date , tzoffset , tzname , epoch = time_operations ( dt ) # Tune time functions for Checkpoint epoch = epoch [: - 3 ] mt = env . from_string ( # Extract mark, iso timestamp and host fields # Make sure all needed characters are escaped # If string contains single quotes wrap it in double qutes '{{ mark }} {{ iso }} {{ host }} - - - - Carbon Black App Control event: text=\"File \\' c: \\\\ program files \\\\ azure advanced threat protection sensor \\\\ 0.0.0.0 \\\\ winpcap \\\\ x86 \\\\ packet.dll \\' [c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363] would have blocked if the rule was not in Report Only mode.\" type=\"Policy Enforcement\" subtype=\"Execution block (unapproved file)\" hostname=\"CORP \\\\ USER\" username=\"NT AUTHORITY \\\\ SYSTEM\" date=\"3/30/2022 3:16:40 PM\" ip_address=\"0.0.0.0\" process=\"c: \\\\ program files \\\\ azure advanced threat protection sensor \\\\ 0.0.0.0 \\\\ microsoft.tri.sensor.updater.exe\" file_path=\"c: \\\\ program files \\\\ azure advanced threat protection sensor \\\\ 0.0.0.0 \\\\ winpcap \\\\ x86 \\\\ packet.dll\" file_name=\"packet.dll\" file_hash=\"c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363\" policy=\"High Enforcement - Domain Controllers\" rule_name=\"Report read-only memory map operations on unapproved executables by .NET applications\" process_key=\"00000433-0000-23d8-01d8-44491b26f203\" server_version=\"0.0.0.0\" file_trust=\"-2\" file_threat=\"-2\" process_trust=\"-2\" process_threat=\"-2\" prevalence=\"50\"' ) message = mt . render ( mark = \"<134>1\" , host = host , bsd = bsd , iso = iso , epoch = epoch ) sendsingle ( message , setup_sc4s [ 0 ], setup_sc4s [ 1 ][ 514 ]) st = env . from_string ( # Make sure you changed index and sourcetype properly 'search _time={{ epoch }} index=epintel host=\"{{ host }}\" sourcetype=\"vmware:cb:protect\"' ) search = st . render ( epoch = epoch , bsd = bsd , host = host ) resultCount , eventCount = splunk_single ( setup_splunk , search ) record_property ( \"host\" , host ) record_property ( \"resultCount\" , resultCount ) record_property ( \"message\" , message ) assert resultCount == 1 NOTE: It is a known issue that the test case will timeout when it starts. When it fails, just re-run it. Now run the test: poetry run pytest test/test_vendor_product.py This test will spin up a Splunk instance on your localhost and forward the parsed message there. Now the parsed log should appear in Splunk: As you can see, at this moment, the message is being parsed as a generic *nix:syslog sourcetype. To assign it to the proper index and sourcetype you will need an actual parser. So far we have ensured that the fields in the messages are properly recognized.","title":"Create a unit test"},{"location":"gettingstarted/create-parser/#create-a-parser_1","text":"Your parser needs to be declared in package/etc/conf.d/conflib . The naming convention is app-type-vendor_product.conf . If there is a similar parser existing already you can use it as a reference. In the parser, make sure you assign the proper sourcetype, index, vendor, product, and template. The template tells how your message should be parsed before sending it to Splunk. The most basic configuration will only forward raw log with correct metadata. Here is an example: block parser app-syslog-vmware_cb-protect () { channel { rewrite { r_set_splunk_dest_default ( index ( \"epintel\" ) sourcetype ( 'vmware:cb:protect' ) vendor ( \"vmware\" ) product ( \"cb-protect\" ) template ( \"t_msg_only\" ) ) ; } ; } ; } ; application app-syslog-vmware_cb-protect [ sc4s-syslog ] { filter { message ( 'Carbon Black App Control event: ' type ( string ) flags ( prefix )) ; } ; parser { app-syslog-vmware_cb-protect () ; } ; } ; Now all messages that start with the string Carbon Black App Control event: will be routed to the proper index and assigned the given sourcetype: For more info about using message filtering go to sources documentation. If you wish to apply more transformations you will need to add the parser: block parser app-syslog-vmware_cb-protect () { channel { rewrite { r_set_splunk_dest_default ( index ( \"epintel\" ) sourcetype ( 'vmware:cb:protect' ) vendor ( \"vmware\" ) product ( \"cb-protect\" ) template ( \"t_kv_values\" ) ) ; } ; parser { csv-parser ( delimiters ( chars ( '' ) strings ( ': ' )) columns ( 'header' , 'message' ) prefix ( '.tmp.' ) flags ( greedy, drop-invalid )) ; kv-parser ( prefix ( \".values.\" ) pair-separator ( \" \" ) template ( '${.tmp.message}' ) ) ; } ; } ; } ; application app-syslog-vmware_cb-protect [ sc4s-syslog ] { filter { message ( 'Carbon Black App Control event: ' type ( string ) flags ( prefix )) ; } ; parser { app-syslog-vmware_cb-protect () ; } ; } ; In this case, we will extract all fields that are nested in the raw log message first by using csv-parser to split Carbon Black App Control event and the rest of message as a two separate fields named header and message . On top of that, we will use kv-parser to extract all key-value pairs in the message field. The best way to test your parser is to run a previously created test case. If you need more debugging, use docker ps to see running containers, and docker logs to see what\u2019s happening to the parsed message. Once you are content with the results, you can commit your changes and open pull request.","title":"Create a parser"},{"location":"gettingstarted/docker-compose-MacOS/","text":"Install Docker Desktop for MacOS \u00b6 Refer to Installation SC4S Initial Configuration \u00b6 SC4S can be run with docker-compose or directly from the CLI with the simple docker run command. Both options are outlined below. Create a directory on the server for local configurations and disk buffering. This should be available to all administrators, for example: /opt/sc4s/ (Optional for docker-compose ) Create a docker-compose.yml file in the directory created above, based on the template below: IMPORTANT: Always use the latest compose file (below) with the current release. By default, the latest container is automatically downloaded at each restart. Therefore, make it a habit to check back here regularly to be sure any changes that may have been made to the compose template file below (e.g. suggested mount points) are incorporated in production prior to relaunching via compose. version : \"3.7\" services : sc4s : deploy : replicas : 2 restart_policy : condition : on-failure image : ghcr.io/splunk/splunk-connect-for-syslog/container2:2 ports : - target : 514 published : 514 protocol : tcp - target : 514 published : 514 protocol : udp - target : 601 published : 601 protocol : tcp - target : 6514 published : 6514 protocol : tcp env_file : - /opt/sc4s/env_file volumes : - /opt/sc4s/local:/etc/syslog-ng/conf.d/local:z - splunk-sc4s-var:/var/lib/syslog-ng # Uncomment the following line if local disk archiving is desired # - /opt/sc4s/archive:/var/lib/syslog-ng/archive:z # Map location of TLS custom TLS # - /opt/sc4s/tls:/etc/syslog-ng/tls:z volumes : splunk-sc4s-var : Set /opt/sc4s folder as shared in Docker (Settings -> Resources -> File Sharing) Execute the following command to create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destination(s). This will also be used to keep track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. This is a required step. sudo docker volume create splunk - sc4s - var NOTE: Be sure to account for disk space requirements for the docker volume created above. This volume is located in /var/lib/docker/volumes/ and could grow significantly if there is an extended outage to the SC4S destinations (typically HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. IMPORTANT: When creating the directories below, ensure the directories created match the volume mounts specified in the docker-compose.yml file (if used). Failure to do this will cause SC4S to abort at startup. Create subdirectories /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls Create a file named /opt/sc4s/env_file and add the following environment variables and values: SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // your . splunk . instance : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx - xxxx - xxxx - xxxx - xxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Update SC4S_DEST_SPLUNK_HEC_DEFAULT_URL and SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN to reflect the correct values for your environment. Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. The default number of SC4S_DEST_SPLUNK_HEC_WORKERS is 10. Consult the community if you feel the number of workers (threads) should deviate from this. NOTE: Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above. Dedicated (Unique) Listening Ports \u00b6 NOTE: Container networking differs on MacOS compared to that for linux. On Docker Desktop, there is no \u201chost\u201d networking driver, so NAT networking must be used. For this reason, each listening port on the container must be mapped to a listening port on the host. These port mappings are configured in the docker-compose.yml file or directly as a runtime option when run out of the CLI. Be sure to update the docker-compose.yml file or CLI arguments when adding listening ports for new data sources. Follow these steps to configure unique ports: Modify the /opt/sc4s/env_file file to include the port-specific environment variable(s). Refer to the \u201cSources\u201d documentation to identify the specific environment variables that are mapped to each data source vendor/technology. (Optional for docker-compose ) The docker compose file used to start the SC4S container needs to be modified as well to reflect the additional listening ports configured by the environment variable(s) added above. The docker compose file can be amended with additional target stanzas in the ports section of the file (after the default ports). For example, the following additional target and published lines provide for 21 additional technology-specific UDP and TCP ports: - target: 5000-5020 published: 5000-5020 protocol: tcp - target: 5000-5020 published: 5000-5020 protocol: udp Restart SC4S using the command in the \u201cStart/Restart SC4S\u201d section below. For more information about configuration refer to Docker and Podman basic configurations and detailed configuration . Start/Restart SC4S \u00b6 You can use the following command to directly start SC4S if you are not using docker-compose . Be sure to map the listening ports ( -p arguments) according to your needs: /usr/bin/podman run -p 514:514 -p 514:514/udp -p 6514:6514 -p 5000-5020:5000-5020 -p 5000-5020:5000-5020/udp \\ --env-file=/opt/sc4s/env_file \\ --name SC4S \\ --rm splunk/scs:latest If you are using docker-compose , from the catalog where you created compose file execute: docker-compose up Otherwise use docker-compose with -f flag pointing to the compose file docker-compose up -f /path/to/compose/file/docker-compose.yml Stop SC4S \u00b6 If the container is run directly from the CLI, simply stop the container using the docker stop <containerID> command. If using docker-compose , execute: docker-compose down or docker-compose down -f /path/to/compose/file/docker-compose.yml Verify Proper Operation \u00b6 SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container. docker logs SC4S You should see events similar to those below in the output: syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng If you do not see the output above, proceed to the \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information.","title":"Docker Desktop + Compose (MacOS)"},{"location":"gettingstarted/docker-compose-MacOS/#install-docker-desktop-for-macos","text":"Refer to Installation","title":"Install Docker Desktop for MacOS"},{"location":"gettingstarted/docker-compose-MacOS/#sc4s-initial-configuration","text":"SC4S can be run with docker-compose or directly from the CLI with the simple docker run command. Both options are outlined below. Create a directory on the server for local configurations and disk buffering. This should be available to all administrators, for example: /opt/sc4s/ (Optional for docker-compose ) Create a docker-compose.yml file in the directory created above, based on the template below: IMPORTANT: Always use the latest compose file (below) with the current release. By default, the latest container is automatically downloaded at each restart. Therefore, make it a habit to check back here regularly to be sure any changes that may have been made to the compose template file below (e.g. suggested mount points) are incorporated in production prior to relaunching via compose. version : \"3.7\" services : sc4s : deploy : replicas : 2 restart_policy : condition : on-failure image : ghcr.io/splunk/splunk-connect-for-syslog/container2:2 ports : - target : 514 published : 514 protocol : tcp - target : 514 published : 514 protocol : udp - target : 601 published : 601 protocol : tcp - target : 6514 published : 6514 protocol : tcp env_file : - /opt/sc4s/env_file volumes : - /opt/sc4s/local:/etc/syslog-ng/conf.d/local:z - splunk-sc4s-var:/var/lib/syslog-ng # Uncomment the following line if local disk archiving is desired # - /opt/sc4s/archive:/var/lib/syslog-ng/archive:z # Map location of TLS custom TLS # - /opt/sc4s/tls:/etc/syslog-ng/tls:z volumes : splunk-sc4s-var : Set /opt/sc4s folder as shared in Docker (Settings -> Resources -> File Sharing) Execute the following command to create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destination(s). This will also be used to keep track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. This is a required step. sudo docker volume create splunk - sc4s - var NOTE: Be sure to account for disk space requirements for the docker volume created above. This volume is located in /var/lib/docker/volumes/ and could grow significantly if there is an extended outage to the SC4S destinations (typically HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. IMPORTANT: When creating the directories below, ensure the directories created match the volume mounts specified in the docker-compose.yml file (if used). Failure to do this will cause SC4S to abort at startup. Create subdirectories /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls Create a file named /opt/sc4s/env_file and add the following environment variables and values: SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // your . splunk . instance : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx - xxxx - xxxx - xxxx - xxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Update SC4S_DEST_SPLUNK_HEC_DEFAULT_URL and SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN to reflect the correct values for your environment. Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. The default number of SC4S_DEST_SPLUNK_HEC_WORKERS is 10. Consult the community if you feel the number of workers (threads) should deviate from this. NOTE: Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above.","title":"SC4S Initial Configuration"},{"location":"gettingstarted/docker-compose-MacOS/#dedicated-unique-listening-ports","text":"NOTE: Container networking differs on MacOS compared to that for linux. On Docker Desktop, there is no \u201chost\u201d networking driver, so NAT networking must be used. For this reason, each listening port on the container must be mapped to a listening port on the host. These port mappings are configured in the docker-compose.yml file or directly as a runtime option when run out of the CLI. Be sure to update the docker-compose.yml file or CLI arguments when adding listening ports for new data sources. Follow these steps to configure unique ports: Modify the /opt/sc4s/env_file file to include the port-specific environment variable(s). Refer to the \u201cSources\u201d documentation to identify the specific environment variables that are mapped to each data source vendor/technology. (Optional for docker-compose ) The docker compose file used to start the SC4S container needs to be modified as well to reflect the additional listening ports configured by the environment variable(s) added above. The docker compose file can be amended with additional target stanzas in the ports section of the file (after the default ports). For example, the following additional target and published lines provide for 21 additional technology-specific UDP and TCP ports: - target: 5000-5020 published: 5000-5020 protocol: tcp - target: 5000-5020 published: 5000-5020 protocol: udp Restart SC4S using the command in the \u201cStart/Restart SC4S\u201d section below. For more information about configuration refer to Docker and Podman basic configurations and detailed configuration .","title":"Dedicated (Unique) Listening Ports"},{"location":"gettingstarted/docker-compose-MacOS/#startrestart-sc4s","text":"You can use the following command to directly start SC4S if you are not using docker-compose . Be sure to map the listening ports ( -p arguments) according to your needs: /usr/bin/podman run -p 514:514 -p 514:514/udp -p 6514:6514 -p 5000-5020:5000-5020 -p 5000-5020:5000-5020/udp \\ --env-file=/opt/sc4s/env_file \\ --name SC4S \\ --rm splunk/scs:latest If you are using docker-compose , from the catalog where you created compose file execute: docker-compose up Otherwise use docker-compose with -f flag pointing to the compose file docker-compose up -f /path/to/compose/file/docker-compose.yml","title":"Start/Restart SC4S"},{"location":"gettingstarted/docker-compose-MacOS/#stop-sc4s","text":"If the container is run directly from the CLI, simply stop the container using the docker stop <containerID> command. If using docker-compose , execute: docker-compose down or docker-compose down -f /path/to/compose/file/docker-compose.yml","title":"Stop SC4S"},{"location":"gettingstarted/docker-compose-MacOS/#verify-proper-operation","text":"SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container. docker logs SC4S You should see events similar to those below in the output: syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng If you do not see the output above, proceed to the \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information.","title":"Verify Proper Operation"},{"location":"gettingstarted/docker-compose/","text":"Install Docker Desktop \u00b6 Refer to Installation SC4S Initial Configuration \u00b6 SC4S can be run with docker-compose or directly from the CLI with the simple docker run command. Both options are outlined below. Create a directory on the server for local configurations and disk buffering. This should be available to all administrators, for example: /opt/sc4s/ (Optional for docker-compose ) Create a docker-compose.yml file in the directory created above, based on the template below: IMPORTANT: Always use the latest compose file (below) with the current release. By default, the latest container is automatically downloaded at each restart. Therefore, make it a habit to check back here regularly to be sure any changes that may have been made to the compose template file below (e.g. suggested mount points) are incorporated in production prior to relaunching via compose. version : \"3.7\" services : sc4s : deploy : replicas : 2 restart_policy : condition : on-failure image : ghcr.io/splunk/splunk-connect-for-syslog/container2:2 ports : - target : 514 published : 514 protocol : tcp - target : 514 published : 514 protocol : udp - target : 601 published : 601 protocol : tcp - target : 6514 published : 6514 protocol : tcp env_file : - /opt/sc4s/env_file volumes : - /opt/sc4s/local:/etc/syslog-ng/conf.d/local:z - splunk-sc4s-var:/var/lib/syslog-ng # Uncomment the following line if local disk archiving is desired # - /opt/sc4s/archive:/var/lib/syslog-ng/archive:z # Map location of TLS custom TLS # - /opt/sc4s/tls:/etc/syslog-ng/tls:z volumes : splunk-sc4s-var : Set /opt/sc4s folder as shared in Docker (Settings -> Resources -> File Sharing) Execute the following command to create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destination(s). This will also be used to keep track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. This is a required step. sudo docker volume create splunk - sc4s - var NOTE: Be sure to account for disk space requirements for the docker volume created above. This volume is located in /var/lib/docker/volumes/ and could grow significantly if there is an extended outage to the SC4S destinations (typically HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. IMPORTANT: When creating the directories below, ensure the directories created match the volume mounts specified in the docker-compose.yml file (if used). Failure to do this will cause SC4S to abort at startup. Create subdirectories /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls Create a file named /opt/sc4s/env_file and add the following environment variables and values: SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // your . splunk . instance : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx - xxxx - xxxx - xxxx - xxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Update SC4S_DEST_SPLUNK_HEC_DEFAULT_URL and SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN to reflect the correct values for your environment. Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. The default number of SC4S_DEST_SPLUNK_HEC_WORKERS is 10. Consult the community if you feel the number of workers (threads) should deviate from this. NOTE: Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above. For more information about configuration refer to Docker and Podman basic configurations and detailed configuration . Start/Restart SC4S \u00b6 You can use the following command to directly start SC4S if you are not using docker-compose . Be sure to map the listening ports ( -p arguments) according to your needs: /usr/bin/podman run -p 514 :514 -p 514 :514/udp -p 6514 :6514 -p 5000 -5020:5000-5020 -p 5000 -5020:5000-5020/udp \\ --env-file = /opt/sc4s/env_file \\ --name SC4S \\ --rm splunk/scs:latest If you are using docker compose , from the catalog where you created compose file execute: docker compose up Otherwise use docker compose with -f flag pointing to the compose file docker compose up -f /path/to/compose/file/docker-compose.yml Stop SC4S \u00b6 If the container is run directly from the CLI, simply stop the container using the docker stop <containerID> command. If using docker compose , execute: docker compose down or docker compose down -f /path/to/compose/file/docker-compose.yml Verify Proper Operation \u00b6 SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container. docker logs SC4S You should see events similar to those below in the output: syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng If you do not see the output above, proceed to the \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information.","title":"Docker Compose"},{"location":"gettingstarted/docker-compose/#install-docker-desktop","text":"Refer to Installation","title":"Install Docker Desktop"},{"location":"gettingstarted/docker-compose/#sc4s-initial-configuration","text":"SC4S can be run with docker-compose or directly from the CLI with the simple docker run command. Both options are outlined below. Create a directory on the server for local configurations and disk buffering. This should be available to all administrators, for example: /opt/sc4s/ (Optional for docker-compose ) Create a docker-compose.yml file in the directory created above, based on the template below: IMPORTANT: Always use the latest compose file (below) with the current release. By default, the latest container is automatically downloaded at each restart. Therefore, make it a habit to check back here regularly to be sure any changes that may have been made to the compose template file below (e.g. suggested mount points) are incorporated in production prior to relaunching via compose. version : \"3.7\" services : sc4s : deploy : replicas : 2 restart_policy : condition : on-failure image : ghcr.io/splunk/splunk-connect-for-syslog/container2:2 ports : - target : 514 published : 514 protocol : tcp - target : 514 published : 514 protocol : udp - target : 601 published : 601 protocol : tcp - target : 6514 published : 6514 protocol : tcp env_file : - /opt/sc4s/env_file volumes : - /opt/sc4s/local:/etc/syslog-ng/conf.d/local:z - splunk-sc4s-var:/var/lib/syslog-ng # Uncomment the following line if local disk archiving is desired # - /opt/sc4s/archive:/var/lib/syslog-ng/archive:z # Map location of TLS custom TLS # - /opt/sc4s/tls:/etc/syslog-ng/tls:z volumes : splunk-sc4s-var : Set /opt/sc4s folder as shared in Docker (Settings -> Resources -> File Sharing) Execute the following command to create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destination(s). This will also be used to keep track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. This is a required step. sudo docker volume create splunk - sc4s - var NOTE: Be sure to account for disk space requirements for the docker volume created above. This volume is located in /var/lib/docker/volumes/ and could grow significantly if there is an extended outage to the SC4S destinations (typically HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. IMPORTANT: When creating the directories below, ensure the directories created match the volume mounts specified in the docker-compose.yml file (if used). Failure to do this will cause SC4S to abort at startup. Create subdirectories /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls Create a file named /opt/sc4s/env_file and add the following environment variables and values: SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // your . splunk . instance : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx - xxxx - xxxx - xxxx - xxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Update SC4S_DEST_SPLUNK_HEC_DEFAULT_URL and SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN to reflect the correct values for your environment. Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. The default number of SC4S_DEST_SPLUNK_HEC_WORKERS is 10. Consult the community if you feel the number of workers (threads) should deviate from this. NOTE: Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above. For more information about configuration refer to Docker and Podman basic configurations and detailed configuration .","title":"SC4S Initial Configuration"},{"location":"gettingstarted/docker-compose/#startrestart-sc4s","text":"You can use the following command to directly start SC4S if you are not using docker-compose . Be sure to map the listening ports ( -p arguments) according to your needs: /usr/bin/podman run -p 514 :514 -p 514 :514/udp -p 6514 :6514 -p 5000 -5020:5000-5020 -p 5000 -5020:5000-5020/udp \\ --env-file = /opt/sc4s/env_file \\ --name SC4S \\ --rm splunk/scs:latest If you are using docker compose , from the catalog where you created compose file execute: docker compose up Otherwise use docker compose with -f flag pointing to the compose file docker compose up -f /path/to/compose/file/docker-compose.yml","title":"Start/Restart SC4S"},{"location":"gettingstarted/docker-compose/#stop-sc4s","text":"If the container is run directly from the CLI, simply stop the container using the docker stop <containerID> command. If using docker compose , execute: docker compose down or docker compose down -f /path/to/compose/file/docker-compose.yml","title":"Stop SC4S"},{"location":"gettingstarted/docker-compose/#verify-proper-operation","text":"SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container. docker logs SC4S You should see events similar to those below in the output: syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng If you do not see the output above, proceed to the \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information.","title":"Verify Proper Operation"},{"location":"gettingstarted/docker-podman-offline/","text":"Offline Container Installation \u00b6 Follow these instructions to \u201cstage\u201d SC4S by downloading the container so that it can be loaded \u201cout of band\u201d on a host machine, such as an airgapped system, without internet connectivity. Download container image \u201coci_container.tgz\u201d from our Github Page . The following example downloads v1.12; replace the URL with the latest release or pre-release version as desired. sudo wget https : // github . com / splunk / splunk - connect - for - syslog / releases / download / v1 . 12.0 / oci_container . tar . gz Distribute the container to the airgapped host machine using an appropriate file transfer utility. Execute the following command, using docker or podman as appropriate < podman or docker > load < oci_container . tar . gz Note the container ID of the resultant load Loaded image : docker . pkg . github . com / splunk / splunk - connect - for - syslog / ci : 90196 f77f7525bc55b3b966b5fa1ce74861c0250 Use the container ID to create a local label < podman or docker > tag docker . pkg . github . com / splunk / splunk - connect - for - syslog / ci : 90196 f77f7525bc55b3b966b5fa1ce74861c0250 sc4slocal : latest Use this local label sc4slocal:latest in the relevant unit or yaml file to launch SC4S (see the runtime options above) by setting the SC4S_IMAGE environment variable in the unit file (example below), or the relevant image: tag if using Docker Compose/Swarm. Using this label will cause the runtime to select the locally loaded image, and will not attempt to obtain the container image via the internet. Environment=\"SC4S_IMAGE=sc4slocal:latest\" Remove the entry ExecStartPre=/usr/bin/docker pull $SC4S_IMAGE from the relevant unit file when using systemd, as an external connection to pull the container is no longer needed (or available).","title":"Docker&Podman offline installation"},{"location":"gettingstarted/docker-podman-offline/#offline-container-installation","text":"Follow these instructions to \u201cstage\u201d SC4S by downloading the container so that it can be loaded \u201cout of band\u201d on a host machine, such as an airgapped system, without internet connectivity. Download container image \u201coci_container.tgz\u201d from our Github Page . The following example downloads v1.12; replace the URL with the latest release or pre-release version as desired. sudo wget https : // github . com / splunk / splunk - connect - for - syslog / releases / download / v1 . 12.0 / oci_container . tar . gz Distribute the container to the airgapped host machine using an appropriate file transfer utility. Execute the following command, using docker or podman as appropriate < podman or docker > load < oci_container . tar . gz Note the container ID of the resultant load Loaded image : docker . pkg . github . com / splunk / splunk - connect - for - syslog / ci : 90196 f77f7525bc55b3b966b5fa1ce74861c0250 Use the container ID to create a local label < podman or docker > tag docker . pkg . github . com / splunk / splunk - connect - for - syslog / ci : 90196 f77f7525bc55b3b966b5fa1ce74861c0250 sc4slocal : latest Use this local label sc4slocal:latest in the relevant unit or yaml file to launch SC4S (see the runtime options above) by setting the SC4S_IMAGE environment variable in the unit file (example below), or the relevant image: tag if using Docker Compose/Swarm. Using this label will cause the runtime to select the locally loaded image, and will not attempt to obtain the container image via the internet. Environment=\"SC4S_IMAGE=sc4slocal:latest\" Remove the entry ExecStartPre=/usr/bin/docker pull $SC4S_IMAGE from the relevant unit file when using systemd, as an external connection to pull the container is no longer needed (or available).","title":"Offline Container Installation"},{"location":"gettingstarted/docker-systemd-general/","text":"Install Docker CE \u00b6 Refer to relevant installation guides: CentOS Ubuntu Debian NOTE: READ FIRST (IPv4 forwarding) Initial Setup \u00b6 IMPORTANT: Always use the latest unit file (below) with the current release. By default, the latest container is automatically downloaded at each restart. Therefore, make it a habit to check back here regularly to be sure any changes that may have been made to the template unit file below (e.g. suggested mount points) are incorporated in production prior to relaunching via systemd. Create the systemd unit file /lib/systemd/system/sc4s.service based on the following template: Unit file \u00b6 [Unit] Description = SC4S Container Wants = NetworkManager.service network-online.target docker.service After = NetworkManager.service network-online.target docker.service Requires = docker.service [Install] WantedBy = multi-user.target [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2\" # Required mount point for syslog-ng persist data (including disk buffer) Environment = \"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\" # Optional mount point for local overrides and configurations; see notes in docs Environment = \"SC4S_LOCAL_MOUNT=/opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\" # Optional mount point for local disk archive (EWMM output) files Environment = \"SC4S_ARCHIVE_MOUNT=/opt/sc4s/archive:/var/lib/syslog-ng/archive:z\" # Map location of TLS custom TLS Environment = \"SC4S_TLS_MOUNT=/opt/sc4s/tls:/etc/syslog-ng/tls:z\" TimeoutStartSec = 0 ExecStartPre = /usr/bin/docker pull $SC4S_IMAGE # Note: /usr/bin/bash will not be valid path for all OS # when startup fails on running bash check if the path is correct ExecStartPre = /usr/bin/bash -c \"/usr/bin/systemctl set-environment SC4SHOST=$(hostname -s)\" ExecStart = /usr/bin/docker run \\ -e \"SC4S_CONTAINER_HOST = ${SC4SHOST}\" \\ -v \"$SC4S_PERSIST_MOUNT\" \\ -v \"$SC4S_LOCAL_MOUNT\" \\ -v \"$SC4S_ARCHIVE_MOUNT\" \\ -v \"$SC4S_TLS_MOUNT\" \\ --env-file = /opt/sc4s/env_file \\ --network host \\ --name SC4S \\ --rm $SC4S_IMAGE Restart = on-abnormal Execute the following command to create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destination(s). This will also be used to keep track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. This is a required step. sudo docker volume create splunk - sc4s - var NOTE: Be sure to account for disk space requirements for the docker volume created above. This volume is located in /var/lib/docker/volumes/ and could grow significantly if there is an extended outage to the SC4S destinations (typically HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. Create subdirectories /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls Create a file named /opt/sc4s/env_file and add the following environment variables and values: SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // your . splunk . instance : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx - xxxx - xxxx - xxxx - xxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Update SC4S_DEST_SPLUNK_HEC_DEFAULT_URL and SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN to reflect the correct values for your environment. Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. The default number of SC4S_DEST_SPLUNK_HEC_WORKERS is 10. Consult the community if you feel the number of workers (threads) should deviate from this. NOTE: Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above. For more information about configuration refer to Docker and Podman basic configurations and detailed configuration . Configure SC4S for systemd and start SC4S \u00b6 sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s Restart SC4S \u00b6 sudo systemctl restart sc4s If changes were made to the configuration Unit file above (e.g. to configure with dedicated ports), you must first stop SC4S and re-run the systemd configuration commands: sudo systemctl stop sc4s sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s Stop SC4S \u00b6 sudo systemctl stop sc4s Verify Proper Operation \u00b6 SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container. docker logs SC4S You should see events similar to those below in the output: syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng If you do not see the output above, proceed to the \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information.","title":"Docker CE + systemd"},{"location":"gettingstarted/docker-systemd-general/#install-docker-ce","text":"Refer to relevant installation guides: CentOS Ubuntu Debian NOTE: READ FIRST (IPv4 forwarding)","title":"Install Docker CE"},{"location":"gettingstarted/docker-systemd-general/#initial-setup","text":"IMPORTANT: Always use the latest unit file (below) with the current release. By default, the latest container is automatically downloaded at each restart. Therefore, make it a habit to check back here regularly to be sure any changes that may have been made to the template unit file below (e.g. suggested mount points) are incorporated in production prior to relaunching via systemd. Create the systemd unit file /lib/systemd/system/sc4s.service based on the following template:","title":"Initial Setup"},{"location":"gettingstarted/docker-systemd-general/#unit-file","text":"[Unit] Description = SC4S Container Wants = NetworkManager.service network-online.target docker.service After = NetworkManager.service network-online.target docker.service Requires = docker.service [Install] WantedBy = multi-user.target [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2\" # Required mount point for syslog-ng persist data (including disk buffer) Environment = \"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\" # Optional mount point for local overrides and configurations; see notes in docs Environment = \"SC4S_LOCAL_MOUNT=/opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\" # Optional mount point for local disk archive (EWMM output) files Environment = \"SC4S_ARCHIVE_MOUNT=/opt/sc4s/archive:/var/lib/syslog-ng/archive:z\" # Map location of TLS custom TLS Environment = \"SC4S_TLS_MOUNT=/opt/sc4s/tls:/etc/syslog-ng/tls:z\" TimeoutStartSec = 0 ExecStartPre = /usr/bin/docker pull $SC4S_IMAGE # Note: /usr/bin/bash will not be valid path for all OS # when startup fails on running bash check if the path is correct ExecStartPre = /usr/bin/bash -c \"/usr/bin/systemctl set-environment SC4SHOST=$(hostname -s)\" ExecStart = /usr/bin/docker run \\ -e \"SC4S_CONTAINER_HOST = ${SC4SHOST}\" \\ -v \"$SC4S_PERSIST_MOUNT\" \\ -v \"$SC4S_LOCAL_MOUNT\" \\ -v \"$SC4S_ARCHIVE_MOUNT\" \\ -v \"$SC4S_TLS_MOUNT\" \\ --env-file = /opt/sc4s/env_file \\ --network host \\ --name SC4S \\ --rm $SC4S_IMAGE Restart = on-abnormal Execute the following command to create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destination(s). This will also be used to keep track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. This is a required step. sudo docker volume create splunk - sc4s - var NOTE: Be sure to account for disk space requirements for the docker volume created above. This volume is located in /var/lib/docker/volumes/ and could grow significantly if there is an extended outage to the SC4S destinations (typically HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. Create subdirectories /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls Create a file named /opt/sc4s/env_file and add the following environment variables and values: SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // your . splunk . instance : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx - xxxx - xxxx - xxxx - xxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Update SC4S_DEST_SPLUNK_HEC_DEFAULT_URL and SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN to reflect the correct values for your environment. Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. The default number of SC4S_DEST_SPLUNK_HEC_WORKERS is 10. Consult the community if you feel the number of workers (threads) should deviate from this. NOTE: Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above. For more information about configuration refer to Docker and Podman basic configurations and detailed configuration .","title":"Unit file"},{"location":"gettingstarted/docker-systemd-general/#configure-sc4s-for-systemd-and-start-sc4s","text":"sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s","title":"Configure SC4S for systemd and start SC4S"},{"location":"gettingstarted/docker-systemd-general/#restart-sc4s","text":"sudo systemctl restart sc4s If changes were made to the configuration Unit file above (e.g. to configure with dedicated ports), you must first stop SC4S and re-run the systemd configuration commands: sudo systemctl stop sc4s sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s","title":"Restart SC4S"},{"location":"gettingstarted/docker-systemd-general/#stop-sc4s","text":"sudo systemctl stop sc4s","title":"Stop SC4S"},{"location":"gettingstarted/docker-systemd-general/#verify-proper-operation","text":"SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container. docker logs SC4S You should see events similar to those below in the output: syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng If you do not see the output above, proceed to the \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information.","title":"Verify Proper Operation"},{"location":"gettingstarted/getting-started-runtime-configuration/","text":"Implement a Container Runtime and SC4S \u00b6 Prerequisites \u00b6 Linux host with Docker (CE 19.x or greater) or Podman enabled, depending on runtime choice (below). A network load balancer (NLB) configured for round-robin. Note: Special consideration may be required when more advanced products are used. The optimal configuration of the load balancer will round-robin each http POST request (not each connection). The host linux OS receive buffer size should be tuned to match the sc4s default to avoid dropping events (packets) at the network level. The default receive buffer for sc4s is set to 16 MB for UDP traffic, which should be OK for most environments. To set the host OS kernel to match this, edit /etc/sysctl.conf using the following whole-byte values corresponding to 16 MB: net.core.rmem_default = 17039360 net.core.rmem_max = 17039360 and apply to the kernel: sysctl -p Ensure the kernel is not dropping packets by periodically monitoring the buffer with the command netstat -su | grep \"receive errors\" . NOTE: Failure to account for high-volume traffic (especially UDP) by tuning the kernel will result in message loss, which can be very unpredictable and difficult to detect. See this helpful discussion in the syslog-ng Professional Edition documentation regarding tuning syslog-ng in particular (via the SC4S_SOURCE_*_SO_RCVBUFF environment variable in sc4s) as well as overall host kernel tuning. The default values for receive kernel buffers in most distros is 2 MB, which has proven inadequate for many. IPv4 Forwarding \u00b6 In many distributions (e.g. CentOS provisioned in AWS), IPV4 forwarding is not enabled by default. This needs to be enabled for container networking to function properly. The following is an example to check and set this up; as usual this needs to be vetted with your enterprise security policy: To check: sudo sysctl net.ipv4.ip_forward To set: sudo sysctl net.ipv4.ip_forward=1 To ensure the change survives a reboot: sysctl settings are defined through files in /usr/lib/sysctl.d/ , /run/sysctl.d/ , and /etc/sysctl.d/ . To override only specific settings, you can either add a file with a lexically later name in /etc/sysctl.d/ and put following setting there: net.ipv4.ip_forward=1 or find this specific setting in one of existing configuration files (mentioned above) and set value to 1 . net.ipv4.ip_forward=1 Select a Container Runtime and SC4S Configuration \u00b6 Container Runtime and Orchestration Operating Systems MicroK8s Ubuntu with Microk8s Podman 1.7 & 1.9 + systemd RHEL 8.x or CentOS 8.x (best option), Debian or Ubuntu 18.04LTS Docker CE 19 (and greater) + systemd RHEL or CentOS >7.7 (best option), Debian or Ubuntu 18.04LTS Docker Desktop + Compose MacOS Docker Desktop + Compose RHEL or CentOS 8.1 & 8.2 (best option) Bring your own Environment RHEL or CentOS 8.1 & 8.2 (best option) Offline Container Installation RHEL 8.x or CentOS 8.x (best option), Debian or Ubuntu 18.04LTS Ansible+Docker Swarm RHEL 8.x or CentOS 8.x (best option), Debian or Ubuntu 18.04LTS Ansible+Podman RHEL 7.x/8.x or CentOS 7.x/8.x (best option), Debian or Ubuntu 20.10LTS(and higher) Ansible+Docker RHEL 7.x/8.x or CentOS 7.x/8.x (best option), Debian or Ubuntu 18.04LTS(and higher) Docker and Podman basic configurations \u00b6 To run properly sc4s you need to create directories: /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls /opt/sc4s/local will be used as a mount point for local overrides and configurations. The empty local directory created above will populate with defaults and examples at the first invocation of SC4S for local configurations and context overrides. Do not change the directory structure of the files that are laid down; change (or add) only individual files if desired. SC4S depends on the directory layout to read the local configurations properly. See the notes below for which files will be preserved on restarts. In the local/config/ directory there are four subdirectories that allow you to provide support for device types that are not provided out of the box in SC4S. To get you started, there is an example log path template ( lp-example.conf.tmpl ) and a filter ( example.conf ) in the log_paths and filters subdirectories, respectively. These should not be used directly, but copied as templates for your own log path development. They will get overwritten at each SC4S start. In the local/context directory, if you change the \u201cnon-example\u201d version of a file (e.g. splunk_metadata.csv ) the changes will be preserved on a restart. /opt/sc4s/archive will be used as a mount point for local storage of syslog events (if the optional mount is uncommented above). The events will be written in the syslog-ng EWMM format. See the \u201cconfiguration\u201d document for details on the directory structure the archive uses. /opt/sc4s/tls will be used as a mount point for custom TLS certificates (if the optional mount is uncommented above). IMPORTANT: When creating the directories above, ensure the directories created match the volume mounts specified in the sc4s.service unit file . Failure to do this will cause SC4S to abort at startup. Dedicated (Unique) Listening Ports \u00b6 For certain source technologies, categorization by message content is impossible due to the lack of a unique \u201cfingerprint\u201d in the data. In other cases, a unique listening port is required for certain devices due to network requirements in the enterprise. For collection of such sources, we provide a means of dedicating a unique listening port to a specific source. Follow this step to configure unique ports for one or more sources: Modify the /opt/sc4s/env_file file to include the port-specific environment variable(s). Refer to the \u201cSources\u201d documentation to identify the specific environment variables that are mapped to each data source vendor/technology. Modify index destinations for Splunk \u00b6 Log paths are preconfigured to utilize a convention of index destinations that are suitable for most customers. If changes need to be made to index destinations, navigate to the /opt/sc4s/local/context directory to start. Edit splunk_metadata.csv to review or change the index configuration as required for the data sources utilized in your environment. The key (1st column) in this file uses the syntax vendor_product . Simply replace the index value (the 3rd column) in the desired row with the index appropriate for your Splunk installation. The \u201cSources\u201d document details the specific vendor_product keys (rows) in this table that pertain to the individual data source filters that are included with SC4S. Other Splunk metadata (e.g. source and sourcetype) can be overridden via this file as well. This is an advanced topic, and further information is covered in the \u201cLog Path overrides\u201d section of the Configuration document. Configure source filtering by source IP or host name \u00b6 Legacy sources and non-standard-compliant sources require configuration by source IP or hostname as included in the event. The following steps apply to support such sources. To identify sources that require this step, refer to the \u201csources\u201d section of this documentation. See documentation for your vendor/product to determine if specific configuration is required Configure compliance index/metadata overrides \u00b6 In some cases, devices that have been properly sourcetyped need to be further categorized by compliance, geography, or other criterion. The two files compliance_meta_by_source.conf and compliance_meta_by_source.csv can be used for this purpose. These operate similarly to the files above, where the conf file specifies a filter to uniquely identify the messages that should be overridden, and the csv file lists one or more metadata items that can be overridden based on the filter name. This is an advanced topic, and further information is covered in the \u201cOverride index or metadata based on host, ip, or subnet\u201d section of the Configuration document.","title":"Runtime Configuration"},{"location":"gettingstarted/getting-started-runtime-configuration/#implement-a-container-runtime-and-sc4s","text":"","title":"Implement a Container Runtime and SC4S"},{"location":"gettingstarted/getting-started-runtime-configuration/#prerequisites","text":"Linux host with Docker (CE 19.x or greater) or Podman enabled, depending on runtime choice (below). A network load balancer (NLB) configured for round-robin. Note: Special consideration may be required when more advanced products are used. The optimal configuration of the load balancer will round-robin each http POST request (not each connection). The host linux OS receive buffer size should be tuned to match the sc4s default to avoid dropping events (packets) at the network level. The default receive buffer for sc4s is set to 16 MB for UDP traffic, which should be OK for most environments. To set the host OS kernel to match this, edit /etc/sysctl.conf using the following whole-byte values corresponding to 16 MB: net.core.rmem_default = 17039360 net.core.rmem_max = 17039360 and apply to the kernel: sysctl -p Ensure the kernel is not dropping packets by periodically monitoring the buffer with the command netstat -su | grep \"receive errors\" . NOTE: Failure to account for high-volume traffic (especially UDP) by tuning the kernel will result in message loss, which can be very unpredictable and difficult to detect. See this helpful discussion in the syslog-ng Professional Edition documentation regarding tuning syslog-ng in particular (via the SC4S_SOURCE_*_SO_RCVBUFF environment variable in sc4s) as well as overall host kernel tuning. The default values for receive kernel buffers in most distros is 2 MB, which has proven inadequate for many.","title":"Prerequisites"},{"location":"gettingstarted/getting-started-runtime-configuration/#ipv4-forwarding","text":"In many distributions (e.g. CentOS provisioned in AWS), IPV4 forwarding is not enabled by default. This needs to be enabled for container networking to function properly. The following is an example to check and set this up; as usual this needs to be vetted with your enterprise security policy: To check: sudo sysctl net.ipv4.ip_forward To set: sudo sysctl net.ipv4.ip_forward=1 To ensure the change survives a reboot: sysctl settings are defined through files in /usr/lib/sysctl.d/ , /run/sysctl.d/ , and /etc/sysctl.d/ . To override only specific settings, you can either add a file with a lexically later name in /etc/sysctl.d/ and put following setting there: net.ipv4.ip_forward=1 or find this specific setting in one of existing configuration files (mentioned above) and set value to 1 . net.ipv4.ip_forward=1","title":"IPv4 Forwarding"},{"location":"gettingstarted/getting-started-runtime-configuration/#select-a-container-runtime-and-sc4s-configuration","text":"Container Runtime and Orchestration Operating Systems MicroK8s Ubuntu with Microk8s Podman 1.7 & 1.9 + systemd RHEL 8.x or CentOS 8.x (best option), Debian or Ubuntu 18.04LTS Docker CE 19 (and greater) + systemd RHEL or CentOS >7.7 (best option), Debian or Ubuntu 18.04LTS Docker Desktop + Compose MacOS Docker Desktop + Compose RHEL or CentOS 8.1 & 8.2 (best option) Bring your own Environment RHEL or CentOS 8.1 & 8.2 (best option) Offline Container Installation RHEL 8.x or CentOS 8.x (best option), Debian or Ubuntu 18.04LTS Ansible+Docker Swarm RHEL 8.x or CentOS 8.x (best option), Debian or Ubuntu 18.04LTS Ansible+Podman RHEL 7.x/8.x or CentOS 7.x/8.x (best option), Debian or Ubuntu 20.10LTS(and higher) Ansible+Docker RHEL 7.x/8.x or CentOS 7.x/8.x (best option), Debian or Ubuntu 18.04LTS(and higher)","title":"Select a Container Runtime and SC4S Configuration"},{"location":"gettingstarted/getting-started-runtime-configuration/#docker-and-podman-basic-configurations","text":"To run properly sc4s you need to create directories: /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls /opt/sc4s/local will be used as a mount point for local overrides and configurations. The empty local directory created above will populate with defaults and examples at the first invocation of SC4S for local configurations and context overrides. Do not change the directory structure of the files that are laid down; change (or add) only individual files if desired. SC4S depends on the directory layout to read the local configurations properly. See the notes below for which files will be preserved on restarts. In the local/config/ directory there are four subdirectories that allow you to provide support for device types that are not provided out of the box in SC4S. To get you started, there is an example log path template ( lp-example.conf.tmpl ) and a filter ( example.conf ) in the log_paths and filters subdirectories, respectively. These should not be used directly, but copied as templates for your own log path development. They will get overwritten at each SC4S start. In the local/context directory, if you change the \u201cnon-example\u201d version of a file (e.g. splunk_metadata.csv ) the changes will be preserved on a restart. /opt/sc4s/archive will be used as a mount point for local storage of syslog events (if the optional mount is uncommented above). The events will be written in the syslog-ng EWMM format. See the \u201cconfiguration\u201d document for details on the directory structure the archive uses. /opt/sc4s/tls will be used as a mount point for custom TLS certificates (if the optional mount is uncommented above). IMPORTANT: When creating the directories above, ensure the directories created match the volume mounts specified in the sc4s.service unit file . Failure to do this will cause SC4S to abort at startup.","title":"Docker and Podman basic configurations"},{"location":"gettingstarted/getting-started-runtime-configuration/#dedicated-unique-listening-ports","text":"For certain source technologies, categorization by message content is impossible due to the lack of a unique \u201cfingerprint\u201d in the data. In other cases, a unique listening port is required for certain devices due to network requirements in the enterprise. For collection of such sources, we provide a means of dedicating a unique listening port to a specific source. Follow this step to configure unique ports for one or more sources: Modify the /opt/sc4s/env_file file to include the port-specific environment variable(s). Refer to the \u201cSources\u201d documentation to identify the specific environment variables that are mapped to each data source vendor/technology.","title":"Dedicated (Unique) Listening Ports"},{"location":"gettingstarted/getting-started-runtime-configuration/#modify-index-destinations-for-splunk","text":"Log paths are preconfigured to utilize a convention of index destinations that are suitable for most customers. If changes need to be made to index destinations, navigate to the /opt/sc4s/local/context directory to start. Edit splunk_metadata.csv to review or change the index configuration as required for the data sources utilized in your environment. The key (1st column) in this file uses the syntax vendor_product . Simply replace the index value (the 3rd column) in the desired row with the index appropriate for your Splunk installation. The \u201cSources\u201d document details the specific vendor_product keys (rows) in this table that pertain to the individual data source filters that are included with SC4S. Other Splunk metadata (e.g. source and sourcetype) can be overridden via this file as well. This is an advanced topic, and further information is covered in the \u201cLog Path overrides\u201d section of the Configuration document.","title":"Modify index destinations for Splunk"},{"location":"gettingstarted/getting-started-runtime-configuration/#configure-source-filtering-by-source-ip-or-host-name","text":"Legacy sources and non-standard-compliant sources require configuration by source IP or hostname as included in the event. The following steps apply to support such sources. To identify sources that require this step, refer to the \u201csources\u201d section of this documentation. See documentation for your vendor/product to determine if specific configuration is required","title":"Configure source filtering by source IP or host name"},{"location":"gettingstarted/getting-started-runtime-configuration/#configure-compliance-indexmetadata-overrides","text":"In some cases, devices that have been properly sourcetyped need to be further categorized by compliance, geography, or other criterion. The two files compliance_meta_by_source.conf and compliance_meta_by_source.csv can be used for this purpose. These operate similarly to the files above, where the conf file specifies a filter to uniquely identify the messages that should be overridden, and the csv file lists one or more metadata items that can be overridden based on the filter name. This is an advanced topic, and further information is covered in the \u201cOverride index or metadata based on host, ip, or subnet\u201d section of the Configuration document.","title":"Configure compliance index/metadata overrides"},{"location":"gettingstarted/getting-started-splunk-setup/","text":"Splunk setup \u00b6 Create Indexes \u00b6 SC4S is pre-configured to map each sourcetype to a typical index. For new installations, it is best practice to create them in Splunk when using the SC4S defaults. SC4S can be easily customized to use different indexes if desired. email epav epintel infraops netauth netdlp netdns netfw netids netlb netops netwaf netproxy netipam oswin oswinsec osnix print em_metrics (Optional opt-in for SC4S operational metrics; ensure this is created as a metrics index) Install Related Splunk Apps \u00b6 Install the following: IT Essentials Work Configure the Splunk HTTP Event Collector \u00b6 Set up the Splunk HTTP Event Collector with the HEC endpoints behind a load balancer (VIP) configured for https round robin WITHOUT sticky session. Alternatively, a list of HEC endpoint URLs can be configured in SC4S (native syslog-ng load balancing) if no load balancer is in place. In most scenarios the recommendation is to use an external load balancer, as that makes longer term maintenance simpler by eliminating the need to manually keep the list of HEC URLs specified in sc4s current. However, if a LB is not available, native load balancing can be used with 10 or fewer Indexers where HEC is used exclusively for syslog. In either case, it is strongly recommended that SC4S traffic be sent to HEC endpoints configured directly on the indexers rather than an intermediate tier of HWFs. - Create a HEC token that will be used by SC4S and ensure the token has access to place events in main, em_metrics, and all indexes used as event destinations. NOTE: It is recommended that the \u201cSelected Indexes\u201d on the token configuration page be left blank so that the token has access to all indexes, including the lastChanceIndex . If this list is populated, extreme care must be taken to keep it up to date, as an attempt to send data to an index not in this list will result in a 400 error from the HEC endpoint. Furthermore, the lastChanceIndex will not be consulted in the event the index specified in the event is not configured on Splunk. Keep in mind just one bad message will \u201ctaint\u201d the whole batch (by default 1000 events) and prevent the entire batch from being sent to Splunk. In case you are not using TLS on SC4S- turn off SSL on global settings for HEC in Splunk. Refer to Splunk Cloud or Splunk Enterprise for specific HEC configuration instructions based on your Splunk type.","title":"Splunk Setup"},{"location":"gettingstarted/getting-started-splunk-setup/#splunk-setup","text":"","title":"Splunk setup"},{"location":"gettingstarted/getting-started-splunk-setup/#create-indexes","text":"SC4S is pre-configured to map each sourcetype to a typical index. For new installations, it is best practice to create them in Splunk when using the SC4S defaults. SC4S can be easily customized to use different indexes if desired. email epav epintel infraops netauth netdlp netdns netfw netids netlb netops netwaf netproxy netipam oswin oswinsec osnix print em_metrics (Optional opt-in for SC4S operational metrics; ensure this is created as a metrics index)","title":"Create Indexes"},{"location":"gettingstarted/getting-started-splunk-setup/#install-related-splunk-apps","text":"Install the following: IT Essentials Work","title":"Install Related Splunk Apps"},{"location":"gettingstarted/getting-started-splunk-setup/#configure-the-splunk-http-event-collector","text":"Set up the Splunk HTTP Event Collector with the HEC endpoints behind a load balancer (VIP) configured for https round robin WITHOUT sticky session. Alternatively, a list of HEC endpoint URLs can be configured in SC4S (native syslog-ng load balancing) if no load balancer is in place. In most scenarios the recommendation is to use an external load balancer, as that makes longer term maintenance simpler by eliminating the need to manually keep the list of HEC URLs specified in sc4s current. However, if a LB is not available, native load balancing can be used with 10 or fewer Indexers where HEC is used exclusively for syslog. In either case, it is strongly recommended that SC4S traffic be sent to HEC endpoints configured directly on the indexers rather than an intermediate tier of HWFs. - Create a HEC token that will be used by SC4S and ensure the token has access to place events in main, em_metrics, and all indexes used as event destinations. NOTE: It is recommended that the \u201cSelected Indexes\u201d on the token configuration page be left blank so that the token has access to all indexes, including the lastChanceIndex . If this list is populated, extreme care must be taken to keep it up to date, as an attempt to send data to an index not in this list will result in a 400 error from the HEC endpoint. Furthermore, the lastChanceIndex will not be consulted in the event the index specified in the event is not configured on Splunk. Keep in mind just one bad message will \u201ctaint\u201d the whole batch (by default 1000 events) and prevent the entire batch from being sent to Splunk. In case you are not using TLS on SC4S- turn off SSL on global settings for HEC in Splunk. Refer to Splunk Cloud or Splunk Enterprise for specific HEC configuration instructions based on your Splunk type.","title":"Configure the Splunk HTTP Event Collector"},{"location":"gettingstarted/k8s-microk8s/","text":"Install MicroK8s \u00b6 The SC4S deployment model with Microk8s uses specific features of this distribution of k8s. While this may be reproducible with other distributions such an undertaking requires more advanced awareness and responsibility for the administrator. (metalLB) ensure source IP is preserved Bring any operating system (window/centos/rhel/ubuntu/debian) This configuration requires as least 2 IP addressed one for host and one for the internal load balancer. We suggest allocation of 3 ip addresses for the host and 5-10 addresses for later use FAQ \u00b6 Question: How is this deployment model supported? Answer: Similar to other deployment methods, Splunk supports the container itself and the procedural guidance for implementation but does not directly support or otherwise provide resolutions for issues within the runtime environment. Question: Why is this \u201cload balancer\u201d ok but others are not? Answer: While we are using a load balancer with one instance per host, the traffic is restricted to the entry node and one instance of sc4s will run per node. This limits the function of MetalLB to the same function as a Cluster Manager. Question: Is this a recommended deployment model? Answer: Yes, the single-server microk8s model is a recommended option. The use of clustering does have additional tradeoffs and should be carefully considered on a deployment-specific basis. #we need to have a normal install of kubectl because of operator scripts sudo snap install microk8s --classic --channel = 1 .24 # Basic setup of k8s sudo usermod -a -G microk8s $USER sudo chown -f -R $USER ~/.kube su - $USER microk8s status --wait-ready #Note when installing metallb you will be prompted for one or more IPs to used as entry points #Into the cluster if your plan to enable clustering this IP should not be assigned to the host (floats) #If you do not plan to cluster then this IP may be the same IP as the host #Note2: a single IP in cidr format is x.x.x.x/32 use CIDR or range syntax microk8s enable dns microk8s enable community microk8s enable metallb microk8s enable rbac microk8s enable storage microk8s enable openebs microk8s enable helm3 microk8s status --wait-ready Add SC4S Helm repo \u00b6 microk8s helm3 repo add splunk-connect-for-syslog https://splunk.github.io/splunk-connect-for-syslog microk8s helm3 repo update Create a config file \u00b6 Dependent on whether you want to store HEC token as a kubernetes secret create values.yaml file. If you wish to provide HEC token value in plaintext configure it as in example below: The HEC token can be configured either as a plane text or as a secret. As Plaintext Configuration: #values.yaml splunk : hec_url : \"https://xxx.xxx.xxx.xxx:8088/services/collector/event\" hec_token : \"00000000-0000-0000-0000-000000000000\" hec_verify_tls : \"yes\" As Secret Configuration: #values.yaml splunk : hec_url : \"https://xxx.xxx.xxx.xxx:8088/services/collector/event\" hec_verify_tls : \"yes\" Install SC4S \u00b6 microk8s helm3 install sc4s splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml HEC token as a kubernetes secret: export HEC_TOKEN = \"00000000-0000-0000-0000-000000000000\" # provide your token here! microk8s helm3 install sc4s --set splunk.hec_token = $HEC_TOKEN splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml Upgrade SC4S \u00b6 microk8s helm3 upgrade sc4s splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml Setup for HA with multiple nodes \u00b6 See https://microk8s.io/docs/high-availability Note: Three identically-sized nodes are required for HA #values.yaml replicaCount : 6 #2x node count splunk : hec_url : \"https://xxx.xxx.xxx.xxx:8088/services/collector/event\" hec_token : \"00000000-0000-0000-0000-000000000000\" hec_verify_tls : \"yes\" Upgrade sc4s to apply the new config Advanced Configuration \u00b6 Using helm based deployment precludes direct configuration of environment variables and context files but most configuration can be set via the values.yaml sc4s : # Certificate as a k8s Secret with tls.key and tls.crt fields # Ideally produced and managed by cert-manager.io existingCert : example-com-tls # vendor_product : - name : checkpoint ports : tcp : [ 9000 ] #Same as SC4S_LISTEN_CHECKPOINT_TCP_PORT=9000 udp : [ 9000 ] options : listen : old_host_rules : \"yes\" #Same as SC4S_LISTEN_CHECKPOINT_OLD_HOST_RULES=yes - name : infoblox ports : tcp : [ 9001 , 9002 ] tls : [ 9003 ] - name : fortinet ports : ietf_udp : - 9100 - 9101 context_files : splunk_metadata.csv : |- cisco_meraki,index,foo host.csv : |- 192.168.1.1,foo 192.168.1.2,moon config_files and context_files are variables used to specify configuration and context files that need to be passed to the splunk-connect-for-syslog. config_files : This variable contains a dictionary that maps the name of the configuration file to its content in the form of a YAML block scalar. context_file : This variable contains a dictionary that maps the name of the context files to its content in the form of a YAML block scalar. The context file named splunk_metadata.csv and host.csv are being passed with the values.yaml sc4s : # Certificate as a k8s Secret with tls.key and tls.crt fields # Ideally produced and managed by cert-manager.io # vendor_product : - name : checkpoint ports : tcp : [ 9000 ] #Same as SC4S_LISTEN_CHECKPOINT_TCP_PORT=9000 udp : [ 9000 ] options : listen : old_host_rules : \"yes\" #Same as SC4S_LISTEN_CHECKPOINT_OLD_HOST_RULES=yes - name : fortinet ports : ietf_udp : - 9100 - 9101 context_files : splunk_metadata.csv : |+ cisco_meraki,index,foo cisco_asa,index,bar config_files : app-workaround-cisco_asa.conf : |+ block parser app-postfilter-cisco_asa_metadata() { channel { rewrite { unset(value('fields.sc4s_recv_time')); }; }; }; application app-postfilter-cisco_asa_metadata[sc4s-postfilter] { filter { 'cisco' eq \"${fields.sc4s_vendor}\" and 'asa' eq \"${fields.sc4s_product}\" }; parser { app-postfilter-cisco_asa_metadata(); }; }; Resource Management \u00b6 Generally two instances will be provisioned per node adjust requests and limits to allow each instance to use about 40% of each node presuming no other workload is present resources : limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi","title":"MicroK8s + Linux"},{"location":"gettingstarted/k8s-microk8s/#install-microk8s","text":"The SC4S deployment model with Microk8s uses specific features of this distribution of k8s. While this may be reproducible with other distributions such an undertaking requires more advanced awareness and responsibility for the administrator. (metalLB) ensure source IP is preserved Bring any operating system (window/centos/rhel/ubuntu/debian) This configuration requires as least 2 IP addressed one for host and one for the internal load balancer. We suggest allocation of 3 ip addresses for the host and 5-10 addresses for later use","title":"Install MicroK8s"},{"location":"gettingstarted/k8s-microk8s/#faq","text":"Question: How is this deployment model supported? Answer: Similar to other deployment methods, Splunk supports the container itself and the procedural guidance for implementation but does not directly support or otherwise provide resolutions for issues within the runtime environment. Question: Why is this \u201cload balancer\u201d ok but others are not? Answer: While we are using a load balancer with one instance per host, the traffic is restricted to the entry node and one instance of sc4s will run per node. This limits the function of MetalLB to the same function as a Cluster Manager. Question: Is this a recommended deployment model? Answer: Yes, the single-server microk8s model is a recommended option. The use of clustering does have additional tradeoffs and should be carefully considered on a deployment-specific basis. #we need to have a normal install of kubectl because of operator scripts sudo snap install microk8s --classic --channel = 1 .24 # Basic setup of k8s sudo usermod -a -G microk8s $USER sudo chown -f -R $USER ~/.kube su - $USER microk8s status --wait-ready #Note when installing metallb you will be prompted for one or more IPs to used as entry points #Into the cluster if your plan to enable clustering this IP should not be assigned to the host (floats) #If you do not plan to cluster then this IP may be the same IP as the host #Note2: a single IP in cidr format is x.x.x.x/32 use CIDR or range syntax microk8s enable dns microk8s enable community microk8s enable metallb microk8s enable rbac microk8s enable storage microk8s enable openebs microk8s enable helm3 microk8s status --wait-ready","title":"FAQ"},{"location":"gettingstarted/k8s-microk8s/#add-sc4s-helm-repo","text":"microk8s helm3 repo add splunk-connect-for-syslog https://splunk.github.io/splunk-connect-for-syslog microk8s helm3 repo update","title":"Add SC4S Helm repo"},{"location":"gettingstarted/k8s-microk8s/#create-a-config-file","text":"Dependent on whether you want to store HEC token as a kubernetes secret create values.yaml file. If you wish to provide HEC token value in plaintext configure it as in example below: The HEC token can be configured either as a plane text or as a secret. As Plaintext Configuration: #values.yaml splunk : hec_url : \"https://xxx.xxx.xxx.xxx:8088/services/collector/event\" hec_token : \"00000000-0000-0000-0000-000000000000\" hec_verify_tls : \"yes\" As Secret Configuration: #values.yaml splunk : hec_url : \"https://xxx.xxx.xxx.xxx:8088/services/collector/event\" hec_verify_tls : \"yes\"","title":"Create a config file"},{"location":"gettingstarted/k8s-microk8s/#install-sc4s","text":"microk8s helm3 install sc4s splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml HEC token as a kubernetes secret: export HEC_TOKEN = \"00000000-0000-0000-0000-000000000000\" # provide your token here! microk8s helm3 install sc4s --set splunk.hec_token = $HEC_TOKEN splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml","title":"Install SC4S"},{"location":"gettingstarted/k8s-microk8s/#upgrade-sc4s","text":"microk8s helm3 upgrade sc4s splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml","title":"Upgrade SC4S"},{"location":"gettingstarted/k8s-microk8s/#setup-for-ha-with-multiple-nodes","text":"See https://microk8s.io/docs/high-availability Note: Three identically-sized nodes are required for HA #values.yaml replicaCount : 6 #2x node count splunk : hec_url : \"https://xxx.xxx.xxx.xxx:8088/services/collector/event\" hec_token : \"00000000-0000-0000-0000-000000000000\" hec_verify_tls : \"yes\" Upgrade sc4s to apply the new config","title":"Setup for HA with multiple nodes"},{"location":"gettingstarted/k8s-microk8s/#advanced-configuration","text":"Using helm based deployment precludes direct configuration of environment variables and context files but most configuration can be set via the values.yaml sc4s : # Certificate as a k8s Secret with tls.key and tls.crt fields # Ideally produced and managed by cert-manager.io existingCert : example-com-tls # vendor_product : - name : checkpoint ports : tcp : [ 9000 ] #Same as SC4S_LISTEN_CHECKPOINT_TCP_PORT=9000 udp : [ 9000 ] options : listen : old_host_rules : \"yes\" #Same as SC4S_LISTEN_CHECKPOINT_OLD_HOST_RULES=yes - name : infoblox ports : tcp : [ 9001 , 9002 ] tls : [ 9003 ] - name : fortinet ports : ietf_udp : - 9100 - 9101 context_files : splunk_metadata.csv : |- cisco_meraki,index,foo host.csv : |- 192.168.1.1,foo 192.168.1.2,moon config_files and context_files are variables used to specify configuration and context files that need to be passed to the splunk-connect-for-syslog. config_files : This variable contains a dictionary that maps the name of the configuration file to its content in the form of a YAML block scalar. context_file : This variable contains a dictionary that maps the name of the context files to its content in the form of a YAML block scalar. The context file named splunk_metadata.csv and host.csv are being passed with the values.yaml sc4s : # Certificate as a k8s Secret with tls.key and tls.crt fields # Ideally produced and managed by cert-manager.io # vendor_product : - name : checkpoint ports : tcp : [ 9000 ] #Same as SC4S_LISTEN_CHECKPOINT_TCP_PORT=9000 udp : [ 9000 ] options : listen : old_host_rules : \"yes\" #Same as SC4S_LISTEN_CHECKPOINT_OLD_HOST_RULES=yes - name : fortinet ports : ietf_udp : - 9100 - 9101 context_files : splunk_metadata.csv : |+ cisco_meraki,index,foo cisco_asa,index,bar config_files : app-workaround-cisco_asa.conf : |+ block parser app-postfilter-cisco_asa_metadata() { channel { rewrite { unset(value('fields.sc4s_recv_time')); }; }; }; application app-postfilter-cisco_asa_metadata[sc4s-postfilter] { filter { 'cisco' eq \"${fields.sc4s_vendor}\" and 'asa' eq \"${fields.sc4s_product}\" }; parser { app-postfilter-cisco_asa_metadata(); }; };","title":"Advanced Configuration"},{"location":"gettingstarted/k8s-microk8s/#resource-management","text":"Generally two instances will be provisioned per node adjust requests and limits to allow each instance to use about 40% of each node presuming no other workload is present resources : limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi","title":"Resource Management"},{"location":"gettingstarted/podman-systemd-general/","text":"Install podman \u00b6 Refer to Installation NOTE: READ FIRST (IPv4 forwarding) Initial Setup \u00b6 IMPORTANT: Always use the latest unit file (below) with the current release. By default, the latest container is automatically downloaded at each restart. Therefore, make it a habit to check back here regularly to be sure any changes that may have been made to the template unit file below (e.g. suggested mount points) are incorporated in production prior to relaunching via systemd. Create the systemd unit file /lib/systemd/system/sc4s.service based on the following template: Unit file \u00b6 [Unit] Description = SC4S Container Wants = NetworkManager.service network-online.target After = NetworkManager.service network-online.target [Install] WantedBy = multi-user.target [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2\" # Required mount point for syslog-ng persist data (including disk buffer) Environment = \"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\" # Optional mount point for local overrides and configurations; see notes in docs Environment = \"SC4S_LOCAL_MOUNT=/opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\" # Optional mount point for local disk archive (EWMM output) files Environment = \"SC4S_ARCHIVE_MOUNT=/opt/sc4s/archive:/var/lib/syslog-ng/archive:z\" # Map location of TLS custom TLS Environment = \"SC4S_TLS_MOUNT=/opt/sc4s/tls:/etc/syslog-ng/tls:z\" TimeoutStartSec = 0 ExecStartPre = /usr/bin/podman pull $SC4S_IMAGE # Note: /usr/bin/bash will not be valid path for all OS # when startup fails on running bash check if the path is correct ExecStartPre = /usr/bin/bash -c \"/usr/bin/systemctl set-environment SC4SHOST=$(hostname -s)\" ExecStart = /usr/bin/podman run \\ -e \"SC4S_CONTAINER_HOST = ${SC4SHOST}\" \\ -v \"$SC4S_PERSIST_MOUNT\" \\ -v \"$SC4S_LOCAL_MOUNT\" \\ -v \"$SC4S_ARCHIVE_MOUNT\" \\ -v \"$SC4S_TLS_MOUNT\" \\ --env-file = /opt/sc4s/env_file \\ --health-cmd = \"/healthcheck.sh\" \\ --health-interval = 10s --health-retries=6 --health-timeout=6s \\ --network host \\ --name SC4S \\ --rm $SC4S_IMAGE Restart = on-abnormal Execute the following command to create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destination(s). This will also be used to keep track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. This is a required step. sudo podman volume create splunk - sc4s - var NOTE: Be sure to account for disk space requirements for the podman volume created above. This volume is located in /var/lib/containers/storage/volumes/ and could grow significantly if there is an extended outage to the SC4S destinations (typically HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. Create subdirectories /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls Create a file named /opt/sc4s/env_file and add the following environment variables and values: SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // your . splunk . instance : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx - xxxx - xxxx - xxxx - xxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Update SC4S_DEST_SPLUNK_HEC_DEFAULT_URL and SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN to reflect the correct values for your environment. Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. The default number of SC4S_DEST_SPLUNK_HEC_WORKERS is 10. Consult the community if you feel the number of workers (threads) should deviate from this. NOTE: Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above. For more information about configuration refer to Docker and Podman basic configurations and detailed configuration . Configure SC4S for systemd and start SC4S \u00b6 sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s Restart SC4S \u00b6 sudo systemctl restart sc4s If changes were made to the configuration Unit file above (e.g. to configure with dedicated ports), you must first stop SC4S and re-run the systemd configuration commands: sudo systemctl stop sc4s sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s Stop SC4S \u00b6 sudo systemctl stop sc4s Verify Proper Operation \u00b6 SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container. docker logs SC4S You should see events similar to those below in the output: syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng If you do not see the output above, proceed to the \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information. SC4S non-root operation \u00b6 NOTE: \u00b6 Using non-root prevents the use of standard ports 514 and 601 many device can not alter their destination port this is not a valid configuration for general use, and may only be appropriate for cases where accepting syslog from the public internet can not be avoided. Prequisites \u00b6 Podman and slirp4netns installed. Increase number of user namespaces \u00b6 With user that has sudo privileges: $ echo \u201cuser.max_user_namespaces = 28633 \u201d > /etc/sysctl.d/userns.conf $ sysctl -p /etc/sysctl.d/userns.conf Prepare sc4s user \u00b6 Create a non-root user in which to run SC4S and prepare podman for non-root operation: sudo useradd -m -d /home/sc4s -s /bin/bash sc4s sudo passwd sc4s # type password here sudo su - sc4s mkdir -p /home/sc4s/local mkdir -p /home/sc4s/archive mkdir -p /home/sc4s/tls podman system migrate Next login as different user and login back again as sc4s user not using su command. For example: ssh sc4s@localhost (using su will not set needed env variables). Create unit file in changed location (with changes) \u00b6 Create unit file under ~/.config/systemd/user/sc4s.service with following content: [Unit] User = sc4s Description = SC4S Container Wants = NetworkManager.service network-online.target After = NetworkManager.service network-online.target [Install] WantedBy = multi-user.target [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2\" # Required mount point for syslog-ng persist data (including disk buffer) Environment = \"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\" # Optional mount point for local overrides and configurations; see notes in docs Environment = \"SC4S_LOCAL_MOUNT=/home/sc4s/local:/etc/syslog-ng/conf.d/local:z\" # Optional mount point for local disk archive (EWMM output) files Environment = \"SC4S_ARCHIVE_MOUNT=/home/sc4s/archive:/var/lib/syslog-ng/archive:z\" # Map location of TLS custom TLS Environment = \"SC4S_TLS_MOUNT=/home/sc4s/tls:/etc/syslog-ng/tls:z\" TimeoutStartSec = 0 ExecStartPre = /usr/bin/podman pull $SC4S_IMAGE # Note: /usr/bin/bash will not be valid path for all OS # when startup fails on running bash check if the path is correct ExecStartPre = /usr/bin/bash -c \"/usr/bin/systemctl --user set-environment SC4SHOST=$(hostname -s)\" ExecStart = /usr/bin/podman run -p 2514:514 -p 2514:514/udp -p 6514:6514 \\ -e \"SC4S_CONTAINER_HOST = ${SC4SHOST}\" \\ -v \"$SC4S_PERSIST_MOUNT\" \\ -v \"$SC4S_LOCAL_MOUNT\" \\ -v \"$SC4S_ARCHIVE_MOUNT\" \\ -v \"$SC4S_TLS_MOUNT\" \\ --env-file = /opt/sc4s/env_file \\ --health-cmd = \"/healthcheck.sh\" \\ --health-interval = 10s --health-retries=6 --health-timeout=6s \\ --network host \\ --name SC4S \\ --rm $SC4S_IMAGE Restart = on-abnormal Create env file \u00b6 Create env_file at /home/sc4s/env_file . SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = http : // xxx . xxx . xxx . xxx : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no SC4S_LISTEN_DEFAULT_TCP_PORT = 8514 SC4S_LISTEN_DEFAULT_UDP_PORT = 8514 SC4S_LISTEN_DEFAULT_RFC5426_PORT = 8601 SC4S_LISTEN_DEFAULT_RFC6587_PORT = 8601 Run service \u00b6 To run service as non root user run systemctl command wit --user flag: systemctl -- user daemon - reload systemctl -- user enable sc4s systemctl -- user start sc4s The remainder of the setup can be followed directly from the main setup instructions.","title":"Podman + systemd"},{"location":"gettingstarted/podman-systemd-general/#install-podman","text":"Refer to Installation NOTE: READ FIRST (IPv4 forwarding)","title":"Install podman"},{"location":"gettingstarted/podman-systemd-general/#initial-setup","text":"IMPORTANT: Always use the latest unit file (below) with the current release. By default, the latest container is automatically downloaded at each restart. Therefore, make it a habit to check back here regularly to be sure any changes that may have been made to the template unit file below (e.g. suggested mount points) are incorporated in production prior to relaunching via systemd. Create the systemd unit file /lib/systemd/system/sc4s.service based on the following template:","title":"Initial Setup"},{"location":"gettingstarted/podman-systemd-general/#unit-file","text":"[Unit] Description = SC4S Container Wants = NetworkManager.service network-online.target After = NetworkManager.service network-online.target [Install] WantedBy = multi-user.target [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2\" # Required mount point for syslog-ng persist data (including disk buffer) Environment = \"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\" # Optional mount point for local overrides and configurations; see notes in docs Environment = \"SC4S_LOCAL_MOUNT=/opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\" # Optional mount point for local disk archive (EWMM output) files Environment = \"SC4S_ARCHIVE_MOUNT=/opt/sc4s/archive:/var/lib/syslog-ng/archive:z\" # Map location of TLS custom TLS Environment = \"SC4S_TLS_MOUNT=/opt/sc4s/tls:/etc/syslog-ng/tls:z\" TimeoutStartSec = 0 ExecStartPre = /usr/bin/podman pull $SC4S_IMAGE # Note: /usr/bin/bash will not be valid path for all OS # when startup fails on running bash check if the path is correct ExecStartPre = /usr/bin/bash -c \"/usr/bin/systemctl set-environment SC4SHOST=$(hostname -s)\" ExecStart = /usr/bin/podman run \\ -e \"SC4S_CONTAINER_HOST = ${SC4SHOST}\" \\ -v \"$SC4S_PERSIST_MOUNT\" \\ -v \"$SC4S_LOCAL_MOUNT\" \\ -v \"$SC4S_ARCHIVE_MOUNT\" \\ -v \"$SC4S_TLS_MOUNT\" \\ --env-file = /opt/sc4s/env_file \\ --health-cmd = \"/healthcheck.sh\" \\ --health-interval = 10s --health-retries=6 --health-timeout=6s \\ --network host \\ --name SC4S \\ --rm $SC4S_IMAGE Restart = on-abnormal Execute the following command to create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destination(s). This will also be used to keep track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. This is a required step. sudo podman volume create splunk - sc4s - var NOTE: Be sure to account for disk space requirements for the podman volume created above. This volume is located in /var/lib/containers/storage/volumes/ and could grow significantly if there is an extended outage to the SC4S destinations (typically HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info. Create subdirectories /opt/sc4s/local /opt/sc4s/archive /opt/sc4s/tls Create a file named /opt/sc4s/env_file and add the following environment variables and values: SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = https : // your . splunk . instance : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx - xxxx - xxxx - xxxx - xxxxxxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no Update SC4S_DEST_SPLUNK_HEC_DEFAULT_URL and SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN to reflect the correct values for your environment. Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog. The default number of SC4S_DEST_SPLUNK_HEC_WORKERS is 10. Consult the community if you feel the number of workers (threads) should deviate from this. NOTE: Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above. For more information about configuration refer to Docker and Podman basic configurations and detailed configuration .","title":"Unit file"},{"location":"gettingstarted/podman-systemd-general/#configure-sc4s-for-systemd-and-start-sc4s","text":"sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s","title":"Configure SC4S for systemd and start SC4S"},{"location":"gettingstarted/podman-systemd-general/#restart-sc4s","text":"sudo systemctl restart sc4s If changes were made to the configuration Unit file above (e.g. to configure with dedicated ports), you must first stop SC4S and re-run the systemd configuration commands: sudo systemctl stop sc4s sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s","title":"Restart SC4S"},{"location":"gettingstarted/podman-systemd-general/#stop-sc4s","text":"sudo systemctl stop sc4s","title":"Stop SC4S"},{"location":"gettingstarted/podman-systemd-general/#verify-proper-operation","text":"SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. After this step completes, to verify SC4S is properly communicating with Splunk, execute the following search in Splunk: index = * sourcetype=sc4s:events \"starting up\" This should yield an event similar to the following: syslog-ng starting up; version = '3.28.1' When the startup process proceeds normally (without syntax errors). If you do not see this, follow the steps below before proceeding to deeper-level troubleshooting: Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443). Check to see that the proper indexes are created in Splunk, and that the token has access to them. Ensure the proper operation of the load balancer if used. Lastly, execute the following command to check the sc4s startup process running in the container. docker logs SC4S You should see events similar to those below in the output: syslog-ng checking config sc4s version = v1.36.0 starting goss starting syslog-ng If you do not see the output above, proceed to the \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information.","title":"Verify Proper Operation"},{"location":"gettingstarted/podman-systemd-general/#sc4s-non-root-operation","text":"","title":"SC4S non-root operation"},{"location":"gettingstarted/podman-systemd-general/#note","text":"Using non-root prevents the use of standard ports 514 and 601 many device can not alter their destination port this is not a valid configuration for general use, and may only be appropriate for cases where accepting syslog from the public internet can not be avoided.","title":"NOTE:"},{"location":"gettingstarted/podman-systemd-general/#prequisites","text":"Podman and slirp4netns installed.","title":"Prequisites"},{"location":"gettingstarted/podman-systemd-general/#increase-number-of-user-namespaces","text":"With user that has sudo privileges: $ echo \u201cuser.max_user_namespaces = 28633 \u201d > /etc/sysctl.d/userns.conf $ sysctl -p /etc/sysctl.d/userns.conf","title":"Increase number of user namespaces"},{"location":"gettingstarted/podman-systemd-general/#prepare-sc4s-user","text":"Create a non-root user in which to run SC4S and prepare podman for non-root operation: sudo useradd -m -d /home/sc4s -s /bin/bash sc4s sudo passwd sc4s # type password here sudo su - sc4s mkdir -p /home/sc4s/local mkdir -p /home/sc4s/archive mkdir -p /home/sc4s/tls podman system migrate Next login as different user and login back again as sc4s user not using su command. For example: ssh sc4s@localhost (using su will not set needed env variables).","title":"Prepare sc4s user"},{"location":"gettingstarted/podman-systemd-general/#create-unit-file-in-changed-location-with-changes","text":"Create unit file under ~/.config/systemd/user/sc4s.service with following content: [Unit] User = sc4s Description = SC4S Container Wants = NetworkManager.service network-online.target After = NetworkManager.service network-online.target [Install] WantedBy = multi-user.target [Service] Environment = \"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container2:2\" # Required mount point for syslog-ng persist data (including disk buffer) Environment = \"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\" # Optional mount point for local overrides and configurations; see notes in docs Environment = \"SC4S_LOCAL_MOUNT=/home/sc4s/local:/etc/syslog-ng/conf.d/local:z\" # Optional mount point for local disk archive (EWMM output) files Environment = \"SC4S_ARCHIVE_MOUNT=/home/sc4s/archive:/var/lib/syslog-ng/archive:z\" # Map location of TLS custom TLS Environment = \"SC4S_TLS_MOUNT=/home/sc4s/tls:/etc/syslog-ng/tls:z\" TimeoutStartSec = 0 ExecStartPre = /usr/bin/podman pull $SC4S_IMAGE # Note: /usr/bin/bash will not be valid path for all OS # when startup fails on running bash check if the path is correct ExecStartPre = /usr/bin/bash -c \"/usr/bin/systemctl --user set-environment SC4SHOST=$(hostname -s)\" ExecStart = /usr/bin/podman run -p 2514:514 -p 2514:514/udp -p 6514:6514 \\ -e \"SC4S_CONTAINER_HOST = ${SC4SHOST}\" \\ -v \"$SC4S_PERSIST_MOUNT\" \\ -v \"$SC4S_LOCAL_MOUNT\" \\ -v \"$SC4S_ARCHIVE_MOUNT\" \\ -v \"$SC4S_TLS_MOUNT\" \\ --env-file = /opt/sc4s/env_file \\ --health-cmd = \"/healthcheck.sh\" \\ --health-interval = 10s --health-retries=6 --health-timeout=6s \\ --network host \\ --name SC4S \\ --rm $SC4S_IMAGE Restart = on-abnormal","title":"Create unit file in changed location (with changes)"},{"location":"gettingstarted/podman-systemd-general/#create-env-file","text":"Create env_file at /home/sc4s/env_file . SC4S_DEST_SPLUNK_HEC_DEFAULT_URL = http : // xxx . xxx . xxx . xxx : 8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN = xxxxxxxx # Uncomment the following line if using untrusted SSL certificates # SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY = no SC4S_LISTEN_DEFAULT_TCP_PORT = 8514 SC4S_LISTEN_DEFAULT_UDP_PORT = 8514 SC4S_LISTEN_DEFAULT_RFC5426_PORT = 8601 SC4S_LISTEN_DEFAULT_RFC6587_PORT = 8601","title":"Create env file"},{"location":"gettingstarted/podman-systemd-general/#run-service","text":"To run service as non root user run systemctl command wit --user flag: systemctl -- user daemon - reload systemctl -- user enable sc4s systemctl -- user start sc4s The remainder of the setup can be followed directly from the main setup instructions.","title":"Run service"},{"location":"gettingstarted/quickstart_guide/","text":"Quickstart Guide \u00b6 Splunk setup \u00b6 Create the following default indexes that are used by SC4S email epav netauth netdlp netdns netfw netids netops netwaf netproxy netipam oswinsec osnix em_metrics (Optional opt-in for SC4S operational metrics; ensure this is created as a metrics index) Create a HEC token for SC4S. When filling out the form for the token, it is recommended that the \u201cSelected Indexes\u201d pane be left blank and that a lastChanceIndex be created so that all data received by SC4S will land somewhere in Splunk. SC4S setup (using RHEL 7.6) \u00b6 Set the host OS kernel to match the default receive buffer of sc4s which is set to 16MB Add following to /etc/sysctl.conf net.core.rmem_default = 17039360 net.core.rmem_max = 17039360 Apply to the kernel sysctl -p Ensure the kernel is not dropping packets netstat -su | grep \"receive errors\" Create the systemd unit file /lib/systemd/system/sc4s.service . Copy and paste from the SC4S sample unit file (Docker) or SC4S sample unit file (Podman) . Install podman or docker sudo yum -y install podman or sudo yum install docker-engine -y Create a podman/docker local volume that will contain the disk buffer files and other SC4S state files (choose one in the command below) sudo podman|docker volume create splunk-sc4s-var Create directories used as a mount point for local overrides and configurations mkdir /opt/sc4s/local mkdir /opt/sc4s/archive mkdir /opt/sc4s/tls Create the environment file /opt/sc4s/env_file and replace the HEC_URL and HEC_TOKEN as appropriate SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=https://your.splunk.instance:8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx #Uncomment the following line if using untrusted SSL certificates #SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no Configure SC4S for systemd and start SC4S sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s Check podman/docker logs for errors (choose one in command below) sudo podman|docker logs SC4S Search on Splunk for successful installation of SC4S index=* sourcetype=sc4s:events \"starting up\" Send sample data to default udp port 514 of SC4S host echo \u201cHello SC4S\u201d > /dev/udp/<SC4S_ip>/514","title":"Quickstart Guide"},{"location":"gettingstarted/quickstart_guide/#quickstart-guide","text":"","title":"Quickstart Guide"},{"location":"gettingstarted/quickstart_guide/#splunk-setup","text":"Create the following default indexes that are used by SC4S email epav netauth netdlp netdns netfw netids netops netwaf netproxy netipam oswinsec osnix em_metrics (Optional opt-in for SC4S operational metrics; ensure this is created as a metrics index) Create a HEC token for SC4S. When filling out the form for the token, it is recommended that the \u201cSelected Indexes\u201d pane be left blank and that a lastChanceIndex be created so that all data received by SC4S will land somewhere in Splunk.","title":"Splunk setup"},{"location":"gettingstarted/quickstart_guide/#sc4s-setup-using-rhel-76","text":"Set the host OS kernel to match the default receive buffer of sc4s which is set to 16MB Add following to /etc/sysctl.conf net.core.rmem_default = 17039360 net.core.rmem_max = 17039360 Apply to the kernel sysctl -p Ensure the kernel is not dropping packets netstat -su | grep \"receive errors\" Create the systemd unit file /lib/systemd/system/sc4s.service . Copy and paste from the SC4S sample unit file (Docker) or SC4S sample unit file (Podman) . Install podman or docker sudo yum -y install podman or sudo yum install docker-engine -y Create a podman/docker local volume that will contain the disk buffer files and other SC4S state files (choose one in the command below) sudo podman|docker volume create splunk-sc4s-var Create directories used as a mount point for local overrides and configurations mkdir /opt/sc4s/local mkdir /opt/sc4s/archive mkdir /opt/sc4s/tls Create the environment file /opt/sc4s/env_file and replace the HEC_URL and HEC_TOKEN as appropriate SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=https://your.splunk.instance:8088 SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx #Uncomment the following line if using untrusted SSL certificates #SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no Configure SC4S for systemd and start SC4S sudo systemctl daemon-reload sudo systemctl enable sc4s sudo systemctl start sc4s Check podman/docker logs for errors (choose one in command below) sudo podman|docker logs SC4S Search on Splunk for successful installation of SC4S index=* sourcetype=sc4s:events \"starting up\" Send sample data to default udp port 514 of SC4S host echo \u201cHello SC4S\u201d > /dev/udp/<SC4S_ip>/514","title":"SC4S setup (using RHEL 7.6)"},{"location":"sources/","text":"Introduction \u00b6 When using Splunk Connect for Syslog to onboard a data source, the syslog-ng \u201capp-parser\u201d performs the operations that are traditionally performed at index-time by the corresponding Technical Add-on installed there. These index-time operations include linebreaking, source/sourcetype setting and timestamping. For this reason, if a data source is exclusively onboarded using SC4S then you will not need to install its corresponding Add-On on the indexers. You must, however, install the Add-on on the search head(s) for the user communities interested in this data source. SC4S is designed to process \u201csyslog\u201d referring to IETF RFC standards 5424, legacy BSD syslog, RFC3164 (Not a standard document), and many \u201calmost\u201d syslog formats. When possible data sources are identified and processed based on characteristics of the event that make them unique as compared to other events for example. Cisco devices using IOS will include \u201d : %\u201d followed by a string. While Arista EOS devices will use a valid RFC3164 header with a value in the \u201cPROGRAM\u201d position with \u201c%\u201d as the first char in the \u201cMESSAGE\u201d portion. This allows two similar event structures to be processed correctly. When identification by message content alone is not possible for example the \u201csshd\u201d program field is commonly used across vendors additional \u201chint\u201d or guidance configuration allows SC4S to better classify events. The hints can be applied by definition of a specific port which will be used as a property of the event or by configuration of a host name/ip pattern. For example \u201cVMWARE VSPHERE\u201d products have a number of \u201cPROGRAM\u201d fields which can be used to identify vmware specific events in the syslog stream and these can be properly sourcetyped automatically however because \u201csshd\u201d is not unique it will be treated as generic \u201cos:nix\u201d events until further configuration is applied. The administrator can take one of two actions to refine the processing for vmware Define a specific port for vmware and reconfigure sources to use the defined port \u201cSC4S_LISTEN_VMWARE_VSPHERE_TCP=9000\u201d. Any events arriving on port 9000 will now have a metadata field attached \u201c.netsource.sc4s_vendor_product=VMWARE_VSPHERE\u201d Define a \u201capp-parser\u201d to apply the metadata field by using a syslog-ng filter to apply the metadata field. Supporting previously unknown sources \u00b6 Many log sources can be supported using one of the flexible options available without specific code known as app-parsers. New supported sources are added regularly. Please submit an issue with a description of the vend/product. Configuration information an a compressed pcap (.zip) from a non-production environment to request support for a new source. Many sources can be self supported. While we encourage sharing new sources via the github project to promote consistency and develop best-practices there is no requirement to engage in the community. Sources that are compliant with RFC 5424,RFC 5425, RFC 5426, or RFC 6587 can be onboarded as simple sources Sources \u201ccompatible\u201d with RFC3164 Note incorrect use of the syslog version, or \u201ccreative\u201d formats in the time stamp or other fields may prevent use as simple sources Common Event Format CEF Also known as ArcSight format Log Extended Format LEEF Almost Syslog \u00b6 Sources sending legacy non conformant 3164 like streams can be assisted by the creation of an \u201cAlmost Syslog\u201d Parser. In an such a parser the goal is to process the syslog header allowing other parsers to correctly parse and handle the event. The following example is take from a currently supported format where the source product used epoch in the time stamp field. #Example event #<134>1 1563249630.774247467 devicename security_event ids_alerted signature=1:28423:1 # In the example note the vendor incorrectly included \"1\" following PRI defined in RFC5424 as indicating a compliant message # The parser must remove the 1 before properly parsing # The epoch time is captured by regex # The epoch time is converted back into an RFC3306 date and provided to the parser block parser syslog_epoch - parser () { channel { filter { message ( ' ^ ( \\ < \\ d + \\ > )( ?: 1 ( ?= )) ? ? ( \\ d { 10 , 13 }( ?: \\ . \\ d + ) ? ) (. * ) ' , flags ( store - matches )); }; parser { date - parser ( format ( ' % s . % f ' , ' % s ' ) template ( \"$2\" ) ); }; parser { syslog - parser ( flags ( assume - utf8 , expect - hostname , guess - timezone ) template ( \"$1 $S_ISODATE $3\" ) ); }; rewrite ( set_rfc3164_epoch ); }; }; application syslog_epoch [ sc4s - almost - syslog ] { parser { syslog_epoch - parser (); }; }; Standard Syslog using message parsing \u00b6 Syslog data conforming to RFC3164 or complying with RFC standards mentioned above can be processed with an app-parser allowing the use of the default port rather than requiring custom ports the following example take from a currently supported source uses the value of \u201cprogram\u201d to identify the source as this program value is unique. Care must be taken to write filter conditions strictly enough to not conflict with similar sources block parser alcatel_switch - parser () { channel { rewrite { r_set_splunk_dest_default ( index ( ' netops ' ) sourcetype ( ' alcatel : switch ' ) vendor ( ' alcatel ' ) product ( ' switch ' ) template ( ' t_hdr_msg ' ) ); }; }; }; application alcatel_switch [ sc4s - syslog ] { filter { program ( ' swlogd ' type ( string ) flags ( prefix )); }; parser { alcatel_switch - parser (); }; }; Standard Syslog vendor product by source \u00b6 In some cases standard syslog is also generic and can not be disambiguated from other sources by message content alone. When this happens and only a single source type is desired the \u201csimple\u201d option above is valid but requires managing a port. The following example allows use of a named port OR the vendor product by source configuration. block parser dell_poweredge_cmc - parser () { channel { rewrite { r_set_splunk_dest_default ( index ( ' infraops ' ) sourcetype ( ' dell : poweredge : cmc : syslog ' ) vendor ( ' dell ' ) product ( ' poweredge ' ) class ( ' cmc ' ) ); }; }; }; application dell_poweredge_cmc [ sc4s - network - source ] { filter { ( \"${.netsource.sc4s_vendor_product}\" eq \"dell_poweredge_cmc\" or \"${SOURCE}\" eq \"s_DELL_POWEREDGE_CMC\" ) and \"${fields.sc4s_vendor_product}\" eq \"\" }; parser { dell_poweredge_cmc - parser (); }; }; Filtering events from output \u00b6 In some cases specific events may be considered \u201cnoise\u201d and functionality must be implemented to prevent forwarding of these events to Splunk In version 2.0.0 of SC4S a new feature was implemented to improve the ease of use and efficiency of this progress. The following example will \u201cnull_queue\u201d or drop cisco IOS device events at the debug level. Note Cisco does not use the PRI to indicate DEBUG a message filter is required. block parser cisco_ios_debug - postfilter () { channel { #In this case the outcome is drop the event other logic such as adding indexed fields or editing the message is possible rewrite ( r_set_dest_splunk_null_queue ); }; }; application cisco_ios_debug - postfilter [ sc4s - postfilter ] { filter { \"${fields.sc4s_vendor}\" eq \"cisco\" and \"${fields.sc4s_product}\" eq \"ios\" #Note regex reads as # start from first position # Any atleast 1 char that is not a `-` # constant '-7-' and message ( ' ^% [ ^ \\ - ] + -7 - ' ); }; parser { cisco_ios_debug - postfilter (); }; }; Another example to drop events based on \u201csrc\u201d and \u201caction\u201d values in message \u00b6 #filename: /opt/sc4s/local/config/app_parsers/rewriters/app-dest-rewrite-checkpoint_drop block parser app - dest - rewrite - checkpoint_drop - d_fmt_hec_default () { channel { rewrite ( r_set_dest_splunk_null_queue ); }; }; application app - dest - rewrite - checkpoint_drop - d_fmt_hec_default [ sc4s - lp - dest - format - d_hec_fmt ] { filter { match ( ' checkpoint ' value ( ' fields . sc4s_vendor ' ) type ( string )) and match ( ' syslog ' value ( ' fields . sc4s_product ' ) type ( string )) and match ( ' Drop ' value ( ' . SDATA . sc4s @ 2620. action ' ) type ( string )) and match ( ' 12. ' value ( ' . SDATA . sc4s @ 2620. src ' ) type ( string ) flags ( prefix ) ); }; parser { app - dest - rewrite - checkpoint_drop - d_fmt_hec_default (); }; }; The SC4S \u201cfallback\u201d sourcetype \u00b6 If SC4S receives an event on port 514 which has no soup filter, that event will be given a \u201cfallback\u201d sourcetype. If you see events in Splunk with the fallback sourcetype, then you should figure out what source the events are from and determine why these events are not being sourcetyped correctly. The most common reason for events categorized as \u201cfallback\u201d is the lack of a SC4S filter for that source, and in some cases a misconfigured relay which alters the integrity of the message format. In most cases this means a new SC4S filter must be developed. In this situation you can either build a filter or file an issue with the community to request help. The \u201cfallback\u201d sourcetype is formatted in JSON to allow the administrator to see the constituent syslog-ng \u201cmacros\u201d (fields) that have been automatically parsed by the syslog-ng server An RFC3164 (legacy BSD syslog) \u201con the wire\u201d raw message is usually (but unfortunately not always) comprised of the following syslog-ng macros, in this order and spacing: <$PRI> $HOST $LEGACY_MSGHDR$MESSAGE These fields can be very useful in building a new filter for that sourcetype. In addition, the indexed field sc4s_syslog_format is helpful in determining if the incoming message is standard RFC3164. A value of anything other than rfc3164 or rfc5424_strict indicates a vendor perturbation of standard syslog, which will warrant more careful examination when building a filter. Splunk Connect for Syslog and Splunk metadata \u00b6 A key aspect of SC4S is to properly set Splunk metadata prior to the data arriving in Splunk (and before any TA processing takes place. The filters will apply the proper index, source, sourcetype, host, and timestamp metadata automatically by individual data source. Proper values for this metadata (including a recommended index) are included with all \u201cout-of-the-box\u201d log paths included with SC4S and are chosen to properly interface with the corresponding TA in Splunk. The administrator will need to ensure all recommended indexes be created to accept this data if the defaults are not changed. It is understood that default values will need to be changed in many installations. Each source documented in this section has a table entitled \u201cSourcetype and Index Configuration\u201d, which highlights the default index and sourcetype for each source. See the section \u201cSC4S metadata configuration\u201d in the \u201cConfiguration\u201d page for more information on how to override the default values in this table. Unique listening ports \u00b6 SC4S supports unique listening ports for each source technology/log path (e.g. Cisco ASA), which is useful when the device is sending data on a port different from the typical default syslog port (UDP port 514). In some cases, when the source device emits data that is not able to be distinguished from other device types, a unique port is sometimes required. The specific environment variables used for setting \u201cunique ports\u201d are outlined in each source document in this section. In most cases only one \u201cunique port\u201d is needed for each source. However, SC4S also supports multiple network listening ports per source, which can be useful for a narrow set of compliance use cases. When configuring a source port variable to enable multiple ports, use a comma-separated list with no spaces (e.g. SC4S_LISTEN_CISCO_ASA_UDP_PORT=5005,6005 ). Filtering by an extra product description \u00b6 Due to the fact that unique listening port feature differentiate vendor and product based on the first two underscore characters (\u2018_\u2019), it is possible to filter events by an extra string added to the product. For example in case of having several devices of the same type sending logs over different ports it is possible to route it to different indexes based only on port value while retaining proper vendor and product fields. In general, it follows convention: SC4S_LISTEN_ { VENDOR } _ { PRODUCT } _ { PROTOCOL } _PORT = { PORT VALUE 1 },{ PORT VALUE 2 }... But for special use cases it can be extended to: SC4S_LISTEN_ { VENDOR } _ { PRODUCT } _ { ADDITIONAL_STRING } _ { PROTOCOL } _PORT = { PORT VALUE },{ PORT VALUE 2 }... This feature removes the need for complex pre/post filters. Example: SC4S_LISTEN_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01 - 001 _UDP_PORT = 18514 sets : vendor = < example vendor > product = < example product > tag = . source . s_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01 - 001 SC4S_LISTEN_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01 - 002 _UDP_PORT = 28514 sets : vendor = < example vendor > product = < example product > tag = . source . s_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01 - 002","title":"Read First"},{"location":"sources/#introduction","text":"When using Splunk Connect for Syslog to onboard a data source, the syslog-ng \u201capp-parser\u201d performs the operations that are traditionally performed at index-time by the corresponding Technical Add-on installed there. These index-time operations include linebreaking, source/sourcetype setting and timestamping. For this reason, if a data source is exclusively onboarded using SC4S then you will not need to install its corresponding Add-On on the indexers. You must, however, install the Add-on on the search head(s) for the user communities interested in this data source. SC4S is designed to process \u201csyslog\u201d referring to IETF RFC standards 5424, legacy BSD syslog, RFC3164 (Not a standard document), and many \u201calmost\u201d syslog formats. When possible data sources are identified and processed based on characteristics of the event that make them unique as compared to other events for example. Cisco devices using IOS will include \u201d : %\u201d followed by a string. While Arista EOS devices will use a valid RFC3164 header with a value in the \u201cPROGRAM\u201d position with \u201c%\u201d as the first char in the \u201cMESSAGE\u201d portion. This allows two similar event structures to be processed correctly. When identification by message content alone is not possible for example the \u201csshd\u201d program field is commonly used across vendors additional \u201chint\u201d or guidance configuration allows SC4S to better classify events. The hints can be applied by definition of a specific port which will be used as a property of the event or by configuration of a host name/ip pattern. For example \u201cVMWARE VSPHERE\u201d products have a number of \u201cPROGRAM\u201d fields which can be used to identify vmware specific events in the syslog stream and these can be properly sourcetyped automatically however because \u201csshd\u201d is not unique it will be treated as generic \u201cos:nix\u201d events until further configuration is applied. The administrator can take one of two actions to refine the processing for vmware Define a specific port for vmware and reconfigure sources to use the defined port \u201cSC4S_LISTEN_VMWARE_VSPHERE_TCP=9000\u201d. Any events arriving on port 9000 will now have a metadata field attached \u201c.netsource.sc4s_vendor_product=VMWARE_VSPHERE\u201d Define a \u201capp-parser\u201d to apply the metadata field by using a syslog-ng filter to apply the metadata field.","title":"Introduction"},{"location":"sources/#supporting-previously-unknown-sources","text":"Many log sources can be supported using one of the flexible options available without specific code known as app-parsers. New supported sources are added regularly. Please submit an issue with a description of the vend/product. Configuration information an a compressed pcap (.zip) from a non-production environment to request support for a new source. Many sources can be self supported. While we encourage sharing new sources via the github project to promote consistency and develop best-practices there is no requirement to engage in the community. Sources that are compliant with RFC 5424,RFC 5425, RFC 5426, or RFC 6587 can be onboarded as simple sources Sources \u201ccompatible\u201d with RFC3164 Note incorrect use of the syslog version, or \u201ccreative\u201d formats in the time stamp or other fields may prevent use as simple sources Common Event Format CEF Also known as ArcSight format Log Extended Format LEEF","title":"Supporting previously unknown sources"},{"location":"sources/#almost-syslog","text":"Sources sending legacy non conformant 3164 like streams can be assisted by the creation of an \u201cAlmost Syslog\u201d Parser. In an such a parser the goal is to process the syslog header allowing other parsers to correctly parse and handle the event. The following example is take from a currently supported format where the source product used epoch in the time stamp field. #Example event #<134>1 1563249630.774247467 devicename security_event ids_alerted signature=1:28423:1 # In the example note the vendor incorrectly included \"1\" following PRI defined in RFC5424 as indicating a compliant message # The parser must remove the 1 before properly parsing # The epoch time is captured by regex # The epoch time is converted back into an RFC3306 date and provided to the parser block parser syslog_epoch - parser () { channel { filter { message ( ' ^ ( \\ < \\ d + \\ > )( ?: 1 ( ?= )) ? ? ( \\ d { 10 , 13 }( ?: \\ . \\ d + ) ? ) (. * ) ' , flags ( store - matches )); }; parser { date - parser ( format ( ' % s . % f ' , ' % s ' ) template ( \"$2\" ) ); }; parser { syslog - parser ( flags ( assume - utf8 , expect - hostname , guess - timezone ) template ( \"$1 $S_ISODATE $3\" ) ); }; rewrite ( set_rfc3164_epoch ); }; }; application syslog_epoch [ sc4s - almost - syslog ] { parser { syslog_epoch - parser (); }; };","title":"Almost Syslog"},{"location":"sources/#standard-syslog-using-message-parsing","text":"Syslog data conforming to RFC3164 or complying with RFC standards mentioned above can be processed with an app-parser allowing the use of the default port rather than requiring custom ports the following example take from a currently supported source uses the value of \u201cprogram\u201d to identify the source as this program value is unique. Care must be taken to write filter conditions strictly enough to not conflict with similar sources block parser alcatel_switch - parser () { channel { rewrite { r_set_splunk_dest_default ( index ( ' netops ' ) sourcetype ( ' alcatel : switch ' ) vendor ( ' alcatel ' ) product ( ' switch ' ) template ( ' t_hdr_msg ' ) ); }; }; }; application alcatel_switch [ sc4s - syslog ] { filter { program ( ' swlogd ' type ( string ) flags ( prefix )); }; parser { alcatel_switch - parser (); }; };","title":"Standard Syslog using message parsing"},{"location":"sources/#standard-syslog-vendor-product-by-source","text":"In some cases standard syslog is also generic and can not be disambiguated from other sources by message content alone. When this happens and only a single source type is desired the \u201csimple\u201d option above is valid but requires managing a port. The following example allows use of a named port OR the vendor product by source configuration. block parser dell_poweredge_cmc - parser () { channel { rewrite { r_set_splunk_dest_default ( index ( ' infraops ' ) sourcetype ( ' dell : poweredge : cmc : syslog ' ) vendor ( ' dell ' ) product ( ' poweredge ' ) class ( ' cmc ' ) ); }; }; }; application dell_poweredge_cmc [ sc4s - network - source ] { filter { ( \"${.netsource.sc4s_vendor_product}\" eq \"dell_poweredge_cmc\" or \"${SOURCE}\" eq \"s_DELL_POWEREDGE_CMC\" ) and \"${fields.sc4s_vendor_product}\" eq \"\" }; parser { dell_poweredge_cmc - parser (); }; };","title":"Standard Syslog vendor product by source"},{"location":"sources/#filtering-events-from-output","text":"In some cases specific events may be considered \u201cnoise\u201d and functionality must be implemented to prevent forwarding of these events to Splunk In version 2.0.0 of SC4S a new feature was implemented to improve the ease of use and efficiency of this progress. The following example will \u201cnull_queue\u201d or drop cisco IOS device events at the debug level. Note Cisco does not use the PRI to indicate DEBUG a message filter is required. block parser cisco_ios_debug - postfilter () { channel { #In this case the outcome is drop the event other logic such as adding indexed fields or editing the message is possible rewrite ( r_set_dest_splunk_null_queue ); }; }; application cisco_ios_debug - postfilter [ sc4s - postfilter ] { filter { \"${fields.sc4s_vendor}\" eq \"cisco\" and \"${fields.sc4s_product}\" eq \"ios\" #Note regex reads as # start from first position # Any atleast 1 char that is not a `-` # constant '-7-' and message ( ' ^% [ ^ \\ - ] + -7 - ' ); }; parser { cisco_ios_debug - postfilter (); }; };","title":"Filtering events from output"},{"location":"sources/#another-example-to-drop-events-based-on-src-and-action-values-in-message","text":"#filename: /opt/sc4s/local/config/app_parsers/rewriters/app-dest-rewrite-checkpoint_drop block parser app - dest - rewrite - checkpoint_drop - d_fmt_hec_default () { channel { rewrite ( r_set_dest_splunk_null_queue ); }; }; application app - dest - rewrite - checkpoint_drop - d_fmt_hec_default [ sc4s - lp - dest - format - d_hec_fmt ] { filter { match ( ' checkpoint ' value ( ' fields . sc4s_vendor ' ) type ( string )) and match ( ' syslog ' value ( ' fields . sc4s_product ' ) type ( string )) and match ( ' Drop ' value ( ' . SDATA . sc4s @ 2620. action ' ) type ( string )) and match ( ' 12. ' value ( ' . SDATA . sc4s @ 2620. src ' ) type ( string ) flags ( prefix ) ); }; parser { app - dest - rewrite - checkpoint_drop - d_fmt_hec_default (); }; };","title":"Another example to drop events based on \"src\" and \"action\" values in  message"},{"location":"sources/#the-sc4s-fallback-sourcetype","text":"If SC4S receives an event on port 514 which has no soup filter, that event will be given a \u201cfallback\u201d sourcetype. If you see events in Splunk with the fallback sourcetype, then you should figure out what source the events are from and determine why these events are not being sourcetyped correctly. The most common reason for events categorized as \u201cfallback\u201d is the lack of a SC4S filter for that source, and in some cases a misconfigured relay which alters the integrity of the message format. In most cases this means a new SC4S filter must be developed. In this situation you can either build a filter or file an issue with the community to request help. The \u201cfallback\u201d sourcetype is formatted in JSON to allow the administrator to see the constituent syslog-ng \u201cmacros\u201d (fields) that have been automatically parsed by the syslog-ng server An RFC3164 (legacy BSD syslog) \u201con the wire\u201d raw message is usually (but unfortunately not always) comprised of the following syslog-ng macros, in this order and spacing: <$PRI> $HOST $LEGACY_MSGHDR$MESSAGE These fields can be very useful in building a new filter for that sourcetype. In addition, the indexed field sc4s_syslog_format is helpful in determining if the incoming message is standard RFC3164. A value of anything other than rfc3164 or rfc5424_strict indicates a vendor perturbation of standard syslog, which will warrant more careful examination when building a filter.","title":"The SC4S \"fallback\" sourcetype"},{"location":"sources/#splunk-connect-for-syslog-and-splunk-metadata","text":"A key aspect of SC4S is to properly set Splunk metadata prior to the data arriving in Splunk (and before any TA processing takes place. The filters will apply the proper index, source, sourcetype, host, and timestamp metadata automatically by individual data source. Proper values for this metadata (including a recommended index) are included with all \u201cout-of-the-box\u201d log paths included with SC4S and are chosen to properly interface with the corresponding TA in Splunk. The administrator will need to ensure all recommended indexes be created to accept this data if the defaults are not changed. It is understood that default values will need to be changed in many installations. Each source documented in this section has a table entitled \u201cSourcetype and Index Configuration\u201d, which highlights the default index and sourcetype for each source. See the section \u201cSC4S metadata configuration\u201d in the \u201cConfiguration\u201d page for more information on how to override the default values in this table.","title":"Splunk Connect for Syslog and Splunk metadata"},{"location":"sources/#unique-listening-ports","text":"SC4S supports unique listening ports for each source technology/log path (e.g. Cisco ASA), which is useful when the device is sending data on a port different from the typical default syslog port (UDP port 514). In some cases, when the source device emits data that is not able to be distinguished from other device types, a unique port is sometimes required. The specific environment variables used for setting \u201cunique ports\u201d are outlined in each source document in this section. In most cases only one \u201cunique port\u201d is needed for each source. However, SC4S also supports multiple network listening ports per source, which can be useful for a narrow set of compliance use cases. When configuring a source port variable to enable multiple ports, use a comma-separated list with no spaces (e.g. SC4S_LISTEN_CISCO_ASA_UDP_PORT=5005,6005 ).","title":"Unique listening ports"},{"location":"sources/#filtering-by-an-extra-product-description","text":"Due to the fact that unique listening port feature differentiate vendor and product based on the first two underscore characters (\u2018_\u2019), it is possible to filter events by an extra string added to the product. For example in case of having several devices of the same type sending logs over different ports it is possible to route it to different indexes based only on port value while retaining proper vendor and product fields. In general, it follows convention: SC4S_LISTEN_ { VENDOR } _ { PRODUCT } _ { PROTOCOL } _PORT = { PORT VALUE 1 },{ PORT VALUE 2 }... But for special use cases it can be extended to: SC4S_LISTEN_ { VENDOR } _ { PRODUCT } _ { ADDITIONAL_STRING } _ { PROTOCOL } _PORT = { PORT VALUE },{ PORT VALUE 2 }... This feature removes the need for complex pre/post filters. Example: SC4S_LISTEN_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01 - 001 _UDP_PORT = 18514 sets : vendor = < example vendor > product = < example product > tag = . source . s_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01 - 001 SC4S_LISTEN_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01 - 002 _UDP_PORT = 28514 sets : vendor = < example vendor > product = < example product > tag = . source . s_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01 - 002","title":"Filtering by an extra product description"},{"location":"sources/base/cef/","text":"Common Event Format (CEF) \u00b6 Product - Various products that send CEF-format messages via syslog \u00b6 Each CEF product should have their own source entry in this documentation set. In a departure from normal configuration, all CEF products should use the \u201cCEF\u201d version of the unique port and archive environment variable settings (rather than a unique one per product), as the CEF log path handles all products sending events to SC4S in the CEF format. Examples of this include Arcsight, Imperva, and Cyberark. Therefore, the CEF environment variables for unique port, archive, etc. should be set only once . If your deployment has multiple CEF devices that send to more than one port, set the CEF unique port variable(s) as a comma-separated list. See Unique Listening Ports for details. The source documentation included below is a reference baseline for any product that sends data using the CEF log path. Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Product Manual https://docs.imperva.com/bundle/cloud-application-security/page/more/log-configuration.htm Splunk Metadata with CEF events \u00b6 The keys (first column) in splunk_metadata.csv for CEF data sources have a slightly different meaning than those for non-CEF ones. The typical vendor_product syntax is instead replaced by checks against specific columns of the CEF event \u2013 namely the first, second, and fourth columns following the leading CEF:0 (\u201ccolumn 0\u201d). These specific columns refer to the CEF device_vendor , device_product , and device_event_class , respectively. The third column, device_version , is not used for metadata assignment. SC4S sets metadata based on the first two columns, and (optionally) the fourth. While the key (first column) in the splunk_metadata file for non-CEF sources uses a \u201cvendor_product\u201d syntax that is arbitrary, the syntax for this key for CEF events is based on the actual contents of columns 1,2 and 4 from the CEF event, namely: device_vendor _ device_product _ device_class The final device_class portion is optional. Therefore, CEF entries in splunk_metadata can have a key representing the vendor and product, and others representing a vendor and product coupled with one or more additional classes. This allows for more granular metadata assignment (or overrides). Here is a snippet of a sample Imperva CEF event that includes a CEF device class entry (which is \u201cFirewall\u201d): Apr 19 10:29:53 3.3.3.3 CEF:0|Imperva Inc.|SecureSphere|12.0.0|Firewall|SSL Untraceable Connection|Medium| and the corresponding match in splunk_metadata.csv : Imperva Inc._SecureSphere_Firewall,sourcetype,imperva:waf:firewall:cef Default Sourcetype \u00b6 sourcetype notes cef Common sourcetype Default Source \u00b6 source notes Varies Varies Default Index Configuration \u00b6 key source index notes Vendor_Product Varies main none Filter type \u00b6 MSG Parse: This filter parses message content Options \u00b6 Variable default description SC4S_LISTEN_CEF_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_CEF_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_CEF_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_CEF no Enable archive to disk for this specific source SC4S_DEST_CEF_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source","title":"Common Event Format (CEF)"},{"location":"sources/base/cef/#common-event-format-cef","text":"","title":"Common Event Format (CEF)"},{"location":"sources/base/cef/#product-various-products-that-send-cef-format-messages-via-syslog","text":"Each CEF product should have their own source entry in this documentation set. In a departure from normal configuration, all CEF products should use the \u201cCEF\u201d version of the unique port and archive environment variable settings (rather than a unique one per product), as the CEF log path handles all products sending events to SC4S in the CEF format. Examples of this include Arcsight, Imperva, and Cyberark. Therefore, the CEF environment variables for unique port, archive, etc. should be set only once . If your deployment has multiple CEF devices that send to more than one port, set the CEF unique port variable(s) as a comma-separated list. See Unique Listening Ports for details. The source documentation included below is a reference baseline for any product that sends data using the CEF log path. Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Product Manual https://docs.imperva.com/bundle/cloud-application-security/page/more/log-configuration.htm","title":"Product - Various products that send CEF-format messages via syslog"},{"location":"sources/base/cef/#splunk-metadata-with-cef-events","text":"The keys (first column) in splunk_metadata.csv for CEF data sources have a slightly different meaning than those for non-CEF ones. The typical vendor_product syntax is instead replaced by checks against specific columns of the CEF event \u2013 namely the first, second, and fourth columns following the leading CEF:0 (\u201ccolumn 0\u201d). These specific columns refer to the CEF device_vendor , device_product , and device_event_class , respectively. The third column, device_version , is not used for metadata assignment. SC4S sets metadata based on the first two columns, and (optionally) the fourth. While the key (first column) in the splunk_metadata file for non-CEF sources uses a \u201cvendor_product\u201d syntax that is arbitrary, the syntax for this key for CEF events is based on the actual contents of columns 1,2 and 4 from the CEF event, namely: device_vendor _ device_product _ device_class The final device_class portion is optional. Therefore, CEF entries in splunk_metadata can have a key representing the vendor and product, and others representing a vendor and product coupled with one or more additional classes. This allows for more granular metadata assignment (or overrides). Here is a snippet of a sample Imperva CEF event that includes a CEF device class entry (which is \u201cFirewall\u201d): Apr 19 10:29:53 3.3.3.3 CEF:0|Imperva Inc.|SecureSphere|12.0.0|Firewall|SSL Untraceable Connection|Medium| and the corresponding match in splunk_metadata.csv : Imperva Inc._SecureSphere_Firewall,sourcetype,imperva:waf:firewall:cef","title":"Splunk Metadata with CEF events"},{"location":"sources/base/cef/#default-sourcetype","text":"sourcetype notes cef Common sourcetype","title":"Default Sourcetype"},{"location":"sources/base/cef/#default-source","text":"source notes Varies Varies","title":"Default Source"},{"location":"sources/base/cef/#default-index-configuration","text":"key source index notes Vendor_Product Varies main none","title":"Default Index Configuration"},{"location":"sources/base/cef/#filter-type","text":"MSG Parse: This filter parses message content","title":"Filter type"},{"location":"sources/base/cef/#options","text":"Variable default description SC4S_LISTEN_CEF_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_CEF_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_CEF_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_CEF no Enable archive to disk for this specific source SC4S_DEST_CEF_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source","title":"Options"},{"location":"sources/base/leef/","text":"Log Extended Event Format (LEEF) \u00b6 Product - Various products that send LEEF V1 and V2 format messages via syslog \u00b6 Each LEEF product should have their own source entry in this documentation set by vendor. In a departure from normal configuration, all LEEF products should use the \u201cLEEF\u201d version of the unique port and archive environment variable settings (rather than a unique one per product), as the LEEF log path handles all products sending events to SC4S in the LEEF format. Examples of this include QRadar itself as well as other legacy systems. Therefore, the LEEF environment variables for unique port, archive, etc. should be set only once . If your deployment has multiple LEEF devices that send to more than one port, set the LEEF unique port variable(s) as a comma-separated list. See Unique Listening Ports for details. The source documentation included below is a reference baseline for any product that sends data using the LEEF log path. Some vendors implement LEEF v2.0 format events incorrectly, omitting the required \u201ckey=value\u201d separator field from the LEEF header, thus forcing the consumer to assume the default tab \\t character. SC4S will correctly process this omission, but will not correctly process other non-compliant formats. The LEEF format allows for the inclusion of a field devTime containing the device timestamp and allows the sender to also specify the format of this timestamp in another field called devTimeFormat , which uses the Java Time format. SC4S uses syslog-ng strptime format which is not directly translatable to the Java Time format. Therefore, SC4S has provided support for the following common formats. If needed, additional time formats can be requested via an issue on github. '%s.%f', '%s', '%b %d %H:%M:%S.%f', '%b %d %H:%M:%S', '%b %d %Y %H:%M:%S.%f', '%b %e %Y %H:%M:%S', '%b %e %H:%M:%S.%f', '%b %e %H:%M:%S', '%b %e %Y %H:%M:%S.%f', '%b %e %Y %H:%M:%S' Ref Link Splunk Add-on LEEF None Product Manual https://www.ibm.com/support/knowledgecenter/SS42VS_DSM/com.ibm.dsm.doc/c_LEEF_Format_Guide_intro.html Splunk Metadata with LEEF events \u00b6 The keys (first column) in splunk_metadata.csv for LEEF data sources have a slightly different meaning than those for non-LEEF ones. The typical vendor_product syntax is instead replaced by checks against specific columns of the LEEF event \u2013 namely the first and second, columns following the leading LEEF:VERSION (\u201ccolumn 0\u201d). These specific columns refer to the LEEF device_vendor , and device_product , respectively. device_vendor _ device_product Here is a snippet of a sample LANCOPE event in LEEF 2.0 format: <111>Apr 19 10:29:53 3.3.3.3 LEEF:2.0|Lancope|StealthWatch|1.0|41|^|src=192.0.2.0^dst=172.50.123.1^sev=5^cat=anomaly^srcPort=81^dstPort=21^usrName=joe.black and the corresponding match in splunk_metadata.csv : Lancope_StealthWatch,source,lancope:stealthwatch Default Sourcetype \u00b6 sourcetype notes LEEF:1 Common sourcetype for all LEEF v1 events LEEF:2: <separator> Common sourcetype for all LEEF v2 events separator is the printable literal or hex value of the separator used in the event Default Source \u00b6 source notes vendor : product Varies Default Index Configuration \u00b6 key source index notes Vendor_Product Varies main none Filter type \u00b6 MSG Parse: This filter parses message content Options \u00b6 Variable default description SC4S_LISTEN_LEEF_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_LEEF_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_LEEF_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_LEEF no Enable archive to disk for this specific source SC4S_DEST_LEEF_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source","title":"Log Extended Event Format (LEEF)"},{"location":"sources/base/leef/#log-extended-event-format-leef","text":"","title":"Log Extended Event Format (LEEF)"},{"location":"sources/base/leef/#product-various-products-that-send-leef-v1-and-v2-format-messages-via-syslog","text":"Each LEEF product should have their own source entry in this documentation set by vendor. In a departure from normal configuration, all LEEF products should use the \u201cLEEF\u201d version of the unique port and archive environment variable settings (rather than a unique one per product), as the LEEF log path handles all products sending events to SC4S in the LEEF format. Examples of this include QRadar itself as well as other legacy systems. Therefore, the LEEF environment variables for unique port, archive, etc. should be set only once . If your deployment has multiple LEEF devices that send to more than one port, set the LEEF unique port variable(s) as a comma-separated list. See Unique Listening Ports for details. The source documentation included below is a reference baseline for any product that sends data using the LEEF log path. Some vendors implement LEEF v2.0 format events incorrectly, omitting the required \u201ckey=value\u201d separator field from the LEEF header, thus forcing the consumer to assume the default tab \\t character. SC4S will correctly process this omission, but will not correctly process other non-compliant formats. The LEEF format allows for the inclusion of a field devTime containing the device timestamp and allows the sender to also specify the format of this timestamp in another field called devTimeFormat , which uses the Java Time format. SC4S uses syslog-ng strptime format which is not directly translatable to the Java Time format. Therefore, SC4S has provided support for the following common formats. If needed, additional time formats can be requested via an issue on github. '%s.%f', '%s', '%b %d %H:%M:%S.%f', '%b %d %H:%M:%S', '%b %d %Y %H:%M:%S.%f', '%b %e %Y %H:%M:%S', '%b %e %H:%M:%S.%f', '%b %e %H:%M:%S', '%b %e %Y %H:%M:%S.%f', '%b %e %Y %H:%M:%S' Ref Link Splunk Add-on LEEF None Product Manual https://www.ibm.com/support/knowledgecenter/SS42VS_DSM/com.ibm.dsm.doc/c_LEEF_Format_Guide_intro.html","title":"Product - Various products that send LEEF V1 and V2 format messages via syslog"},{"location":"sources/base/leef/#splunk-metadata-with-leef-events","text":"The keys (first column) in splunk_metadata.csv for LEEF data sources have a slightly different meaning than those for non-LEEF ones. The typical vendor_product syntax is instead replaced by checks against specific columns of the LEEF event \u2013 namely the first and second, columns following the leading LEEF:VERSION (\u201ccolumn 0\u201d). These specific columns refer to the LEEF device_vendor , and device_product , respectively. device_vendor _ device_product Here is a snippet of a sample LANCOPE event in LEEF 2.0 format: <111>Apr 19 10:29:53 3.3.3.3 LEEF:2.0|Lancope|StealthWatch|1.0|41|^|src=192.0.2.0^dst=172.50.123.1^sev=5^cat=anomaly^srcPort=81^dstPort=21^usrName=joe.black and the corresponding match in splunk_metadata.csv : Lancope_StealthWatch,source,lancope:stealthwatch","title":"Splunk Metadata with LEEF events"},{"location":"sources/base/leef/#default-sourcetype","text":"sourcetype notes LEEF:1 Common sourcetype for all LEEF v1 events LEEF:2: <separator> Common sourcetype for all LEEF v2 events separator is the printable literal or hex value of the separator used in the event","title":"Default Sourcetype"},{"location":"sources/base/leef/#default-source","text":"source notes vendor : product Varies","title":"Default Source"},{"location":"sources/base/leef/#default-index-configuration","text":"key source index notes Vendor_Product Varies main none","title":"Default Index Configuration"},{"location":"sources/base/leef/#filter-type","text":"MSG Parse: This filter parses message content","title":"Filter type"},{"location":"sources/base/leef/#options","text":"Variable default description SC4S_LISTEN_LEEF_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_LEEF_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_LEEF_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_LEEF no Enable archive to disk for this specific source SC4S_DEST_LEEF_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source","title":"Options"},{"location":"sources/base/nix/","text":"Generic *NIX \u00b6 Many appliance vendor utilize Linux and BSD distributions as the foundation of the solution. When configured to log via syslog, these devices\u2019 OS logs (from a security perspective) can be monitored using the common Splunk Nix TA. Note: This is NOT a replacement for or alternative to the Splunk Universal forwarder on Linux and Unix. For general-purpose server applications, the Universal Forwarder offers more comprehensive collection of events and metrics appropriate for both security and operations use cases. Ref Link Splunk Add-on https://splunkbase.splunk.com/app/833/ Sourcetypes \u00b6 sourcetype notes nix:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes nix_syslog nix:syslog osnix none Filter type \u00b6 MSG Parse: This filter parses message content Setup and Configuration \u00b6 Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source. Options \u00b6 Variable default description SC4S_ARCHIVE_NIX_SYSLOG no Enable archive to disk for this specific source SC4S_DEST_NIX_SYSLOG_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source","title":"Generic *NIX"},{"location":"sources/base/nix/#generic-nix","text":"Many appliance vendor utilize Linux and BSD distributions as the foundation of the solution. When configured to log via syslog, these devices\u2019 OS logs (from a security perspective) can be monitored using the common Splunk Nix TA. Note: This is NOT a replacement for or alternative to the Splunk Universal forwarder on Linux and Unix. For general-purpose server applications, the Universal Forwarder offers more comprehensive collection of events and metrics appropriate for both security and operations use cases. Ref Link Splunk Add-on https://splunkbase.splunk.com/app/833/","title":"Generic *NIX"},{"location":"sources/base/nix/#sourcetypes","text":"sourcetype notes nix:syslog None","title":"Sourcetypes"},{"location":"sources/base/nix/#sourcetype-and-index-configuration","text":"key sourcetype index notes nix_syslog nix:syslog osnix none","title":"Sourcetype and Index Configuration"},{"location":"sources/base/nix/#filter-type","text":"MSG Parse: This filter parses message content","title":"Filter type"},{"location":"sources/base/nix/#setup-and-configuration","text":"Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source.","title":"Setup and Configuration"},{"location":"sources/base/nix/#options","text":"Variable default description SC4S_ARCHIVE_NIX_SYSLOG no Enable archive to disk for this specific source SC4S_DEST_NIX_SYSLOG_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source","title":"Options"},{"location":"sources/base/simple/","text":"Simple Log path by port \u00b6 The SIMPLE source configuration allows configuration of a log path for SC4S using a single port to a single index/sourcetype combination to quickly onboard new sources that have not been formally supported in the product. Source data must use RFC5424 or a common variant of RFC3164 formatting. NOTE: This is an interim step that should be used only to quickly onboard well-formatted data that is being sent over a unique port. A dedicated log path should be developed for the data source to facilitate further parsing and enrichment, as well as allowing the potential sending of this data source over the default (514) listening port. Splunk Metadata with SIMPLE events \u00b6 The keys (first column) in splunk_metadata.csv for SIMPLE data sources is a user-created key using the vendor_product convention. For example, to on-board a new product first firewall using a source type of first:firewall and index netfw , add the following two lines to the configuration file as shown: first_firewall,index,netfw first_firewall,sourcetype,first:firewall Options \u00b6 For the variables below, replace VENDOR_PRODUCT with the key (converted to upper case) used in the splunk_metadata.csv . Based on the example above, to establish a tcp listener for first firewall we would use SC4S_LISTEN_SIMPLE_FIRST_FIREWALL_TCP_PORT . Variable default description SC4S_LISTEN_SIMPLE_VENDOR_PRODUCT_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_SIMPLE_VENDOR_PRODUCT_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_SIMPLE_VENDOR_PRODUCT_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_SIMPLE_VENDOR_PRODUCT no Enable archive to disk for this specific source SC4S_DEST_SIMPLE_VENDOR_PRODUCT_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source Important Notes \u00b6 SIMPLE data sources must use RFC5424 or a common variant of RFC3164 formatting. Each SIMPLE data source must listen on its own unique port list. Port overlap with other sources, either SIMPLE ones or those served by regular log paths, are not allowed and will cause an error at startup. The key(s) chosen for splunk_metadata.csv must be in the form vendor_product (lower case). These same keys can be used for a regular SC4S log path developed in the future. The SIMPLE environment variables must have a core of VENDOR_PRODUCT (upper case). Take care to remove the SIMPLE form of these LISTEN variables after a regular SC4S log path is developed for a given source. You can, of course, continue to listen for this source on the same unique ports after having developed the new log path, but use the SC4S_LISTEN_<VENDOR_PRODUCT>_<protocol>_PORT form of the variable to ensure the newly developed log path will listen on the specified unique ports.","title":"Simple Log path by port"},{"location":"sources/base/simple/#simple-log-path-by-port","text":"The SIMPLE source configuration allows configuration of a log path for SC4S using a single port to a single index/sourcetype combination to quickly onboard new sources that have not been formally supported in the product. Source data must use RFC5424 or a common variant of RFC3164 formatting. NOTE: This is an interim step that should be used only to quickly onboard well-formatted data that is being sent over a unique port. A dedicated log path should be developed for the data source to facilitate further parsing and enrichment, as well as allowing the potential sending of this data source over the default (514) listening port.","title":"Simple Log path by port"},{"location":"sources/base/simple/#splunk-metadata-with-simple-events","text":"The keys (first column) in splunk_metadata.csv for SIMPLE data sources is a user-created key using the vendor_product convention. For example, to on-board a new product first firewall using a source type of first:firewall and index netfw , add the following two lines to the configuration file as shown: first_firewall,index,netfw first_firewall,sourcetype,first:firewall","title":"Splunk Metadata with SIMPLE events"},{"location":"sources/base/simple/#options","text":"For the variables below, replace VENDOR_PRODUCT with the key (converted to upper case) used in the splunk_metadata.csv . Based on the example above, to establish a tcp listener for first firewall we would use SC4S_LISTEN_SIMPLE_FIRST_FIREWALL_TCP_PORT . Variable default description SC4S_LISTEN_SIMPLE_VENDOR_PRODUCT_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_SIMPLE_VENDOR_PRODUCT_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_SIMPLE_VENDOR_PRODUCT_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_SIMPLE_VENDOR_PRODUCT no Enable archive to disk for this specific source SC4S_DEST_SIMPLE_VENDOR_PRODUCT_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source","title":"Options"},{"location":"sources/base/simple/#important-notes","text":"SIMPLE data sources must use RFC5424 or a common variant of RFC3164 formatting. Each SIMPLE data source must listen on its own unique port list. Port overlap with other sources, either SIMPLE ones or those served by regular log paths, are not allowed and will cause an error at startup. The key(s) chosen for splunk_metadata.csv must be in the form vendor_product (lower case). These same keys can be used for a regular SC4S log path developed in the future. The SIMPLE environment variables must have a core of VENDOR_PRODUCT (upper case). Take care to remove the SIMPLE form of these LISTEN variables after a regular SC4S log path is developed for a given source. You can, of course, continue to listen for this source on the same unique ports after having developed the new log path, but use the SC4S_LISTEN_<VENDOR_PRODUCT>_<protocol>_PORT form of the variable to ensure the newly developed log path will listen on the specified unique ports.","title":"Important Notes"},{"location":"sources/vendor/AVI/","text":"Common \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual https://avinetworks.com/docs/latest/syslog-formats/ Sourcetypes \u00b6 sourcetype notes avi:events None Sourcetype and Index Configuration \u00b6 key sourcetype index notes avi_vantage avi:events netops none","title":"Common"},{"location":"sources/vendor/AVI/#common","text":"","title":"Common"},{"location":"sources/vendor/AVI/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/AVI/#links","text":"Ref Link Splunk Add-on None Product Manual https://avinetworks.com/docs/latest/syslog-formats/","title":"Links"},{"location":"sources/vendor/AVI/#sourcetypes","text":"sourcetype notes avi:events None","title":"Sourcetypes"},{"location":"sources/vendor/AVI/#sourcetype-and-index-configuration","text":"key sourcetype index notes avi_vantage avi:events netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Alcatel/Switch/","text":"Switch \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes alcatel:switch None Sourcetype and Index Configuration \u00b6 key sourcetype index notes alcatel_switch alcatel:switch netops none","title":"Switch"},{"location":"sources/vendor/Alcatel/Switch/#switch","text":"","title":"Switch"},{"location":"sources/vendor/Alcatel/Switch/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Alcatel/Switch/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Alcatel/Switch/#sourcetypes","text":"sourcetype notes alcatel:switch None","title":"Sourcetypes"},{"location":"sources/vendor/Alcatel/Switch/#sourcetype-and-index-configuration","text":"key sourcetype index notes alcatel_switch alcatel:switch netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Alsid/Alsid/","text":"Alsid \u00b6 The product has been purchased and republished under a new product name by Tenable this configuration is obsolete. Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/5173/ Product Manual unknown Sourcetypes \u00b6 sourcetype notes alsid:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes alsid_syslog alsid:syslog oswinsec none","title":"Alsid"},{"location":"sources/vendor/Alsid/Alsid/#alsid","text":"The product has been purchased and republished under a new product name by Tenable this configuration is obsolete.","title":"Alsid"},{"location":"sources/vendor/Alsid/Alsid/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Alsid/Alsid/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/5173/ Product Manual unknown","title":"Links"},{"location":"sources/vendor/Alsid/Alsid/#sourcetypes","text":"sourcetype notes alsid:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Alsid/Alsid/#sourcetype-and-index-configuration","text":"key sourcetype index notes alsid_syslog alsid:syslog oswinsec none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Arista/","text":"EOS \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes arista:eos:* None Sourcetype and Index Configuration \u00b6 key sourcetype index notes arista_eos arista:eos netops none arista_eos_$PROCESSNAME arista:eosq netops The \u201cprocess\u201d field is used from the event","title":"EOS"},{"location":"sources/vendor/Arista/#eos","text":"","title":"EOS"},{"location":"sources/vendor/Arista/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Arista/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Arista/#sourcetypes","text":"sourcetype notes arista:eos:* None","title":"Sourcetypes"},{"location":"sources/vendor/Arista/#sourcetype-and-index-configuration","text":"key sourcetype index notes arista_eos arista:eos netops none arista_eos_$PROCESSNAME arista:eosq netops The \u201cprocess\u201d field is used from the event","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Aruba/ap/","text":"Access Points \u00b6 Key facts \u00b6 MSG Format based filter (Partial) Legacy BSD Format default port 514 Links \u00b6 Ref Link Sourcetypes \u00b6 sourcetype notes aruba:syslog Dynamically Created Index Configuration \u00b6 key index notes aruba_ap netops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-aruba_ap.conf #File name provided is a suggestion it must be globally unique application app - vps - test - aruba_ap [ sc4s - vps ] { filter { host ( \"aruba-ap-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' aruba ' ) product ( ' ap ' ) ); }; };","title":"Access Points"},{"location":"sources/vendor/Aruba/ap/#access-points","text":"","title":"Access Points"},{"location":"sources/vendor/Aruba/ap/#key-facts","text":"MSG Format based filter (Partial) Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Aruba/ap/#links","text":"Ref Link","title":"Links"},{"location":"sources/vendor/Aruba/ap/#sourcetypes","text":"sourcetype notes aruba:syslog Dynamically Created","title":"Sourcetypes"},{"location":"sources/vendor/Aruba/ap/#index-configuration","text":"key index notes aruba_ap netops none","title":"Index Configuration"},{"location":"sources/vendor/Aruba/ap/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-aruba_ap.conf #File name provided is a suggestion it must be globally unique application app - vps - test - aruba_ap [ sc4s - vps ] { filter { host ( \"aruba-ap-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' aruba ' ) product ( ' ap ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Aruba/clearpass/","text":"Clearpass \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Sourcetypes \u00b6 sourcetype notes aruba:clearpass Dynamically Created Index Configuration \u00b6 key index notes aruba_clearpass netops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-aruba_clearpass.conf #File name provided is a suggestion it must be globally unique application app - vps - test - aruba_clearpass [ sc4s - vps ] { filter { host ( \"aruba-cp-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' aruba ' ) product ( ' clearpass ' ) ); }; };","title":"Clearpass"},{"location":"sources/vendor/Aruba/clearpass/#clearpass","text":"","title":"Clearpass"},{"location":"sources/vendor/Aruba/clearpass/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Aruba/clearpass/#links","text":"Ref Link","title":"Links"},{"location":"sources/vendor/Aruba/clearpass/#sourcetypes","text":"sourcetype notes aruba:clearpass Dynamically Created","title":"Sourcetypes"},{"location":"sources/vendor/Aruba/clearpass/#index-configuration","text":"key index notes aruba_clearpass netops none","title":"Index Configuration"},{"location":"sources/vendor/Aruba/clearpass/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-aruba_clearpass.conf #File name provided is a suggestion it must be globally unique application app - vps - test - aruba_clearpass [ sc4s - vps ] { filter { host ( \"aruba-cp-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' aruba ' ) product ( ' clearpass ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Avaya/","text":"SIP Manager \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514/UDP Vendor source is not conformant to RFC3194 by improperly sending unescaped \\n Use of TCP will cause dataloss Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes avaya:avaya None Sourcetype and Index Configuration \u00b6 key sourcetype index notes avaya_sipmgr avaya:avaya main none","title":"SIP Manager"},{"location":"sources/vendor/Avaya/#sip-manager","text":"","title":"SIP Manager"},{"location":"sources/vendor/Avaya/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514/UDP Vendor source is not conformant to RFC3194 by improperly sending unescaped \\n Use of TCP will cause dataloss","title":"Key facts"},{"location":"sources/vendor/Avaya/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Avaya/#sourcetypes","text":"sourcetype notes avaya:avaya None","title":"Sourcetypes"},{"location":"sources/vendor/Avaya/#sourcetype-and-index-configuration","text":"key sourcetype index notes avaya_sipmgr avaya:avaya main none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Barracuda/waf/","text":"WAF (Cloud) \u00b6 Key facts \u00b6 MSG Format based filter RFC 5424 Framed Links \u00b6 Ref Link Splunk Add-on None Product Manual https://campus.barracuda.com/product/WAAS/doc/79462622/log-export Sourcetypes \u00b6 sourcetype notes barracuda:tr none Sourcetype and Index Configuration \u00b6 key sourcetype index notes barracuda_waf barracuda:web:firewall netwaf None Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-barracuda_syslog.conf #File name provided is a suggestion it must be globally unique application app - vps - barracuda_syslog [ sc4s - vps ] { filter { netmask ( 169.254.100.1 / 24 ) or host ( \"barracuda\" type ( string ) flags ( ignore - case )) }; parser { p_set_netsource_fields ( vendor ( ' barracuda ' ) product ( ' syslog ' ) ) ; }; };","title":"WAF (Cloud)"},{"location":"sources/vendor/Barracuda/waf/#waf-cloud","text":"","title":"WAF (Cloud)"},{"location":"sources/vendor/Barracuda/waf/#key-facts","text":"MSG Format based filter RFC 5424 Framed","title":"Key facts"},{"location":"sources/vendor/Barracuda/waf/#links","text":"Ref Link Splunk Add-on None Product Manual https://campus.barracuda.com/product/WAAS/doc/79462622/log-export","title":"Links"},{"location":"sources/vendor/Barracuda/waf/#sourcetypes","text":"sourcetype notes barracuda:tr none","title":"Sourcetypes"},{"location":"sources/vendor/Barracuda/waf/#sourcetype-and-index-configuration","text":"key sourcetype index notes barracuda_waf barracuda:web:firewall netwaf None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Barracuda/waf/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-barracuda_syslog.conf #File name provided is a suggestion it must be globally unique application app - vps - barracuda_syslog [ sc4s - vps ] { filter { netmask ( 169.254.100.1 / 24 ) or host ( \"barracuda\" type ( string ) flags ( ignore - case )) }; parser { p_set_netsource_fields ( vendor ( ' barracuda ' ) product ( ' syslog ' ) ) ; }; };","title":"Parser Configuration"},{"location":"sources/vendor/Barracuda/waf_on_prem/","text":"WAF (On Premises) \u00b6 Key facts \u00b6 RFC 3164 Framed Default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual None Sourcetypes \u00b6 sourcetype notes barracuda:waf none Sourcetype and Index Configuration \u00b6 key sourcetype index notes barracuda_waf barracuda:waf netwaf None","title":"WAF (On Premises)"},{"location":"sources/vendor/Barracuda/waf_on_prem/#waf-on-premises","text":"","title":"WAF (On Premises)"},{"location":"sources/vendor/Barracuda/waf_on_prem/#key-facts","text":"RFC 3164 Framed Default port 514","title":"Key facts"},{"location":"sources/vendor/Barracuda/waf_on_prem/#links","text":"Ref Link Splunk Add-on None Product Manual None","title":"Links"},{"location":"sources/vendor/Barracuda/waf_on_prem/#sourcetypes","text":"sourcetype notes barracuda:waf none","title":"Sourcetypes"},{"location":"sources/vendor/Barracuda/waf_on_prem/#sourcetype-and-index-configuration","text":"key sourcetype index notes barracuda_waf barracuda:waf netwaf None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/BeyondTrust/sra/","text":"Secure Remote Access (Bomgar) \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes beyondtrust:sra None Sourcetype and Index Configuration \u00b6 key sourcetype index notes beyondtrust_sra beyondtrust:sra infraops none Options \u00b6 Variable default description SC4S_DEST_BEYONDTRUST_SRA_SPLUNK_HEC_FMT JSON Restructure data from vendor format to json for splunk destinations set to \u201cNONE\u201d for native format SC4S_DEST_BEYONDTRUST_SRA_SYSLOG_FMT SDATA Restructure data from vendor format to SDATA for SYSLOG destinations set to \u201cNONE\u201d for native ormat","title":"Secure Remote Access (Bomgar)"},{"location":"sources/vendor/BeyondTrust/sra/#secure-remote-access-bomgar","text":"","title":"Secure Remote Access (Bomgar)"},{"location":"sources/vendor/BeyondTrust/sra/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/BeyondTrust/sra/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/BeyondTrust/sra/#sourcetypes","text":"sourcetype notes beyondtrust:sra None","title":"Sourcetypes"},{"location":"sources/vendor/BeyondTrust/sra/#sourcetype-and-index-configuration","text":"key sourcetype index notes beyondtrust_sra beyondtrust:sra infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/BeyondTrust/sra/#options","text":"Variable default description SC4S_DEST_BEYONDTRUST_SRA_SPLUNK_HEC_FMT JSON Restructure data from vendor format to json for splunk destinations set to \u201cNONE\u201d for native format SC4S_DEST_BEYONDTRUST_SRA_SYSLOG_FMT SDATA Restructure data from vendor format to SDATA for SYSLOG destinations set to \u201cNONE\u201d for native ormat","title":"Options"},{"location":"sources/vendor/Broadcom/brightmail/","text":"Brightmail \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on TBD Product Manual https://support.symantec.com/us/en/article.howto38250.html Sourcetypes \u00b6 sourcetype notes symantec:smg Requires version TA 3.6 Sourcetype and Index Configuration \u00b6 key sourcetype index notes symantec_brightmail symantec:smg email none Options \u00b6 Variable default description SC4S_SOURCE_FF_SYMANTEC_BRIGHTMAIL_GROUPMSG yes Email processing events generated by the bmserver process will be grouped by host+program+pid+msg ID into a single event SC4S_DEST_SYMANTEC_BRIGHTMAIL_SPLUNK_HEC_FMT empty if \u201cJSON\u201d and GROUPMSG is enabled format the event in json SC4S_DEST_SYMANTEC_BRIGHTMAIL_SYSLOG_FMT empty if \u201cSDATA\u201d and GROUPMSG is enabled format the event in rfc5424 sdata","title":"Brightmail"},{"location":"sources/vendor/Broadcom/brightmail/#brightmail","text":"","title":"Brightmail"},{"location":"sources/vendor/Broadcom/brightmail/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Broadcom/brightmail/#links","text":"Ref Link Splunk Add-on TBD Product Manual https://support.symantec.com/us/en/article.howto38250.html","title":"Links"},{"location":"sources/vendor/Broadcom/brightmail/#sourcetypes","text":"sourcetype notes symantec:smg Requires version TA 3.6","title":"Sourcetypes"},{"location":"sources/vendor/Broadcom/brightmail/#sourcetype-and-index-configuration","text":"key sourcetype index notes symantec_brightmail symantec:smg email none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Broadcom/brightmail/#options","text":"Variable default description SC4S_SOURCE_FF_SYMANTEC_BRIGHTMAIL_GROUPMSG yes Email processing events generated by the bmserver process will be grouped by host+program+pid+msg ID into a single event SC4S_DEST_SYMANTEC_BRIGHTMAIL_SPLUNK_HEC_FMT empty if \u201cJSON\u201d and GROUPMSG is enabled format the event in json SC4S_DEST_SYMANTEC_BRIGHTMAIL_SYSLOG_FMT empty if \u201cSDATA\u201d and GROUPMSG is enabled format the event in rfc5424 sdata","title":"Options"},{"location":"sources/vendor/Broadcom/dlp/","text":"Symantec DLP \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on Symatec DLP https://splunkbase.splunk.com/app/3029/ Source doc https://knowledge.broadcom.com/external/article/159509/generating-syslog-messages-from-data-los.html Sourcetypes \u00b6 sourcetype notes symantec:dlp:syslog None Index Configuration \u00b6 key sourcetype index notes symantec_dlp symantec:dlp:syslog netdlp none Option 1: Correct Source syslog formats \u00b6 Syslog Alert Response \u00b6 Login to Symantec DLP and edit the Syslog Response rule. The default configuration will appear as follows $POLICY$^^$INCIDENT_ID$^^$SUBJECT$^^$SEVERITY$^^$MATCH_COUNT$^^$RULES$^^$SENDER$^^$RECIPIENTS$^^$BLOCKED$^^$FILE_NAME$^^$PARENT_PATH$^^$SCAN$^^$TARGET$^^$PROTOCOL$^^$INCIDENT_SNAPSHOT$ DO NOT replace the text prepend the following literal SymantecDLPAlert: Result note the space between \u2018:\u2019 and \u2018$\u2019 SymantecDLPAlert: $POLICY$^^$INCIDENT_ID$^^$SUBJECT$^^$SEVERITY$^^$MATCH_COUNT$^^$RULES$^^$SENDER$^^$RECIPIENTS$^^$BLOCKED$^^$FILE_NAME$^^$PARENT_PATH$^^$SCAN$^^$TARGET$^^$PROTOCOL$^^$INCIDENT_SNAPSHOT$ Syslog System events \u00b6 Navigate to the installed directory, for example <drive>:\\SymantecDLP\\Protect\\config directory on Windows or the /opt/SymantecDLP/Protect/config directory on Linux. Open the Manager.properties file. Comment out any uncommented line starting with systemevent.syslog.format Add the following line systemevent.syslog.format= {0.EN_US} SymantecDLP: {1.EN_US} - {2.EN_US} Restart symantec DLP Option 2: Manual Vendor Product by source Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-symantec_dlp.conf #File name provided is a suggestion it must be globally unique application app - vps - test - symantec_dlp [ sc4s - vps ] { filter { #netmask(169.254.100.1/24) #host(\"-esx-\") }; parser { p_set_netsource_fields ( vendor ( ' symantec ' ) product ( ' dlp ' ) ); }; };","title":"Symantec DLP"},{"location":"sources/vendor/Broadcom/dlp/#symantec-dlp","text":"","title":"Symantec DLP"},{"location":"sources/vendor/Broadcom/dlp/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Broadcom/dlp/#links","text":"Ref Link Splunk Add-on Symatec DLP https://splunkbase.splunk.com/app/3029/ Source doc https://knowledge.broadcom.com/external/article/159509/generating-syslog-messages-from-data-los.html","title":"Links"},{"location":"sources/vendor/Broadcom/dlp/#sourcetypes","text":"sourcetype notes symantec:dlp:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Broadcom/dlp/#index-configuration","text":"key sourcetype index notes symantec_dlp symantec:dlp:syslog netdlp none","title":"Index Configuration"},{"location":"sources/vendor/Broadcom/dlp/#option-1-correct-source-syslog-formats","text":"","title":"Option 1: Correct Source syslog formats"},{"location":"sources/vendor/Broadcom/dlp/#syslog-alert-response","text":"Login to Symantec DLP and edit the Syslog Response rule. The default configuration will appear as follows $POLICY$^^$INCIDENT_ID$^^$SUBJECT$^^$SEVERITY$^^$MATCH_COUNT$^^$RULES$^^$SENDER$^^$RECIPIENTS$^^$BLOCKED$^^$FILE_NAME$^^$PARENT_PATH$^^$SCAN$^^$TARGET$^^$PROTOCOL$^^$INCIDENT_SNAPSHOT$ DO NOT replace the text prepend the following literal SymantecDLPAlert: Result note the space between \u2018:\u2019 and \u2018$\u2019 SymantecDLPAlert: $POLICY$^^$INCIDENT_ID$^^$SUBJECT$^^$SEVERITY$^^$MATCH_COUNT$^^$RULES$^^$SENDER$^^$RECIPIENTS$^^$BLOCKED$^^$FILE_NAME$^^$PARENT_PATH$^^$SCAN$^^$TARGET$^^$PROTOCOL$^^$INCIDENT_SNAPSHOT$","title":"Syslog Alert Response"},{"location":"sources/vendor/Broadcom/dlp/#syslog-system-events","text":"Navigate to the installed directory, for example <drive>:\\SymantecDLP\\Protect\\config directory on Windows or the /opt/SymantecDLP/Protect/config directory on Linux. Open the Manager.properties file. Comment out any uncommented line starting with systemevent.syslog.format Add the following line systemevent.syslog.format= {0.EN_US} SymantecDLP: {1.EN_US} - {2.EN_US} Restart symantec DLP","title":"Syslog System events"},{"location":"sources/vendor/Broadcom/dlp/#option-2-manual-vendor-product-by-source-parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-symantec_dlp.conf #File name provided is a suggestion it must be globally unique application app - vps - test - symantec_dlp [ sc4s - vps ] { filter { #netmask(169.254.100.1/24) #host(\"-esx-\") }; parser { p_set_netsource_fields ( vendor ( ' symantec ' ) product ( ' dlp ' ) ); }; };","title":"Option 2: Manual Vendor Product by source Parser Configuration"},{"location":"sources/vendor/Broadcom/ep/","text":"Symantec Endpoint Protection (SEPM) \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 KNOWN DATA LOSS ISSUE - The implementation of the syslog output component causes a \u201cburst\u201d behavior when run on schedule this burst can be larger than the udp buffer size on the source and or destination (sc4s) there is no possible workaround and the use of the Splunk Universal Forwarder to monitor file based output is recommended. Product - Symantec Endpoint Protection \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2772/ Product Manual https://techdocs.broadcom.com/content/broadcom/techdocs/us/en/symantec-security-software/endpoint-security-and-management/endpoint-protection/all/Monitoring-Reporting-and-Enforcing-Compliance/viewing-logs-v7522439-d37e464/exporting-data-to-a-syslog-server-v8442743-d15e1107.html Sourcetypes \u00b6 sourcetype notes symantec:ep:syslog Warning the syslog method of accepting EP logs has been reported to show high data loss and is not Supported by Splunk symantec:ep:admin:syslog none symantec:ep:agent:syslog none symantec:ep:agt:system:syslog none symantec:ep:behavior:syslog none symantec:ep:packet:syslog none symantec:ep:policy:syslog none symantec:ep:proactive:syslog none symantec:ep:risk:syslog none symantec:ep:scan:syslog none symantec:ep:scm:system:syslog none symantec:ep:security:syslog none symantec:ep:traffic:syslog none Index Configuration \u00b6 key index notes symantec_ep epav none","title":"Symantec Endpoint Protection (SEPM)"},{"location":"sources/vendor/Broadcom/ep/#symantec-endpoint-protection-sepm","text":"","title":"Symantec Endpoint Protection (SEPM)"},{"location":"sources/vendor/Broadcom/ep/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514 KNOWN DATA LOSS ISSUE - The implementation of the syslog output component causes a \u201cburst\u201d behavior when run on schedule this burst can be larger than the udp buffer size on the source and or destination (sc4s) there is no possible workaround and the use of the Splunk Universal Forwarder to monitor file based output is recommended.","title":"Key facts"},{"location":"sources/vendor/Broadcom/ep/#product-symantec-endpoint-protection","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2772/ Product Manual https://techdocs.broadcom.com/content/broadcom/techdocs/us/en/symantec-security-software/endpoint-security-and-management/endpoint-protection/all/Monitoring-Reporting-and-Enforcing-Compliance/viewing-logs-v7522439-d37e464/exporting-data-to-a-syslog-server-v8442743-d15e1107.html","title":"Product - Symantec Endpoint Protection"},{"location":"sources/vendor/Broadcom/ep/#sourcetypes","text":"sourcetype notes symantec:ep:syslog Warning the syslog method of accepting EP logs has been reported to show high data loss and is not Supported by Splunk symantec:ep:admin:syslog none symantec:ep:agent:syslog none symantec:ep:agt:system:syslog none symantec:ep:behavior:syslog none symantec:ep:packet:syslog none symantec:ep:policy:syslog none symantec:ep:proactive:syslog none symantec:ep:risk:syslog none symantec:ep:scan:syslog none symantec:ep:scm:system:syslog none symantec:ep:security:syslog none symantec:ep:traffic:syslog none","title":"Sourcetypes"},{"location":"sources/vendor/Broadcom/ep/#index-configuration","text":"key index notes symantec_ep epav none","title":"Index Configuration"},{"location":"sources/vendor/Broadcom/proxy/","text":"ProxySG/ASG \u00b6 Symantec now Broadcom ProxySG/ASG is formerly known as the \u201cBluecoat\u201d proxy Broadcom products are inclusive of products formerly marketed under Symantec and Bluecoat brands. Key facts \u00b6 MSG Format based filter The standard/default bluecoat syslog configurations are NOT supported a SC4S specific configuration is provided below RFC5424 without IETF Frame must use 514/TCP or 6514/TLS Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2758/ Product Manual https://support.symantec.com/us/en/article.tech242216.html Sourcetypes \u00b6 sourcetype notes bluecoat:proxysg:access:kv Requires version TA 3.6 bluecoat:proxysg:syslog Requires version TA 3.6 Sourcetype and Index Configuration \u00b6 key sourcetype index notes bluecoat_proxy bluecoat:proxysg:syslog netops none bluecoat_proxy_splunkkv bluecoat:proxysg:access:kv netproxy none Setup and Configuration \u00b6 Refer to the Splunk TA documentation for the specific customer format required for proxy configuration Select TCP or SSL transport option Ensure the format of the event is customized as follows < 111 > 1 $ ( date ) T $ ( x - bluecoat - hour - utc ) :$ ( x - bluecoat - minute - utc ) :$ ( x - bluecoat - second - utc ) Z $ ( s - computername ) ProxySG - splunk_format - c - ip = $ ( c - ip ) rs - Content - Type = $ ( quot ) $ ( rs ( Content - Type )) $ ( quot ) cs - auth - groups = $ ( cs - auth - groups ) cs - bytes = $ ( cs - bytes ) cs - categories = $ ( cs - categories ) cs - host = $ ( cs - host ) cs - ip = $ ( cs - ip ) cs - method = $ ( cs - method ) cs - uri - port = $ ( cs - uri - port ) cs - uri - scheme = $ ( cs - uri - scheme ) cs - User - Agent = $ ( quot ) $ ( cs ( User - Agent )) $ ( quot ) cs - username = $ ( cs - username ) dnslookup - time = $ ( dnslookup - time ) duration = $ ( duration ) rs - status = $ ( rs - status ) rs - version = $ ( rs - version ) s - action = $ ( s - action ) s - ip = $ ( s - ip ) service . name = $ ( service . name ) service . group = $ ( service . group ) s - supplier - ip = $ ( s - supplier - ip ) s - supplier - name = $ ( s - supplier - name ) sc - bytes = $ ( sc - bytes ) sc - filter - result = $ ( sc - filter - result ) sc - status = $ ( sc - status ) time - taken = $ ( time - taken ) x - exception - id = $ ( x - exception - id ) x - virus - id = $ ( x - virus - id ) c - url = $ ( quot ) $ ( url ) $ ( quot ) cs - Referer = $ ( quot ) $ ( cs ( Referer )) $ ( quot ) c - cpu = $ ( c - cpu ) connect - time = $ ( connect - time ) cs - auth - groups = $ ( cs - auth - groups ) cs - headerlength = $ ( cs - headerlength ) cs - threat - risk = $ ( cs - threat - risk ) r - ip = $ ( r - ip ) r - supplier - ip = $ ( r - supplier - ip ) rs - time - taken = $ ( rs - time - taken ) rs - server = $ ( rs ( server )) s - connect - type = $ ( s - connect - type ) s - icap - status = $ ( s - icap - status ) s - sitename = $ ( s - sitename ) s - source - port = $ ( s - source - port ) s - supplier - country = $ ( s - supplier - country ) sc - Content - Encoding = $ ( sc ( Content - Encoding )) sr - Accept - Encoding = $ ( sr ( Accept - Encoding )) x - auth - credential - type = $ ( x - auth - credential - type ) x - cookie - date = $ ( x - cookie - date ) x - cs - certificate - subject = $ ( x - cs - certificate - subject ) x - cs - connection - negotiated - cipher = $ ( x - cs - connection - negotiated - cipher ) x - cs - connection - negotiated - cipher - size = $ ( x - cs - connection - negotiated - cipher - size ) x - cs - connection - negotiated - ssl - version = $ ( x - cs - connection - negotiated - ssl - version ) x - cs - ocsp - error = $ ( x - cs - ocsp - error ) x - cs - Referer - uri = $ ( x - cs ( Referer ) - uri ) x - cs - Referer - uri - address = $ ( x - cs ( Referer ) - uri - address ) x - cs - Referer - uri - extension = $ ( x - cs ( Referer ) - uri - extension ) x - cs - Referer - uri - host = $ ( x - cs ( Referer ) - uri - host ) x - cs - Referer - uri - hostname = $ ( x - cs ( Referer ) - uri - hostname ) x - cs - Referer - uri - path = $ ( x - cs ( Referer ) - uri - path ) x - cs - Referer - uri - pathquery = $ ( x - cs ( Referer ) - uri - pathquery ) x - cs - Referer - uri - port = $ ( x - cs ( Referer ) - uri - port ) x - cs - Referer - uri - query = $ ( x - cs ( Referer ) - uri - query ) x - cs - Referer - uri - scheme = $ ( x - cs ( Referer ) - uri - scheme ) x - cs - Referer - uri - stem = $ ( x - cs ( Referer ) - uri - stem ) x - exception - category = $ ( x - exception - category ) x - exception - category - review - message = $ ( x - exception - category - review - message ) x - exception - company - name = $ ( x - exception - company - name ) x - exception - contact = $ ( x - exception - contact ) x - exception - details = $ ( x - exception - details ) x - exception - header = $ ( x - exception - header ) x - exception - help = $ ( x - exception - help ) x - exception - last - error = $ ( x - exception - last - error ) x - exception - reason = $ ( x - exception - reason ) x - exception - sourcefile = $ ( x - exception - sourcefile ) x - exception - sourceline = $ ( x - exception - sourceline ) x - exception - summary = $ ( x - exception - summary ) x - icap - error - code = $ ( x - icap - error - code ) x - rs - certificate - hostname = $ ( x - rs - certificate - hostname ) x - rs - certificate - hostname - category = $ ( x - rs - certificate - hostname - category ) x - rs - certificate - observed - errors = $ ( x - rs - certificate - observed - errors ) x - rs - certificate - subject = $ ( x - rs - certificate - subject ) x - rs - certificate - validate - status = $ ( x - rs - certificate - validate - status ) x - rs - connection - negotiated - cipher = $ ( x - rs - connection - negotiated - cipher ) x - rs - connection - negotiated - cipher - size = $ ( x - rs - connection - negotiated - cipher - size ) x - rs - connection - negotiated - ssl - version = $ ( x - rs - connection - negotiated - ssl - version ) x - rs - ocsp - error = $ ( x - rs - ocsp - error ) cs - uri - extension = $ ( cs - uri - extension ) cs - uri - path = $ ( cs - uri - path ) cs - uri - query = $ ( quot ) $ ( cs - uri - query ) $ ( quot ) c - uri - pathquery = $ ( c - uri - pathquery )","title":"ProxySG/ASG"},{"location":"sources/vendor/Broadcom/proxy/#proxysgasg","text":"Symantec now Broadcom ProxySG/ASG is formerly known as the \u201cBluecoat\u201d proxy Broadcom products are inclusive of products formerly marketed under Symantec and Bluecoat brands.","title":"ProxySG/ASG"},{"location":"sources/vendor/Broadcom/proxy/#key-facts","text":"MSG Format based filter The standard/default bluecoat syslog configurations are NOT supported a SC4S specific configuration is provided below RFC5424 without IETF Frame must use 514/TCP or 6514/TLS","title":"Key facts"},{"location":"sources/vendor/Broadcom/proxy/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2758/ Product Manual https://support.symantec.com/us/en/article.tech242216.html","title":"Links"},{"location":"sources/vendor/Broadcom/proxy/#sourcetypes","text":"sourcetype notes bluecoat:proxysg:access:kv Requires version TA 3.6 bluecoat:proxysg:syslog Requires version TA 3.6","title":"Sourcetypes"},{"location":"sources/vendor/Broadcom/proxy/#sourcetype-and-index-configuration","text":"key sourcetype index notes bluecoat_proxy bluecoat:proxysg:syslog netops none bluecoat_proxy_splunkkv bluecoat:proxysg:access:kv netproxy none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Broadcom/proxy/#setup-and-configuration","text":"Refer to the Splunk TA documentation for the specific customer format required for proxy configuration Select TCP or SSL transport option Ensure the format of the event is customized as follows < 111 > 1 $ ( date ) T $ ( x - bluecoat - hour - utc ) :$ ( x - bluecoat - minute - utc ) :$ ( x - bluecoat - second - utc ) Z $ ( s - computername ) ProxySG - splunk_format - c - ip = $ ( c - ip ) rs - Content - Type = $ ( quot ) $ ( rs ( Content - Type )) $ ( quot ) cs - auth - groups = $ ( cs - auth - groups ) cs - bytes = $ ( cs - bytes ) cs - categories = $ ( cs - categories ) cs - host = $ ( cs - host ) cs - ip = $ ( cs - ip ) cs - method = $ ( cs - method ) cs - uri - port = $ ( cs - uri - port ) cs - uri - scheme = $ ( cs - uri - scheme ) cs - User - Agent = $ ( quot ) $ ( cs ( User - Agent )) $ ( quot ) cs - username = $ ( cs - username ) dnslookup - time = $ ( dnslookup - time ) duration = $ ( duration ) rs - status = $ ( rs - status ) rs - version = $ ( rs - version ) s - action = $ ( s - action ) s - ip = $ ( s - ip ) service . name = $ ( service . name ) service . group = $ ( service . group ) s - supplier - ip = $ ( s - supplier - ip ) s - supplier - name = $ ( s - supplier - name ) sc - bytes = $ ( sc - bytes ) sc - filter - result = $ ( sc - filter - result ) sc - status = $ ( sc - status ) time - taken = $ ( time - taken ) x - exception - id = $ ( x - exception - id ) x - virus - id = $ ( x - virus - id ) c - url = $ ( quot ) $ ( url ) $ ( quot ) cs - Referer = $ ( quot ) $ ( cs ( Referer )) $ ( quot ) c - cpu = $ ( c - cpu ) connect - time = $ ( connect - time ) cs - auth - groups = $ ( cs - auth - groups ) cs - headerlength = $ ( cs - headerlength ) cs - threat - risk = $ ( cs - threat - risk ) r - ip = $ ( r - ip ) r - supplier - ip = $ ( r - supplier - ip ) rs - time - taken = $ ( rs - time - taken ) rs - server = $ ( rs ( server )) s - connect - type = $ ( s - connect - type ) s - icap - status = $ ( s - icap - status ) s - sitename = $ ( s - sitename ) s - source - port = $ ( s - source - port ) s - supplier - country = $ ( s - supplier - country ) sc - Content - Encoding = $ ( sc ( Content - Encoding )) sr - Accept - Encoding = $ ( sr ( Accept - Encoding )) x - auth - credential - type = $ ( x - auth - credential - type ) x - cookie - date = $ ( x - cookie - date ) x - cs - certificate - subject = $ ( x - cs - certificate - subject ) x - cs - connection - negotiated - cipher = $ ( x - cs - connection - negotiated - cipher ) x - cs - connection - negotiated - cipher - size = $ ( x - cs - connection - negotiated - cipher - size ) x - cs - connection - negotiated - ssl - version = $ ( x - cs - connection - negotiated - ssl - version ) x - cs - ocsp - error = $ ( x - cs - ocsp - error ) x - cs - Referer - uri = $ ( x - cs ( Referer ) - uri ) x - cs - Referer - uri - address = $ ( x - cs ( Referer ) - uri - address ) x - cs - Referer - uri - extension = $ ( x - cs ( Referer ) - uri - extension ) x - cs - Referer - uri - host = $ ( x - cs ( Referer ) - uri - host ) x - cs - Referer - uri - hostname = $ ( x - cs ( Referer ) - uri - hostname ) x - cs - Referer - uri - path = $ ( x - cs ( Referer ) - uri - path ) x - cs - Referer - uri - pathquery = $ ( x - cs ( Referer ) - uri - pathquery ) x - cs - Referer - uri - port = $ ( x - cs ( Referer ) - uri - port ) x - cs - Referer - uri - query = $ ( x - cs ( Referer ) - uri - query ) x - cs - Referer - uri - scheme = $ ( x - cs ( Referer ) - uri - scheme ) x - cs - Referer - uri - stem = $ ( x - cs ( Referer ) - uri - stem ) x - exception - category = $ ( x - exception - category ) x - exception - category - review - message = $ ( x - exception - category - review - message ) x - exception - company - name = $ ( x - exception - company - name ) x - exception - contact = $ ( x - exception - contact ) x - exception - details = $ ( x - exception - details ) x - exception - header = $ ( x - exception - header ) x - exception - help = $ ( x - exception - help ) x - exception - last - error = $ ( x - exception - last - error ) x - exception - reason = $ ( x - exception - reason ) x - exception - sourcefile = $ ( x - exception - sourcefile ) x - exception - sourceline = $ ( x - exception - sourceline ) x - exception - summary = $ ( x - exception - summary ) x - icap - error - code = $ ( x - icap - error - code ) x - rs - certificate - hostname = $ ( x - rs - certificate - hostname ) x - rs - certificate - hostname - category = $ ( x - rs - certificate - hostname - category ) x - rs - certificate - observed - errors = $ ( x - rs - certificate - observed - errors ) x - rs - certificate - subject = $ ( x - rs - certificate - subject ) x - rs - certificate - validate - status = $ ( x - rs - certificate - validate - status ) x - rs - connection - negotiated - cipher = $ ( x - rs - connection - negotiated - cipher ) x - rs - connection - negotiated - cipher - size = $ ( x - rs - connection - negotiated - cipher - size ) x - rs - connection - negotiated - ssl - version = $ ( x - rs - connection - negotiated - ssl - version ) x - rs - ocsp - error = $ ( x - rs - ocsp - error ) cs - uri - extension = $ ( cs - uri - extension ) cs - uri - path = $ ( cs - uri - path ) cs - uri - query = $ ( quot ) $ ( cs - uri - query ) $ ( quot ) c - uri - pathquery = $ ( c - uri - pathquery )","title":"Setup and Configuration"},{"location":"sources/vendor/Broadcom/sslva/","text":"SSL Visibility Appliance \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual https://knowledge.broadcom.com/external/article/168879/when-sending-session-logs-from-ssl-visib.html Sourcetypes \u00b6 sourcetype notes broadcom:sslva none Index Configuration \u00b6 key index notes broadcom_sslva netproxy none","title":"SSL Visibility Appliance"},{"location":"sources/vendor/Broadcom/sslva/#ssl-visibility-appliance","text":"","title":"SSL Visibility Appliance"},{"location":"sources/vendor/Broadcom/sslva/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Broadcom/sslva/#links","text":"Ref Link Splunk Add-on None Product Manual https://knowledge.broadcom.com/external/article/168879/when-sending-session-logs-from-ssl-visib.html","title":"Links"},{"location":"sources/vendor/Broadcom/sslva/#sourcetypes","text":"sourcetype notes broadcom:sslva none","title":"Sourcetypes"},{"location":"sources/vendor/Broadcom/sslva/#index-configuration","text":"key index notes broadcom_sslva netproxy none","title":"Index Configuration"},{"location":"sources/vendor/Brocade/switch/","text":"Switch \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Product - Switches \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes brocade:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes brocade_syslog brocade:syslog netops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app_parsers/app-vps-brocade_syslog.conf #File name provided is a suggestion it must be globally unique application app - vps - test - brocade_syslog [ sc4s - vps ] { filter { host ( \"^test_brocade-\" ) }; parser { p_set_netsource_fields ( vendor ( ' brocade ' ) product ( ' syslog ' ) ); }; };","title":"Switch"},{"location":"sources/vendor/Brocade/switch/#switch","text":"","title":"Switch"},{"location":"sources/vendor/Brocade/switch/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Brocade/switch/#product-switches","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Product - Switches"},{"location":"sources/vendor/Brocade/switch/#sourcetypes","text":"sourcetype notes brocade:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Brocade/switch/#sourcetype-and-index-configuration","text":"key sourcetype index notes brocade_syslog brocade:syslog netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Brocade/switch/#parser-configuration","text":"#/opt/sc4s/local/config/app_parsers/app-vps-brocade_syslog.conf #File name provided is a suggestion it must be globally unique application app - vps - test - brocade_syslog [ sc4s - vps ] { filter { host ( \"^test_brocade-\" ) }; parser { p_set_netsource_fields ( vendor ( ' brocade ' ) product ( ' syslog ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Buffalo/","text":"Terastation \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes buffalo:terastation None Sourcetype and Index Configuration \u00b6 key sourcetype index notes buffalo_terastation buffalo:terastation infraops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-buffalo_terastation.conf #File name provided is a suggestion it must be globally unique application app - vps - test - buffalo_terastation [ sc4s - vps ] { filter { host ( \"^test_buffalo_terastation-\" ) }; parser { p_set_netsource_fields ( vendor ( ' buffalo ' ) product ( ' terastation ' ) ); }; };","title":"Terastation"},{"location":"sources/vendor/Buffalo/#terastation","text":"","title":"Terastation"},{"location":"sources/vendor/Buffalo/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Buffalo/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Buffalo/#sourcetypes","text":"sourcetype notes buffalo:terastation None","title":"Sourcetypes"},{"location":"sources/vendor/Buffalo/#sourcetype-and-index-configuration","text":"key sourcetype index notes buffalo_terastation buffalo:terastation infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Buffalo/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-buffalo_terastation.conf #File name provided is a suggestion it must be globally unique application app - vps - test - buffalo_terastation [ sc4s - vps ] { filter { host ( \"^test_buffalo_terastation-\" ) }; parser { p_set_netsource_fields ( vendor ( ' buffalo ' ) product ( ' terastation ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Checkpoint/firewallos/","text":"Firewall OS \u00b6 Firewall OS format is by devices supporting a direct Syslog output Links \u00b6 Ref Link Splunk Add-on na Product Manual unknown Sourcetypes \u00b6 sourcetype notes cp_log:fw:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes checkpoint_fw cp_log:fw:syslog netops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-checkpoint_fw.conf #File name provided is a suggestion it must be globally unique application app - vps - test - checkpoint_fw [ sc4s - vps ] { filter { host ( \"^checkpoint_fw-\" ) }; parser { p_set_netsource_fields ( vendor ( ' checkpoint ' ) product ( ' fw ' ) ); }; };","title":"Firewall OS"},{"location":"sources/vendor/Checkpoint/firewallos/#firewall-os","text":"Firewall OS format is by devices supporting a direct Syslog output","title":"Firewall OS"},{"location":"sources/vendor/Checkpoint/firewallos/#links","text":"Ref Link Splunk Add-on na Product Manual unknown","title":"Links"},{"location":"sources/vendor/Checkpoint/firewallos/#sourcetypes","text":"sourcetype notes cp_log:fw:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Checkpoint/firewallos/#sourcetype-and-index-configuration","text":"key sourcetype index notes checkpoint_fw cp_log:fw:syslog netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Checkpoint/firewallos/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-checkpoint_fw.conf #File name provided is a suggestion it must be globally unique application app - vps - test - checkpoint_fw [ sc4s - vps ] { filter { host ( \"^checkpoint_fw-\" ) }; parser { p_set_netsource_fields ( vendor ( ' checkpoint ' ) product ( ' fw ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Checkpoint/logexporter_5424/","text":"Log Exporter (Syslog) \u00b6 Key Facts \u00b6 As of 2/1/2022 The Log Exporter configuration provided by CheckPoint is defective and produces invalid data the configuration below is REQUIRED MSG Format based filter RFC5424 without frame use port 514 TCP Ref Link Splunk Add-on Product Manual https://sc1.checkpoint.com/documents/App_for_Splunk/html_frameset.htm Sourcetypes \u00b6 sourcetype notes cp_log:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes checkpoint_syslog cp_log:syslog netops none Source and Index Configuration \u00b6 Checkpoint Software blades with CIM mapping have been sub-grouped into sources to allow routing to appropriate indexes. All other source meta data is left at default key source index notes checkpoint_syslog_dlp dlp netdlp none checkpoint_syslog_email email email none checkpoint_syslog_firewall firewall netfw none checkpoint_syslog_sessions sessions netops none checkpoint_syslog_web web netproxy none checkpoint_syslog_audit audit netops none checkpoint_syslog_endpoint endpoint netops none checkpoint_syslog_network network netops checkpoint_syslog_ids ids netids checkpoint_syslog_ids_malware ids_malware netids Source Configuration \u00b6 Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source. To configure the valid syslog format in Checkpoint, follow the steps below Go to the cp terminal Enter expert command for login in expert mode Enter cd $EXPORTERDIR In this directory check targets if it\u2019s empty then configure a new target for the logs with help of below command cp_log_export add name target-server target-port protocol <(udp|tcp)> format <(syslog)|(cef)|(splunk)(generic)> Then navigate to conf directory Execute cp SyslogFormatDefinition.xml SplunkRecommendedFormatDefinition.xml Open SplunkRecommendedFormatDefinition.xml in edit mode and modify the start_message_body,fields_separator,field_value_separator as shown below. <start_message_body> [sc4s@2620 </start_message_body> <fields_separator> </fields_separator> <field_value_separator> = </field_value_separator> Copy SplunkRecommendedFormatDefinition.xml into $EXPORTERDIR/targets/ /conf Navigate to the configuration file $EXPORTERDIR/targets/ /targetConfiguration.xml and open it in edit mode. Add the reference to the SplunkRecommendedFormatDefinition.xml under the key . For example, if $EXPORTERDIR=/opt/CPrt-R81/log_exporter, the absolute path will become: <formatHeaderFile> /opt/CPrt-R81/log_exporter/targets/ <your_log_exporter> /conf/SplunkRecommendedFormatDefinition.xml </formatHeaderFile> Restart cp_log_exporter by executing the command cp_log_export restart name Warning: Make sure if you migrating to different format, the earlier format is disabled or else it would lead to data duplication.","title":"Log Exporter (Syslog)"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#log-exporter-syslog","text":"","title":"Log Exporter (Syslog)"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#key-facts","text":"As of 2/1/2022 The Log Exporter configuration provided by CheckPoint is defective and produces invalid data the configuration below is REQUIRED MSG Format based filter RFC5424 without frame use port 514 TCP Ref Link Splunk Add-on Product Manual https://sc1.checkpoint.com/documents/App_for_Splunk/html_frameset.htm","title":"Key Facts"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#sourcetypes","text":"sourcetype notes cp_log:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#sourcetype-and-index-configuration","text":"key sourcetype index notes checkpoint_syslog cp_log:syslog netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#source-and-index-configuration","text":"Checkpoint Software blades with CIM mapping have been sub-grouped into sources to allow routing to appropriate indexes. All other source meta data is left at default key source index notes checkpoint_syslog_dlp dlp netdlp none checkpoint_syslog_email email email none checkpoint_syslog_firewall firewall netfw none checkpoint_syslog_sessions sessions netops none checkpoint_syslog_web web netproxy none checkpoint_syslog_audit audit netops none checkpoint_syslog_endpoint endpoint netops none checkpoint_syslog_network network netops checkpoint_syslog_ids ids netids checkpoint_syslog_ids_malware ids_malware netids","title":"Source and Index Configuration"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#source-configuration","text":"Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source. To configure the valid syslog format in Checkpoint, follow the steps below Go to the cp terminal Enter expert command for login in expert mode Enter cd $EXPORTERDIR In this directory check targets if it\u2019s empty then configure a new target for the logs with help of below command cp_log_export add name target-server target-port protocol <(udp|tcp)> format <(syslog)|(cef)|(splunk)(generic)> Then navigate to conf directory Execute cp SyslogFormatDefinition.xml SplunkRecommendedFormatDefinition.xml Open SplunkRecommendedFormatDefinition.xml in edit mode and modify the start_message_body,fields_separator,field_value_separator as shown below. <start_message_body> [sc4s@2620 </start_message_body> <fields_separator> </fields_separator> <field_value_separator> = </field_value_separator> Copy SplunkRecommendedFormatDefinition.xml into $EXPORTERDIR/targets/ /conf Navigate to the configuration file $EXPORTERDIR/targets/ /targetConfiguration.xml and open it in edit mode. Add the reference to the SplunkRecommendedFormatDefinition.xml under the key . For example, if $EXPORTERDIR=/opt/CPrt-R81/log_exporter, the absolute path will become: <formatHeaderFile> /opt/CPrt-R81/log_exporter/targets/ <your_log_exporter> /conf/SplunkRecommendedFormatDefinition.xml </formatHeaderFile> Restart cp_log_exporter by executing the command cp_log_export restart name Warning: Make sure if you migrating to different format, the earlier format is disabled or else it would lead to data duplication.","title":"Source Configuration"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/","text":"Log Exporter (Splunk) \u00b6 The \u201cSplunk Format\u201d is legacy and should not be used for new deployments see Log Exporter (Syslog) Key Facts \u00b6 Format is not conformant to RFC3164 avoid use MSG Format based filter Legacy BSD Format default port 514 The Splunk host field will be derived as follows using the first match Use the hostname field Use the first CN component of origin_sic_name/originsicname If host is not set from CN use the hostname field If host is not set use the BSD syslog header host If the host is in the format <host>-v_<bladename> use bladename for host Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4293/ Product Manual https://sc1.checkpoint.com/documents/App_for_Splunk/html_frameset.htm Sourcetypes \u00b6 sourcetype notes cp_log None Sourcetype and Index Configuration \u00b6 key sourcetype index notes checkpoint_splunk cp_log netops none Source and Index Configuration \u00b6 Checkpoint Software blades with CIM mapping have been sub-grouped into sources to allow routing to appropriate indexes. All other source meta data is left at default key source index notes checkpoint_splunk_dlp dlp netdlp none checkpoint_splunk_email email email none checkpoint_splunk_firewall firewall netfw none checkpoint_splunk_os program:${program} netops none checkpoint_splunk_sessions sessions netops none checkpoint_splunk_web web netproxy none checkpoint_splunk_audit audit netops none checkpoint_splunk_endpoint endpoint netops none checkpoint_splunk_network network netops checkpoint_splunk_ids ids netids checkpoint_splunk_ids_malware ids_malware netids Options \u00b6 Variable default description SC4S_LISTEN_CHECKPOINT_SPLUNK_NOISE_CONTROL no Suppress any duplicate product+loguid pairs processed within 2 seconds of the last matching event SC4S_LISTEN_CHECKPOINT_SPLUNK_OLD_HOST_RULES empty string when set to yes reverts host name selection order to originsicname\u2013>origin_sic_name\u2013>hostname","title":"Log Exporter (Splunk)"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#log-exporter-splunk","text":"The \u201cSplunk Format\u201d is legacy and should not be used for new deployments see Log Exporter (Syslog)","title":"Log Exporter (Splunk)"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#key-facts","text":"Format is not conformant to RFC3164 avoid use MSG Format based filter Legacy BSD Format default port 514 The Splunk host field will be derived as follows using the first match Use the hostname field Use the first CN component of origin_sic_name/originsicname If host is not set from CN use the hostname field If host is not set use the BSD syslog header host If the host is in the format <host>-v_<bladename> use bladename for host","title":"Key Facts"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4293/ Product Manual https://sc1.checkpoint.com/documents/App_for_Splunk/html_frameset.htm","title":"Links"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#sourcetypes","text":"sourcetype notes cp_log None","title":"Sourcetypes"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#sourcetype-and-index-configuration","text":"key sourcetype index notes checkpoint_splunk cp_log netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#source-and-index-configuration","text":"Checkpoint Software blades with CIM mapping have been sub-grouped into sources to allow routing to appropriate indexes. All other source meta data is left at default key source index notes checkpoint_splunk_dlp dlp netdlp none checkpoint_splunk_email email email none checkpoint_splunk_firewall firewall netfw none checkpoint_splunk_os program:${program} netops none checkpoint_splunk_sessions sessions netops none checkpoint_splunk_web web netproxy none checkpoint_splunk_audit audit netops none checkpoint_splunk_endpoint endpoint netops none checkpoint_splunk_network network netops checkpoint_splunk_ids ids netids checkpoint_splunk_ids_malware ids_malware netids","title":"Source and Index Configuration"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#options","text":"Variable default description SC4S_LISTEN_CHECKPOINT_SPLUNK_NOISE_CONTROL no Suppress any duplicate product+loguid pairs processed within 2 seconds of the last matching event SC4S_LISTEN_CHECKPOINT_SPLUNK_OLD_HOST_RULES empty string when set to yes reverts host name selection order to originsicname\u2013>origin_sic_name\u2013>hostname","title":"Options"},{"location":"sources/vendor/Cisco/cisco_ace/","text":"Application Control Engine (ACE) \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Sourcetypes \u00b6 sourcetype notes cisco:ace None Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_ace cisco:ace netops none","title":"Application Control Engine (ACE)"},{"location":"sources/vendor/Cisco/cisco_ace/#application-control-engine-ace","text":"","title":"Application Control Engine (ACE)"},{"location":"sources/vendor/Cisco/cisco_ace/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_ace/#links","text":"Ref Link Splunk Add-on None","title":"Links"},{"location":"sources/vendor/Cisco/cisco_ace/#sourcetypes","text":"sourcetype notes cisco:ace None","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_ace/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_ace cisco:ace netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_acs/","text":"Cisco Access Control System (ACS) \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1811/ Product Manual https://community.cisco.com/t5/security-documents/acs-5-x-configuring-the-external-syslog-server/ta-p/3143143 Sourcetypes \u00b6 sourcetype notes cisco:acs Aggregation used Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_acs cisco:acs netauth None Splunk Setup and Configuration \u00b6 Replace the following extract using Splunk local configuration. Impacts version 1.5.0 of the addond EXTRACT-AA-signature = CSCOacs_(?<signature>\\S+):? # Note the value of this config is empty to disable EXTRACT-AA-syslog_message = EXTRACT-acs_message_header2 = ^CSCOacs_\\S+\\s+(?<log_session_id>\\S+)\\s+(?<total_segments>\\d+)\\s+(?<segment_number>\\d+)\\s+(?<acs_message>.*)","title":"Cisco Access Control System (ACS)"},{"location":"sources/vendor/Cisco/cisco_acs/#cisco-access-control-system-acs","text":"","title":"Cisco Access Control System (ACS)"},{"location":"sources/vendor/Cisco/cisco_acs/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1811/ Product Manual https://community.cisco.com/t5/security-documents/acs-5-x-configuring-the-external-syslog-server/ta-p/3143143","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_acs/#sourcetypes","text":"sourcetype notes cisco:acs Aggregation used","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_acs/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_acs cisco:acs netauth None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_acs/#splunk-setup-and-configuration","text":"Replace the following extract using Splunk local configuration. Impacts version 1.5.0 of the addond EXTRACT-AA-signature = CSCOacs_(?<signature>\\S+):? # Note the value of this config is empty to disable EXTRACT-AA-syslog_message = EXTRACT-acs_message_header2 = ^CSCOacs_\\S+\\s+(?<log_session_id>\\S+)\\s+(?<total_segments>\\d+)\\s+(?<segment_number>\\d+)\\s+(?<acs_message>.*)","title":"Splunk Setup and Configuration"},{"location":"sources/vendor/Cisco/cisco_asa/","text":"ASA/FTD (Firepower) \u00b6 Key facts \u00b6 Note Splunk \u201cASA\u201d TA is also used for FTD appliances MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on for ASA (No long supports FWSM and PIX) https://splunkbase.splunk.com/app/1620/ Cisco eStreamer for Splunk https://splunkbase.splunk.com/app/1629/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/asa/asa82/configuration/guide/config/monitor_syslog.html Sourcetypes \u00b6 sourcetype notes cisco:asa cisco FTD Firepower will also use this source type except those noted below cisco:ftd cisco FTD Firepower will also use this source type except those noted below cisco:fwsm Splunk has cisco:pix cisco PIX will also use this source type except those noted below cisco:firepower:syslog FTD Unified events see https://www.cisco.com/c/en/us/td/docs/security/firepower/Syslogs/b_fptd_syslog_guide.pdf Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_asa cisco:asa netfw none cisco_fwsm cisco:fwsm netfw none cisco_pix cisco:pix netfw none cisco_firepower cisco:firepower:syslog netids none cisco_ftd cisco:ftd netfw none Source Setup and Configuration \u00b6 Follow vendor configuration steps per Product Manual above ensure: Log Level is 6 \u201cInformational\u201d Protocol is TCP/IP permit-hostdown is on device-id is hostname and included timestamp is included","title":"ASA/FTD (Firepower)"},{"location":"sources/vendor/Cisco/cisco_asa/#asaftd-firepower","text":"","title":"ASA/FTD (Firepower)"},{"location":"sources/vendor/Cisco/cisco_asa/#key-facts","text":"Note Splunk \u201cASA\u201d TA is also used for FTD appliances MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_asa/#links","text":"Ref Link Splunk Add-on for ASA (No long supports FWSM and PIX) https://splunkbase.splunk.com/app/1620/ Cisco eStreamer for Splunk https://splunkbase.splunk.com/app/1629/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/asa/asa82/configuration/guide/config/monitor_syslog.html","title":"Links"},{"location":"sources/vendor/Cisco/cisco_asa/#sourcetypes","text":"sourcetype notes cisco:asa cisco FTD Firepower will also use this source type except those noted below cisco:ftd cisco FTD Firepower will also use this source type except those noted below cisco:fwsm Splunk has cisco:pix cisco PIX will also use this source type except those noted below cisco:firepower:syslog FTD Unified events see https://www.cisco.com/c/en/us/td/docs/security/firepower/Syslogs/b_fptd_syslog_guide.pdf","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_asa/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_asa cisco:asa netfw none cisco_fwsm cisco:fwsm netfw none cisco_pix cisco:pix netfw none cisco_firepower cisco:firepower:syslog netids none cisco_ftd cisco:ftd netfw none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_asa/#source-setup-and-configuration","text":"Follow vendor configuration steps per Product Manual above ensure: Log Level is 6 \u201cInformational\u201d Protocol is TCP/IP permit-hostdown is on device-id is hostname and included timestamp is included","title":"Source Setup and Configuration"},{"location":"sources/vendor/Cisco/cisco_dna/","text":"Digital Network Area(DNA) \u00b6 Key facts \u00b6 MSG Format based filter rfc5424 default port 514 Links \u00b6 Ref Link Splunk Add-on na Product Manual multiple Sourcetypes \u00b6 sourcetype notes cisco:dna None Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_dna cisco:dna netops None SC4S Options \u00b6 Variable default description SC4S_SOURCE_CISCO_DNA_FIXHOST yes Current firmware incorrectly sends the value of the syslog server host name (destination) in the host field if this is ever corrected this value will need to be set back to no we suggest using yes","title":"Digital Network Area(DNA)"},{"location":"sources/vendor/Cisco/cisco_dna/#digital-network-areadna","text":"","title":"Digital Network Area(DNA)"},{"location":"sources/vendor/Cisco/cisco_dna/#key-facts","text":"MSG Format based filter rfc5424 default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_dna/#links","text":"Ref Link Splunk Add-on na Product Manual multiple","title":"Links"},{"location":"sources/vendor/Cisco/cisco_dna/#sourcetypes","text":"sourcetype notes cisco:dna None","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_dna/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_dna cisco:dna netops None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_dna/#sc4s-options","text":"Variable default description SC4S_SOURCE_CISCO_DNA_FIXHOST yes Current firmware incorrectly sends the value of the syslog server host name (destination) in the host field if this is ever corrected this value will need to be set back to no we suggest using yes","title":"SC4S Options"},{"location":"sources/vendor/Cisco/cisco_esa/","text":"Email Security Appliance (ESA) \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1761/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/esa/esa14-0/user_guide/b_ESA_Admin_Guide_14-0.pdf ESA Log Configuration \u00b6 If feasible for you, you can use following log configuration on the ESA. The log name configured on the ESA can then be parsed easily by sc4s. ESA Log Name ESA Log Type sc4s_gui_logs HTTP Logs sc4s_mail_logs IronPort Text Mail Logs sc4s_amp AMP Engine Logs sc4s_audit_logs Audit Logs sc4s_antispam Anti-Spam Logs sc4s_content_scanner Content Scanner Logs sc4s_error_logs IronPort Text Mail Logs (Loglevel: Critical) sc4s_system_logs System Logs Sourcetypes \u00b6 sourcetype notes cisco:esa:http The HTTP logs of Cisco IronPort ESA record information about the secure HTTP services enabled on the interface. cisco:esa:textmail Text mail logs of Cisco IronPort ESA record email information and status. cisco:esa:amp Advanced Malware Protection (AMP) of Cisco IronPort ESA records malware detection and blocking, continuous analysis, and retrospective alerting details. cisco:esa:authentication These logs record successful user logins and unsuccessful login attempts. cisco:esa:cef The Consolidated Event Logs summarizes each message event in a single log line. cisco:esa:error_logs Error logs of Cisco IronPort ESA records error that occurred for ESA configurations or internal issues. cisco:esa:content_scanner Content scanner logs of Cisco IronPort ESA scans messages that contain password-protected attachments for malicious activity and data privacy. cisco:esa:antispam Anti-spam logs record the status of the anti-spam scanning feature of your system, including the status on receiving updates of the latest anti-spam rules. Also, any logs related to the Context Adaptive Scanning Engine are logged here. cisco:esa:system_logs System logs record the boot information, virtual appliance license expiration alerts, DNS status information, and comments users typed using commit command. Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_esa cisco:esa:http email None cisco_esa cisco:esa:textmail email None cisco_esa cisco:esa:amp email None cisco_esa cisco:esa:authentication email None cisco_esa cisco:esa:cef email None cisco_esa cisco:esa:error_logs email None cisco_esa cisco:esa:content_scanner email None cisco_esa cisco:esa:antispam email None cisco_esa cisco:esa:system_logs email None Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-cisco_esa.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_esa [ sc4s - vps ] { filter { host ( \"^esa-\" ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' esa ' ) ); }; };","title":"Email Security Appliance (ESA)"},{"location":"sources/vendor/Cisco/cisco_esa/#email-security-appliance-esa","text":"","title":"Email Security Appliance (ESA)"},{"location":"sources/vendor/Cisco/cisco_esa/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_esa/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1761/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/esa/esa14-0/user_guide/b_ESA_Admin_Guide_14-0.pdf","title":"Links"},{"location":"sources/vendor/Cisco/cisco_esa/#esa-log-configuration","text":"If feasible for you, you can use following log configuration on the ESA. The log name configured on the ESA can then be parsed easily by sc4s. ESA Log Name ESA Log Type sc4s_gui_logs HTTP Logs sc4s_mail_logs IronPort Text Mail Logs sc4s_amp AMP Engine Logs sc4s_audit_logs Audit Logs sc4s_antispam Anti-Spam Logs sc4s_content_scanner Content Scanner Logs sc4s_error_logs IronPort Text Mail Logs (Loglevel: Critical) sc4s_system_logs System Logs","title":"ESA Log Configuration"},{"location":"sources/vendor/Cisco/cisco_esa/#sourcetypes","text":"sourcetype notes cisco:esa:http The HTTP logs of Cisco IronPort ESA record information about the secure HTTP services enabled on the interface. cisco:esa:textmail Text mail logs of Cisco IronPort ESA record email information and status. cisco:esa:amp Advanced Malware Protection (AMP) of Cisco IronPort ESA records malware detection and blocking, continuous analysis, and retrospective alerting details. cisco:esa:authentication These logs record successful user logins and unsuccessful login attempts. cisco:esa:cef The Consolidated Event Logs summarizes each message event in a single log line. cisco:esa:error_logs Error logs of Cisco IronPort ESA records error that occurred for ESA configurations or internal issues. cisco:esa:content_scanner Content scanner logs of Cisco IronPort ESA scans messages that contain password-protected attachments for malicious activity and data privacy. cisco:esa:antispam Anti-spam logs record the status of the anti-spam scanning feature of your system, including the status on receiving updates of the latest anti-spam rules. Also, any logs related to the Context Adaptive Scanning Engine are logged here. cisco:esa:system_logs System logs record the boot information, virtual appliance license expiration alerts, DNS status information, and comments users typed using commit command.","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_esa/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_esa cisco:esa:http email None cisco_esa cisco:esa:textmail email None cisco_esa cisco:esa:amp email None cisco_esa cisco:esa:authentication email None cisco_esa cisco:esa:cef email None cisco_esa cisco:esa:error_logs email None cisco_esa cisco:esa:content_scanner email None cisco_esa cisco:esa:antispam email None cisco_esa cisco:esa:system_logs email None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_esa/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-cisco_esa.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_esa [ sc4s - vps ] { filter { host ( \"^esa-\" ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' esa ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Cisco/cisco_imc/","text":"Cisco Integrated Management Controller (IMC) \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Ref Link Splunk Add-on na Product Manual multiple Sourcetypes \u00b6 sourcetype notes cisco:ucm None Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_cimc cisco:infraops infraops None","title":"Cisco Integrated Management Controller (IMC)"},{"location":"sources/vendor/Cisco/cisco_imc/#cisco-integrated-management-controller-imc","text":"","title":"Cisco Integrated Management Controller (IMC)"},{"location":"sources/vendor/Cisco/cisco_imc/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514 Ref Link Splunk Add-on na Product Manual multiple","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_imc/#sourcetypes","text":"sourcetype notes cisco:ucm None","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_imc/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_cimc cisco:infraops infraops None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_ios/","text":"Cisco Networking (IOS and Compatible) \u00b6 Cisco Network Products of multiple types share common logging characteristics the following types are known to be compatible: Cisco AireOS (AP & WLC) Cisco APIC/ACI Cisco IOS Cisco IOS-XR Cisco IOS-XE Cisco NX-OS Cisco FX-OS Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1467/ IOS Manual https://www.cisco.com/c/en/us/td/docs/switches/lan/catalyst2960/software/release/12-2_55_se/configuration/guide/scg_2960/swlog.html NX-OS Manual https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus9000/sw/6-x/system_management/configuration/guide/b_Cisco_Nexus_9000_Series_NX-OS_System_Management_Configuration_Guide/sm_5syslog.html Cisco ACI https://community.cisco.com/legacyfs/online/attachments/document/technote-aci-syslog_external-v1.pdf Cisco WLC & AP https://www.cisco.com/c/en/us/support/docs/wireless/4100-series-wireless-lan-controllers/107252-WLC-Syslog-Server.html#anc8 Sourcetypes \u00b6 sourcetype notes cisco:ios This source type is also used for NX-OS, ACI and WLC product lines Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_ios cisco:ios netops none Filter type \u00b6 Cisco IOS products can be identified by message parsing alone Cisco WLC, and ACI products must be identified by host or ip assignment update the filter f_cisco_ios as required Setup and Configuration \u00b6 IOS Follow vendor configuration steps per Product Manual above ensure: Ensure a reliable NTP server is set and synced Log Level is 6 \u201cInformational\u201d Protocol is TCP/IP permit-hostdown is on device-id is hostname and included timestamp is included NX-OS Follow vendor configuration steps per Product Manual above ensure: Ensure a reliable NTP server is set and synced Log Level is 6 \u201cInformational\u201d user may select alternate levels by module based on use cases Protocol is TCP/IP device-id is hostname and included timestamp is included and millisecond accuracy selected ACI Logging configuration of the ACI product often varies by use case. Ensure NTP sync is configured and active Ensure proper host names are configured WLC Ensure NTP sync is configured and active Ensure proper host names are configured For security use cases per AP logging is required If you want to send raw logs to splunk (without any drop) then only use this feature Please set following property in env_file: SC4S_ENABLE_CISCO_IOS_RAW_MSG=yes Restart SC4S and it will send entire message without any drop. NOTE: Please use this feature only when there is a special need to get entire raw message. This is not supported by splunk.","title":"Cisco Networking (IOS and Compatible)"},{"location":"sources/vendor/Cisco/cisco_ios/#cisco-networking-ios-and-compatible","text":"Cisco Network Products of multiple types share common logging characteristics the following types are known to be compatible: Cisco AireOS (AP & WLC) Cisco APIC/ACI Cisco IOS Cisco IOS-XR Cisco IOS-XE Cisco NX-OS Cisco FX-OS","title":"Cisco Networking (IOS and Compatible)"},{"location":"sources/vendor/Cisco/cisco_ios/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_ios/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1467/ IOS Manual https://www.cisco.com/c/en/us/td/docs/switches/lan/catalyst2960/software/release/12-2_55_se/configuration/guide/scg_2960/swlog.html NX-OS Manual https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus9000/sw/6-x/system_management/configuration/guide/b_Cisco_Nexus_9000_Series_NX-OS_System_Management_Configuration_Guide/sm_5syslog.html Cisco ACI https://community.cisco.com/legacyfs/online/attachments/document/technote-aci-syslog_external-v1.pdf Cisco WLC & AP https://www.cisco.com/c/en/us/support/docs/wireless/4100-series-wireless-lan-controllers/107252-WLC-Syslog-Server.html#anc8","title":"Links"},{"location":"sources/vendor/Cisco/cisco_ios/#sourcetypes","text":"sourcetype notes cisco:ios This source type is also used for NX-OS, ACI and WLC product lines","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_ios/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_ios cisco:ios netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_ios/#filter-type","text":"Cisco IOS products can be identified by message parsing alone Cisco WLC, and ACI products must be identified by host or ip assignment update the filter f_cisco_ios as required","title":"Filter type"},{"location":"sources/vendor/Cisco/cisco_ios/#setup-and-configuration","text":"IOS Follow vendor configuration steps per Product Manual above ensure: Ensure a reliable NTP server is set and synced Log Level is 6 \u201cInformational\u201d Protocol is TCP/IP permit-hostdown is on device-id is hostname and included timestamp is included NX-OS Follow vendor configuration steps per Product Manual above ensure: Ensure a reliable NTP server is set and synced Log Level is 6 \u201cInformational\u201d user may select alternate levels by module based on use cases Protocol is TCP/IP device-id is hostname and included timestamp is included and millisecond accuracy selected ACI Logging configuration of the ACI product often varies by use case. Ensure NTP sync is configured and active Ensure proper host names are configured WLC Ensure NTP sync is configured and active Ensure proper host names are configured For security use cases per AP logging is required If you want to send raw logs to splunk (without any drop) then only use this feature Please set following property in env_file: SC4S_ENABLE_CISCO_IOS_RAW_MSG=yes Restart SC4S and it will send entire message without any drop. NOTE: Please use this feature only when there is a special need to get entire raw message. This is not supported by splunk.","title":"Setup and Configuration"},{"location":"sources/vendor/Cisco/cisco_ise/","text":"Cisco Identity Services Engine (ISE) \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1915/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/ise/syslog/Cisco_ISE_Syslogs/m_IntrotoSyslogs.html Sourcetypes \u00b6 sourcetype notes cisco:ise:syslog Aggregation used Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_ise cisco:ise:syslog netauth None","title":"Cisco ise"},{"location":"sources/vendor/Cisco/cisco_ise/#cisco-identity-services-engine-ise","text":"","title":"Cisco Identity Services Engine (ISE)"},{"location":"sources/vendor/Cisco/cisco_ise/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_ise/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1915/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/ise/syslog/Cisco_ISE_Syslogs/m_IntrotoSyslogs.html","title":"Links"},{"location":"sources/vendor/Cisco/cisco_ise/#sourcetypes","text":"sourcetype notes cisco:ise:syslog Aggregation used","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_ise/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_ise cisco:ise:syslog netauth None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_meraki/","text":"Meraki (MR, MS, MX, MV) \u00b6 Key facts \u00b6 MSG Format based filter (Partial) Requires vendor product by source configuration None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3018/ Product Manual https://documentation.meraki.com/zGeneral_Administration/Monitoring_and_Reporting/Syslog_Server_Overview_and_Configuration Sourcetypes \u00b6 sourcetype notes meraki None Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_meraki meraki netfw The current TA does not sub sourcetype or utilize source preventing segmentation into more appropriate indexes Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-cisco_meraki.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_meraki [ sc4s - vps ] { filter { host ( \"^testcm-\" ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' meraki ' ) ); }; };","title":"Cisco meraki"},{"location":"sources/vendor/Cisco/cisco_meraki/#meraki-mr-ms-mx-mv","text":"","title":"Meraki (MR, MS, MX, MV)"},{"location":"sources/vendor/Cisco/cisco_meraki/#key-facts","text":"MSG Format based filter (Partial) Requires vendor product by source configuration None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_meraki/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3018/ Product Manual https://documentation.meraki.com/zGeneral_Administration/Monitoring_and_Reporting/Syslog_Server_Overview_and_Configuration","title":"Links"},{"location":"sources/vendor/Cisco/cisco_meraki/#sourcetypes","text":"sourcetype notes meraki None","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_meraki/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_meraki meraki netfw The current TA does not sub sourcetype or utilize source preventing segmentation into more appropriate indexes","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_meraki/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-cisco_meraki.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_meraki [ sc4s - vps ] { filter { host ( \"^testcm-\" ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' meraki ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Cisco/cisco_mm/","text":"Meeting Management \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on na Product Manual multiple Sourcetypes \u00b6 sourcetype notes cisco:mm:system:* final component take from the program field of the message header cisco:mm:audit Requires setup of vendor product by source see below Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_mm_system cisco:mm:system:* netops None cisco_mm_audit cisco:mm:audit netops None Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-cisco_mm.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_mm [ sc4s - vps ] { filter { host ( ' ^ test - cmm - ' ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' mm ' ) ); }; };","title":"Meeting Management"},{"location":"sources/vendor/Cisco/cisco_mm/#meeting-management","text":"","title":"Meeting Management"},{"location":"sources/vendor/Cisco/cisco_mm/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_mm/#links","text":"Ref Link Splunk Add-on na Product Manual multiple","title":"Links"},{"location":"sources/vendor/Cisco/cisco_mm/#sourcetypes","text":"sourcetype notes cisco:mm:system:* final component take from the program field of the message header cisco:mm:audit Requires setup of vendor product by source see below","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_mm/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_mm_system cisco:mm:system:* netops None cisco_mm_audit cisco:mm:audit netops None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_mm/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-cisco_mm.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_mm [ sc4s - vps ] { filter { host ( ' ^ test - cmm - ' ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' mm ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Cisco/cisco_ms/","text":"Meeting Server \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on na Product Manual multiple Sourcetypes \u00b6 sourcetype notes cisco:ms None Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_ms cisco:ms netops None Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-cisco_ms.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_ms [ sc4s - vps ] { filter { host ( ' ^ test - cms - ' ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' ms ' ) ); }; };","title":"Meeting Server"},{"location":"sources/vendor/Cisco/cisco_ms/#meeting-server","text":"","title":"Meeting Server"},{"location":"sources/vendor/Cisco/cisco_ms/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_ms/#links","text":"Ref Link Splunk Add-on na Product Manual multiple","title":"Links"},{"location":"sources/vendor/Cisco/cisco_ms/#sourcetypes","text":"sourcetype notes cisco:ms None","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_ms/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_ms cisco:ms netops None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_ms/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-cisco_ms.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_ms [ sc4s - vps ] { filter { host ( ' ^ test - cms - ' ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' ms ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Cisco/cisco_tvcs/","text":"TelePresence Video Communication Server (TVCS) \u00b6 Links \u00b6 Ref Link Product Manual https://www.cisco.com/c/en/us/products/unified-communications/telepresence-video-communication-server-vcs/index.html Sourcetypes \u00b6 sourcetype notes cisco:vcs none Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_tvcs cisco:tvcs main none","title":"TelePresence Video Communication Server (TVCS)"},{"location":"sources/vendor/Cisco/cisco_tvcs/#telepresence-video-communication-server-tvcs","text":"","title":"TelePresence Video Communication Server (TVCS)"},{"location":"sources/vendor/Cisco/cisco_tvcs/#links","text":"Ref Link Product Manual https://www.cisco.com/c/en/us/products/unified-communications/telepresence-video-communication-server-vcs/index.html","title":"Links"},{"location":"sources/vendor/Cisco/cisco_tvcs/#sourcetypes","text":"sourcetype notes cisco:vcs none","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_tvcs/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_tvcs cisco:tvcs main none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_ucm/","text":"Unified Communications Manager (UCM) \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on na Product Manual multiple Sourcetypes \u00b6 sourcetype notes cisco:ucm None Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_ucm cisco:ucm ucm None","title":"Unified Communications Manager (UCM)"},{"location":"sources/vendor/Cisco/cisco_ucm/#unified-communications-manager-ucm","text":"","title":"Unified Communications Manager (UCM)"},{"location":"sources/vendor/Cisco/cisco_ucm/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_ucm/#links","text":"Ref Link Splunk Add-on na Product Manual multiple","title":"Links"},{"location":"sources/vendor/Cisco/cisco_ucm/#sourcetypes","text":"sourcetype notes cisco:ucm None","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_ucm/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_ucm cisco:ucm ucm None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_ucshx/","text":"Unified Computing System (UCS) \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on na Product Manual multiple Sourcetypes \u00b6 sourcetype notes cisco:ucs None Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_ucs cisco:ucs infraops None","title":"Unified Computing System (UCS)"},{"location":"sources/vendor/Cisco/cisco_ucshx/#unified-computing-system-ucs","text":"","title":"Unified Computing System (UCS)"},{"location":"sources/vendor/Cisco/cisco_ucshx/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_ucshx/#links","text":"Ref Link Splunk Add-on na Product Manual multiple","title":"Links"},{"location":"sources/vendor/Cisco/cisco_ucshx/#sourcetypes","text":"sourcetype notes cisco:ucs None","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_ucshx/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_ucs cisco:ucs infraops None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_viptela/","text":"Viptela \u00b6 Key facts \u00b6 MSG Format based filter Links \u00b6 Ref Link Splunk Add-on na Product Manual multiple Sourcetypes \u00b6 sourcetype notes cisco:viptela None Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_viptela cisco:viptela netops None","title":"Viptela"},{"location":"sources/vendor/Cisco/cisco_viptela/#viptela","text":"","title":"Viptela"},{"location":"sources/vendor/Cisco/cisco_viptela/#key-facts","text":"MSG Format based filter","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_viptela/#links","text":"Ref Link Splunk Add-on na Product Manual multiple","title":"Links"},{"location":"sources/vendor/Cisco/cisco_viptela/#sourcetypes","text":"sourcetype notes cisco:viptela None","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_viptela/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_viptela cisco:viptela netops None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_wsa/","text":"Web Security Appliance (WSA) \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1747/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/wsa/wsa11-7/user_guide/b_WSA_UserGuide_11_7.html Sourcetypes \u00b6 | cisco:wsa:l4tm | The L4TM logs of Cisco IronPort WSA record sites added to the L4TM block and allow lists. | | cisco:wsa:squid | The access logs of Cisco IronPort WSA version prior to 11.7 record Web Proxy client history in squid. | | cisco:wsa:squid:new | The access logs of Cisco IronPort WSA version since 11.7 record Web Proxy client history in squid. | | cisco:wsa:w3c:recommended | The access logs of Cisco IronPort WSA version since 12.5 record Web Proxy client history in W3C. | Sourcetype and Index Configuration \u00b6 key sourcetype index notes cisco_wsa cisco:wsa:l4tm netproxy None cisco_wsa cisco:wsa:squid netproxy None cisco_wsa cisco:wsa:squid:new netproxy None cisco_wsa cisco:wsa:w3c:recommended netproxy None Filter type \u00b6 IP, Netmask or Host Source Setup and Configuration \u00b6 Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. WSA Follow vendor configuration steps per Product Manual. Ensure host and timestamp are included. Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-cisco_wsa.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_wsa [ sc4s - vps ] { filter { host ( \"^wsa-\" ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' wsa ' ) ); }; };","title":"Web Security Appliance (WSA)"},{"location":"sources/vendor/Cisco/cisco_wsa/#web-security-appliance-wsa","text":"","title":"Web Security Appliance (WSA)"},{"location":"sources/vendor/Cisco/cisco_wsa/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cisco/cisco_wsa/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1747/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/wsa/wsa11-7/user_guide/b_WSA_UserGuide_11_7.html","title":"Links"},{"location":"sources/vendor/Cisco/cisco_wsa/#sourcetypes","text":"| cisco:wsa:l4tm | The L4TM logs of Cisco IronPort WSA record sites added to the L4TM block and allow lists. | | cisco:wsa:squid | The access logs of Cisco IronPort WSA version prior to 11.7 record Web Proxy client history in squid. | | cisco:wsa:squid:new | The access logs of Cisco IronPort WSA version since 11.7 record Web Proxy client history in squid. | | cisco:wsa:w3c:recommended | The access logs of Cisco IronPort WSA version since 12.5 record Web Proxy client history in W3C. |","title":"Sourcetypes"},{"location":"sources/vendor/Cisco/cisco_wsa/#sourcetype-and-index-configuration","text":"key sourcetype index notes cisco_wsa cisco:wsa:l4tm netproxy None cisco_wsa cisco:wsa:squid netproxy None cisco_wsa cisco:wsa:squid:new netproxy None cisco_wsa cisco:wsa:w3c:recommended netproxy None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Cisco/cisco_wsa/#filter-type","text":"IP, Netmask or Host","title":"Filter type"},{"location":"sources/vendor/Cisco/cisco_wsa/#source-setup-and-configuration","text":"Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. WSA Follow vendor configuration steps per Product Manual. Ensure host and timestamp are included.","title":"Source Setup and Configuration"},{"location":"sources/vendor/Cisco/cisco_wsa/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-cisco_wsa.conf #File name provided is a suggestion it must be globally unique application app - vps - test - cisco_wsa [ sc4s - vps ] { filter { host ( \"^wsa-\" ) }; parser { p_set_netsource_fields ( vendor ( ' cisco ' ) product ( ' wsa ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Citrix/netscaler/","text":"Netscaler ADC/SDX \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2770/ Product Manual https://docs.citrix.com/en-us/citrix-adc/12-1/system/audit-logging/configuring-audit-logging.html Sourcetypes \u00b6 sourcetype notes citrix:netscaler:syslog None citrix:netscaler:appfw None citrix:netscaler:appfw:cef None Sourcetype and Index Configuration \u00b6 key sourcetype index notes citrix_netscaler citrix:netscaler:syslog netfw none citrix_netscaler citrix:netscaler:appfw netfw none citrix_netscaler citrix:netscaler:appfw:cef netfw none Source Setup and Configuration \u00b6 Follow vendor configuration steps per Product Manual above. Ensure the data format selected is \u201cDDMMYYYY\u201d","title":"Netscaler ADC/SDX"},{"location":"sources/vendor/Citrix/netscaler/#netscaler-adcsdx","text":"","title":"Netscaler ADC/SDX"},{"location":"sources/vendor/Citrix/netscaler/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Citrix/netscaler/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2770/ Product Manual https://docs.citrix.com/en-us/citrix-adc/12-1/system/audit-logging/configuring-audit-logging.html","title":"Links"},{"location":"sources/vendor/Citrix/netscaler/#sourcetypes","text":"sourcetype notes citrix:netscaler:syslog None citrix:netscaler:appfw None citrix:netscaler:appfw:cef None","title":"Sourcetypes"},{"location":"sources/vendor/Citrix/netscaler/#sourcetype-and-index-configuration","text":"key sourcetype index notes citrix_netscaler citrix:netscaler:syslog netfw none citrix_netscaler citrix:netscaler:appfw netfw none citrix_netscaler citrix:netscaler:appfw:cef netfw none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Citrix/netscaler/#source-setup-and-configuration","text":"Follow vendor configuration steps per Product Manual above. Ensure the data format selected is \u201cDDMMYYYY\u201d","title":"Source Setup and Configuration"},{"location":"sources/vendor/Clearswift/","text":"WAF (Cloud) \u00b6 Key facts \u00b6 MSG Format based filter RFC 5424 Framed Links \u00b6 Ref Link Splunk Add-on None Product Manual https://clearswifthelp.clearswift.com/SEG/472/en/Content/Sections/SystemsCenter/SYCLogList.htm Sourcetypes \u00b6 sourcetype notes clearswift:${PROGRAM} none Sourcetype and Index Configuration \u00b6 key sourcetype index notes clearswift clearswift:${PROGRAM} email None Parser Configuration \u00b6 ```c /opt/sc4s/local/config/app-parsers/app-vps-clearswift.conf \u00b6 File name provided is a suggestion it must be globally unique \u00b6 application app-vps-clearswift[sc4s-vps] { filter { host(\u201ctest-clearswift-\u201d type(string) flags(prefix)) }; parser { p_set_netsource_fields( vendor(\u2018clearswift\u2019) product(\u2018clearswift\u2019) ); }; };","title":"WAF (Cloud)"},{"location":"sources/vendor/Clearswift/#waf-cloud","text":"","title":"WAF (Cloud)"},{"location":"sources/vendor/Clearswift/#key-facts","text":"MSG Format based filter RFC 5424 Framed","title":"Key facts"},{"location":"sources/vendor/Clearswift/#links","text":"Ref Link Splunk Add-on None Product Manual https://clearswifthelp.clearswift.com/SEG/472/en/Content/Sections/SystemsCenter/SYCLogList.htm","title":"Links"},{"location":"sources/vendor/Clearswift/#sourcetypes","text":"sourcetype notes clearswift:${PROGRAM} none","title":"Sourcetypes"},{"location":"sources/vendor/Clearswift/#sourcetype-and-index-configuration","text":"key sourcetype index notes clearswift clearswift:${PROGRAM} email None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Clearswift/#parser-configuration","text":"```c","title":"Parser Configuration"},{"location":"sources/vendor/Clearswift/#optsc4slocalconfigapp-parsersapp-vps-clearswiftconf","text":"","title":"/opt/sc4s/local/config/app-parsers/app-vps-clearswift.conf"},{"location":"sources/vendor/Clearswift/#file-name-provided-is-a-suggestion-it-must-be-globally-unique","text":"application app-vps-clearswift[sc4s-vps] { filter { host(\u201ctest-clearswift-\u201d type(string) flags(prefix)) }; parser { p_set_netsource_fields( vendor(\u2018clearswift\u2019) product(\u2018clearswift\u2019) ); }; };","title":"File name provided is a suggestion it must be globally unique"},{"location":"sources/vendor/Cohesity/cluster/","text":"Cluster \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes cohesity:cluster:audit None cohesity:cluster:dataprotection None Sourcetype and Index Configuration \u00b6 key sourcetype index notes cohesity_cluster_audit cohesity:cluster:audit infraops none cohesity_cluster_dataprotection cohesity:cluster:dataprotection infraops none","title":"Cluster"},{"location":"sources/vendor/Cohesity/cluster/#cluster","text":"","title":"Cluster"},{"location":"sources/vendor/Cohesity/cluster/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cohesity/cluster/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Cohesity/cluster/#sourcetypes","text":"sourcetype notes cohesity:cluster:audit None cohesity:cluster:dataprotection None","title":"Sourcetypes"},{"location":"sources/vendor/Cohesity/cluster/#sourcetype-and-index-configuration","text":"key sourcetype index notes cohesity_cluster_audit cohesity:cluster:audit infraops none cohesity_cluster_dataprotection cohesity:cluster:dataprotection infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/CyberArk/epv/","text":"Vendor - CyberArk \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Product - EPV \u00b6 Ref Link Splunk Add-on CyberArk https://splunkbase.splunk.com/app/2891/ Add-on Manual https://docs.splunk.com/Documentation/AddOns/latest/CyberArk/About Sourcetypes \u00b6 sourcetype notes cyberark:epv:cef None Index Configuration \u00b6 key sourcetype index notes CyberArk_Vault cyberark:epv:cef netauth none","title":"Vendor - CyberArk"},{"location":"sources/vendor/CyberArk/epv/#vendor-cyberark","text":"","title":"Vendor - CyberArk"},{"location":"sources/vendor/CyberArk/epv/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/CyberArk/epv/#product-epv","text":"Ref Link Splunk Add-on CyberArk https://splunkbase.splunk.com/app/2891/ Add-on Manual https://docs.splunk.com/Documentation/AddOns/latest/CyberArk/About","title":"Product - EPV"},{"location":"sources/vendor/CyberArk/epv/#sourcetypes","text":"sourcetype notes cyberark:epv:cef None","title":"Sourcetypes"},{"location":"sources/vendor/CyberArk/epv/#index-configuration","text":"key sourcetype index notes CyberArk_Vault cyberark:epv:cef netauth none","title":"Index Configuration"},{"location":"sources/vendor/CyberArk/pta/","text":"PTA \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on CyberArk https://splunkbase.splunk.com/app/2891/ Add-on Manual https://docs.splunk.com/Documentation/AddOns/latest/CyberArk/About Sourcetypes \u00b6 sourcetype notes cyberark:pta:cef None Index Configuration \u00b6 key sourcetype index notes Cyber-Ark_Vault cyberark:pta:cef main none","title":"PTA"},{"location":"sources/vendor/CyberArk/pta/#pta","text":"","title":"PTA"},{"location":"sources/vendor/CyberArk/pta/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/CyberArk/pta/#links","text":"Ref Link Splunk Add-on CyberArk https://splunkbase.splunk.com/app/2891/ Add-on Manual https://docs.splunk.com/Documentation/AddOns/latest/CyberArk/About","title":"Links"},{"location":"sources/vendor/CyberArk/pta/#sourcetypes","text":"sourcetype notes cyberark:pta:cef None","title":"Sourcetypes"},{"location":"sources/vendor/CyberArk/pta/#index-configuration","text":"key sourcetype index notes Cyber-Ark_Vault cyberark:pta:cef main none","title":"Index Configuration"},{"location":"sources/vendor/Cylance/protect/","text":"Protect \u00b6 Key facts \u00b6 MSG Format based filter None conformant legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on CyberArk https://splunkbase.splunk.com/app/3709/ Sourcetypes \u00b6 sourcetype notes syslog_protect Catchall syslog_threat_classification None syslog_audit_log None syslog_exploit None syslog_app_control None syslog_threat None syslog_device None syslog_device_control None syslog_script_control None syslog_optics None Index Configuration \u00b6 key sourcetype index notes cylance_protect syslog_protect epintel none cylance_protect_auditlog syslog_audit_log epintel none cylance_protect_threatclassification syslog_threat_classification epintel none cylance_protect_exploitattempt syslog_exploit epintel none cylance_protect_appcontrol syslog_app_control epintel none cylance_protect_threat syslog_threat epintel none cylance_protect_device syslog_device epintel none cylance_protect_devicecontrol syslog_device_control epintel none cylance_protect_scriptcontrol syslog_protect epintel none cylance_protect_scriptcontrol syslog_script_control epintel none cylance_protect_optics syslog_optics epintel none","title":"Protect"},{"location":"sources/vendor/Cylance/protect/#protect","text":"","title":"Protect"},{"location":"sources/vendor/Cylance/protect/#key-facts","text":"MSG Format based filter None conformant legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Cylance/protect/#links","text":"Ref Link Splunk Add-on CyberArk https://splunkbase.splunk.com/app/3709/","title":"Links"},{"location":"sources/vendor/Cylance/protect/#sourcetypes","text":"sourcetype notes syslog_protect Catchall syslog_threat_classification None syslog_audit_log None syslog_exploit None syslog_app_control None syslog_threat None syslog_device None syslog_device_control None syslog_script_control None syslog_optics None","title":"Sourcetypes"},{"location":"sources/vendor/Cylance/protect/#index-configuration","text":"key sourcetype index notes cylance_protect syslog_protect epintel none cylance_protect_auditlog syslog_audit_log epintel none cylance_protect_threatclassification syslog_threat_classification epintel none cylance_protect_exploitattempt syslog_exploit epintel none cylance_protect_appcontrol syslog_app_control epintel none cylance_protect_threat syslog_threat epintel none cylance_protect_device syslog_device epintel none cylance_protect_devicecontrol syslog_device_control epintel none cylance_protect_scriptcontrol syslog_protect epintel none cylance_protect_scriptcontrol syslog_script_control epintel none cylance_protect_optics syslog_optics epintel none","title":"Index Configuration"},{"location":"sources/vendor/DARKTRACE/darktrace/","text":"Darktrace \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes darktrace none darktrace:audit none Sourcetype and Index Configuration \u00b6 key sourcetype index notes darktrace_syslog darktrace netids None darktrace_audit darktrace_audit netids None","title":"Darktrace"},{"location":"sources/vendor/DARKTRACE/darktrace/#darktrace","text":"","title":"Darktrace"},{"location":"sources/vendor/DARKTRACE/darktrace/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/DARKTRACE/darktrace/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/DARKTRACE/darktrace/#sourcetypes","text":"sourcetype notes darktrace none darktrace:audit none","title":"Sourcetypes"},{"location":"sources/vendor/DARKTRACE/darktrace/#sourcetype-and-index-configuration","text":"key sourcetype index notes darktrace_syslog darktrace netids None darktrace_audit darktrace_audit netids None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Dell/cmc/","text":"CMC (VRTX) \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on na Add-on Manual https://www.dell.com/support/manuals/en-us/dell-chassis-management-controller-v3.10-dell-poweredge-vrtx/cmcvrtx31ug/overview?guid=guid-84595265-d37c-4765-8890-90f629737b17 Sourcetypes \u00b6 sourcetype notes dell:poweredge:cmc:syslog None Index Configuration \u00b6 key sourcetype index notes dell_poweredge_cmc dell:poweredge:cmc:syslog infraops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-dell_cmc.conf #File name provided is a suggestion it must be globally unique application app - vps - test - dell_cmc [ sc4s - vps ] { filter { host ( \"test-dell-cmc-\" type ( string ) flags ( prefix )) }; parser { p_set_netsource_fields ( vendor ( ' dell ' ) product ( ' poweredge_cmc ' ) ); }; };","title":"CMC (VRTX)"},{"location":"sources/vendor/Dell/cmc/#cmc-vrtx","text":"","title":"CMC (VRTX)"},{"location":"sources/vendor/Dell/cmc/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Dell/cmc/#links","text":"Ref Link Splunk Add-on na Add-on Manual https://www.dell.com/support/manuals/en-us/dell-chassis-management-controller-v3.10-dell-poweredge-vrtx/cmcvrtx31ug/overview?guid=guid-84595265-d37c-4765-8890-90f629737b17","title":"Links"},{"location":"sources/vendor/Dell/cmc/#sourcetypes","text":"sourcetype notes dell:poweredge:cmc:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Dell/cmc/#index-configuration","text":"key sourcetype index notes dell_poweredge_cmc dell:poweredge:cmc:syslog infraops none","title":"Index Configuration"},{"location":"sources/vendor/Dell/cmc/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-dell_cmc.conf #File name provided is a suggestion it must be globally unique application app - vps - test - dell_cmc [ sc4s - vps ] { filter { host ( \"test-dell-cmc-\" type ( string ) flags ( prefix )) }; parser { p_set_netsource_fields ( vendor ( ' dell ' ) product ( ' poweredge_cmc ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Dell/emc_powerswitchn/","text":"EMC Powerswitch N Series \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes dell:emc:powerswitch:n None nix:syslog Non conforming messages Sourcetype and Index Configuration \u00b6 key sourcetype index notes dellemc_powerswitch_n all netops none","title":"EMC Powerswitch N Series"},{"location":"sources/vendor/Dell/emc_powerswitchn/#emc-powerswitch-n-series","text":"","title":"EMC Powerswitch N Series"},{"location":"sources/vendor/Dell/emc_powerswitchn/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Dell/emc_powerswitchn/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Dell/emc_powerswitchn/#sourcetypes","text":"sourcetype notes dell:emc:powerswitch:n None nix:syslog Non conforming messages","title":"Sourcetypes"},{"location":"sources/vendor/Dell/emc_powerswitchn/#sourcetype-and-index-configuration","text":"key sourcetype index notes dellemc_powerswitch_n all netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Dell/idrac/","text":"iDrac \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on na Add-on Manual https://www.dell.com/support/manuals/en-au/dell-opnmang-sw-v8.1/eemi_13g_v1.2-v1/introduction?guid=guid-8f22a1a9-ac01-43d1-a9d2-390ca6708d5e&lang=en-us Sourcetypes \u00b6 sourcetype notes dell:poweredge:idrac:syslog None Index Configuration \u00b6 key sourcetype index notes dell_poweredge_idrac dell:poweredge:idrac:syslog infraops none","title":"iDrac"},{"location":"sources/vendor/Dell/idrac/#idrac","text":"","title":"iDrac"},{"location":"sources/vendor/Dell/idrac/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Dell/idrac/#links","text":"Ref Link Splunk Add-on na Add-on Manual https://www.dell.com/support/manuals/en-au/dell-opnmang-sw-v8.1/eemi_13g_v1.2-v1/introduction?guid=guid-8f22a1a9-ac01-43d1-a9d2-390ca6708d5e&lang=en-us","title":"Links"},{"location":"sources/vendor/Dell/idrac/#sourcetypes","text":"sourcetype notes dell:poweredge:idrac:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Dell/idrac/#index-configuration","text":"key sourcetype index notes dell_poweredge_idrac dell:poweredge:idrac:syslog infraops none","title":"Index Configuration"},{"location":"sources/vendor/Dell/rsa_secureid/","text":"RSA SecureID \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2958/ Product Manual https://docs.splunk.com/Documentation/AddOns/released/RSASecurID/Aboutthisaddon Sourcetypes \u00b6 sourcetype notes rsa:securid:syslog Catchall; used if a more specific source type can not be identified rsa:securid:admin:syslog None rsa:securid:runtime:syslog None nix:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes dell-rsa_secureid all netauth none dell-rsa_secureid_trace rsa:securid:trace netauth none dell-rsa_secureid nix:syslog osnix uses os_nix key of not configured bye host/ip/port Parser Configuration \u00b6 #/opt/sc4s/local/config/app_parsers/app-vps-dell_rsa_secureid.conf #File name provided is a suggestion it must be globally unique application app - vps - test - dell_rsa_secureid [ sc4s - vps ] { filter { host ( \"test_rsasecureid*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' dell ' ) product ( ' rsa_secureid ' ) ); }; };","title":"RSA SecureID"},{"location":"sources/vendor/Dell/rsa_secureid/#rsa-secureid","text":"","title":"RSA SecureID"},{"location":"sources/vendor/Dell/rsa_secureid/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Dell/rsa_secureid/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2958/ Product Manual https://docs.splunk.com/Documentation/AddOns/released/RSASecurID/Aboutthisaddon","title":"Links"},{"location":"sources/vendor/Dell/rsa_secureid/#sourcetypes","text":"sourcetype notes rsa:securid:syslog Catchall; used if a more specific source type can not be identified rsa:securid:admin:syslog None rsa:securid:runtime:syslog None nix:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Dell/rsa_secureid/#sourcetype-and-index-configuration","text":"key sourcetype index notes dell-rsa_secureid all netauth none dell-rsa_secureid_trace rsa:securid:trace netauth none dell-rsa_secureid nix:syslog osnix uses os_nix key of not configured bye host/ip/port","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Dell/rsa_secureid/#parser-configuration","text":"#/opt/sc4s/local/config/app_parsers/app-vps-dell_rsa_secureid.conf #File name provided is a suggestion it must be globally unique application app - vps - test - dell_rsa_secureid [ sc4s - vps ] { filter { host ( \"test_rsasecureid*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' dell ' ) product ( ' rsa_secureid ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Dell/sonicwall/","text":"Sonicwall \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/6203/ Sourcetypes \u00b6 sourcetype notes dell:sonicwall None Index Configuration \u00b6 key sourcetype index notes dell_sonicwall-firewall dell:sonicwall netfw none Options \u00b6 Variable default description SC4S_DEST_DELL_SONICWALL-FIREWALL_SPLUNK_HEC_FMT JSON Restructure data from vendor format to json for splunk destinations set to \u201cNONE\u201d for native format SC4S_DEST_DELL_SONICWALL-FIREWALL_SYSLOG_FMT SDATA Restructure data from vendor format to SDATA for SYSLOG destinations set to \u201cNONE\u201d for native format Note: \u00b6 The sourcetype has been changed in version 2.35.0 making it compliant with corresponding TA.","title":"Sonicwall"},{"location":"sources/vendor/Dell/sonicwall/#sonicwall","text":"","title":"Sonicwall"},{"location":"sources/vendor/Dell/sonicwall/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Dell/sonicwall/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/6203/","title":"Links"},{"location":"sources/vendor/Dell/sonicwall/#sourcetypes","text":"sourcetype notes dell:sonicwall None","title":"Sourcetypes"},{"location":"sources/vendor/Dell/sonicwall/#index-configuration","text":"key sourcetype index notes dell_sonicwall-firewall dell:sonicwall netfw none","title":"Index Configuration"},{"location":"sources/vendor/Dell/sonicwall/#options","text":"Variable default description SC4S_DEST_DELL_SONICWALL-FIREWALL_SPLUNK_HEC_FMT JSON Restructure data from vendor format to json for splunk destinations set to \u201cNONE\u201d for native format SC4S_DEST_DELL_SONICWALL-FIREWALL_SYSLOG_FMT SDATA Restructure data from vendor format to SDATA for SYSLOG destinations set to \u201cNONE\u201d for native format","title":"Options"},{"location":"sources/vendor/Dell/sonicwall/#note","text":"The sourcetype has been changed in version 2.35.0 making it compliant with corresponding TA.","title":"Note:"},{"location":"sources/vendor/F5/bigip/","text":"BigIP \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Needs host to be defined in log header similarly like in this issue. Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2680/ Product Manual unknown Sourcetypes \u00b6 sourcetype notes f5:bigip:syslog None f5:bigip:irule None f5:bigip:ltm:http:irule None f5:bigip:gtm:dns:request:irule None f5:bigip:gtm:dns:response:irule None f5:bigip:ltm:failed:irule None f5:bigip:asm:syslog None f5:bigip:apm:syslog None nix:syslog None f5:bigip:ltm:access_json User defined configuration via irule producing a RFC5424 syslog event with json content within the message field <111>1 2020-05-28T22:48:15Z foo.example.com F5 - access_json - {\"event_type\":\"HTTP_REQUEST\", \"src_ip\":\"10.66.98.41\"} This source type requires a customer specific Splunk Add-on for utility value Index Configuration \u00b6 key index notes f5_bigip netops none f5_bigip_irule netops none f5_bigip_asm netwaf none f5_bigip_apm netops none f5_bigip_nix netops if f_f5_bigip is not set the index osnix will be used f5_bigip_access_json netops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-f5_bigip.conf #File name provided is a suggestion it must be globally unique application app - vps - test - f5_bigip [ sc4s - vps ] { filter { \"${HOST}\" eq \"f5_bigip\" }; parser { p_set_netsource_fields ( vendor ( ' f5 ' ) product ( ' bigip ' ) ); }; };","title":"BigIP"},{"location":"sources/vendor/F5/bigip/#bigip","text":"","title":"BigIP"},{"location":"sources/vendor/F5/bigip/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514 Needs host to be defined in log header similarly like in this issue.","title":"Key facts"},{"location":"sources/vendor/F5/bigip/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2680/ Product Manual unknown","title":"Links"},{"location":"sources/vendor/F5/bigip/#sourcetypes","text":"sourcetype notes f5:bigip:syslog None f5:bigip:irule None f5:bigip:ltm:http:irule None f5:bigip:gtm:dns:request:irule None f5:bigip:gtm:dns:response:irule None f5:bigip:ltm:failed:irule None f5:bigip:asm:syslog None f5:bigip:apm:syslog None nix:syslog None f5:bigip:ltm:access_json User defined configuration via irule producing a RFC5424 syslog event with json content within the message field <111>1 2020-05-28T22:48:15Z foo.example.com F5 - access_json - {\"event_type\":\"HTTP_REQUEST\", \"src_ip\":\"10.66.98.41\"} This source type requires a customer specific Splunk Add-on for utility value","title":"Sourcetypes"},{"location":"sources/vendor/F5/bigip/#index-configuration","text":"key index notes f5_bigip netops none f5_bigip_irule netops none f5_bigip_asm netwaf none f5_bigip_apm netops none f5_bigip_nix netops if f_f5_bigip is not set the index osnix will be used f5_bigip_access_json netops none","title":"Index Configuration"},{"location":"sources/vendor/F5/bigip/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-f5_bigip.conf #File name provided is a suggestion it must be globally unique application app - vps - test - f5_bigip [ sc4s - vps ] { filter { \"${HOST}\" eq \"f5_bigip\" }; parser { p_set_netsource_fields ( vendor ( ' f5 ' ) product ( ' bigip ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/FireEye/cms/","text":"CMS \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/ Sourcetypes \u00b6 sourcetype notes fe_cef_syslog Index Configuration \u00b6 key sourcetype index notes FireEye_CMS fe_cef_syslog fireeye","title":"CMS"},{"location":"sources/vendor/FireEye/cms/#cms","text":"","title":"CMS"},{"location":"sources/vendor/FireEye/cms/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/FireEye/cms/#links","text":"Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/","title":"Links"},{"location":"sources/vendor/FireEye/cms/#sourcetypes","text":"sourcetype notes fe_cef_syslog","title":"Sourcetypes"},{"location":"sources/vendor/FireEye/cms/#index-configuration","text":"key sourcetype index notes FireEye_CMS fe_cef_syslog fireeye","title":"Index Configuration"},{"location":"sources/vendor/FireEye/emps/","text":"eMPS \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/ Sourcetypes \u00b6 sourcetype notes fe_cef_syslog Index Configuration \u00b6 key sourcetype index notes FireEye_eMPS fe_cef_syslog fireeye","title":"eMPS"},{"location":"sources/vendor/FireEye/emps/#emps","text":"","title":"eMPS"},{"location":"sources/vendor/FireEye/emps/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/FireEye/emps/#links","text":"Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/","title":"Links"},{"location":"sources/vendor/FireEye/emps/#sourcetypes","text":"sourcetype notes fe_cef_syslog","title":"Sourcetypes"},{"location":"sources/vendor/FireEye/emps/#index-configuration","text":"key sourcetype index notes FireEye_eMPS fe_cef_syslog fireeye","title":"Index Configuration"},{"location":"sources/vendor/FireEye/etp/","text":"etp \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/ Sourcetypes \u00b6 sourcetype notes fe_etp source does not provide host name constant \u201cetp.fireeye.com\u201d is use regardless of region Index Configuration \u00b6 key sourcetype index notes FireEye_ETP fe_etp fireeye","title":"etp"},{"location":"sources/vendor/FireEye/etp/#etp","text":"","title":"etp"},{"location":"sources/vendor/FireEye/etp/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/FireEye/etp/#links","text":"Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/","title":"Links"},{"location":"sources/vendor/FireEye/etp/#sourcetypes","text":"sourcetype notes fe_etp source does not provide host name constant \u201cetp.fireeye.com\u201d is use regardless of region","title":"Sourcetypes"},{"location":"sources/vendor/FireEye/etp/#index-configuration","text":"key sourcetype index notes FireEye_ETP fe_etp fireeye","title":"Index Configuration"},{"location":"sources/vendor/FireEye/hx/","text":"hx \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/ Sourcetypes \u00b6 sourcetype notes hx_cef_syslog Index Configuration \u00b6 key sourcetype index notes fireeye_hx hx_cef_syslog fireeye","title":"hx"},{"location":"sources/vendor/FireEye/hx/#hx","text":"","title":"hx"},{"location":"sources/vendor/FireEye/hx/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/FireEye/hx/#links","text":"Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/","title":"Links"},{"location":"sources/vendor/FireEye/hx/#sourcetypes","text":"sourcetype notes hx_cef_syslog","title":"Sourcetypes"},{"location":"sources/vendor/FireEye/hx/#index-configuration","text":"key sourcetype index notes fireeye_hx hx_cef_syslog fireeye","title":"Index Configuration"},{"location":"sources/vendor/Forcepoint/","text":"Email Security \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on none Product Manual none Sourcetypes \u00b6 sourcetype notes forcepoint:email:kv None Sourcetype and Index Configuration \u00b6 key sourcetype index notes forcepoint_email forcepoint:email:kv email none","title":"Email Security"},{"location":"sources/vendor/Forcepoint/#email-security","text":"","title":"Email Security"},{"location":"sources/vendor/Forcepoint/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Forcepoint/#links","text":"Ref Link Splunk Add-on none Product Manual none","title":"Links"},{"location":"sources/vendor/Forcepoint/#sourcetypes","text":"sourcetype notes forcepoint:email:kv None","title":"Sourcetypes"},{"location":"sources/vendor/Forcepoint/#sourcetype-and-index-configuration","text":"key sourcetype index notes forcepoint_email forcepoint:email:kv email none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Forcepoint/webprotect/","text":"Webprotect (Websense) \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2966/ Product Manual http://www.websense.com/content/support/library/web/v85/siem/siem.pdf Sourcetypes \u00b6 sourcetype notes websense:cg:kv None Sourcetype and Index Configuration \u00b6 key sourcetype index notes forcepoint_webprotect websense:cg:kv netproxy none forcepoint_ websense:cg:kv netproxy if the log is in format of vendor=Forcepoint product= , the key will will be forcepoint_random","title":"Webprotect (Websense)"},{"location":"sources/vendor/Forcepoint/webprotect/#webprotect-websense","text":"","title":"Webprotect (Websense)"},{"location":"sources/vendor/Forcepoint/webprotect/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Forcepoint/webprotect/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2966/ Product Manual http://www.websense.com/content/support/library/web/v85/siem/siem.pdf","title":"Links"},{"location":"sources/vendor/Forcepoint/webprotect/#sourcetypes","text":"sourcetype notes websense:cg:kv None","title":"Sourcetypes"},{"location":"sources/vendor/Forcepoint/webprotect/#sourcetype-and-index-configuration","text":"key sourcetype index notes forcepoint_webprotect websense:cg:kv netproxy none forcepoint_ websense:cg:kv netproxy if the log is in format of vendor=Forcepoint product= , the key will will be forcepoint_random","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Fortinet/fortimail/","text":"FortiWMail \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3249 Sourcetypes \u00b6 sourcetype notes fml: type value is determined from the message Sourcetype and Index Configuration \u00b6 key sourcetype index notes fortinet_fortimail_ fml: email type value is determined from the message","title":"FortiWMail"},{"location":"sources/vendor/Fortinet/fortimail/#fortiwmail","text":"","title":"FortiWMail"},{"location":"sources/vendor/Fortinet/fortimail/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Fortinet/fortimail/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3249","title":"Links"},{"location":"sources/vendor/Fortinet/fortimail/#sourcetypes","text":"sourcetype notes fml: type value is determined from the message","title":"Sourcetypes"},{"location":"sources/vendor/Fortinet/fortimail/#sourcetype-and-index-configuration","text":"key sourcetype index notes fortinet_fortimail_ fml: email type value is determined from the message","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Fortinet/fortios/","text":"Fortios \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2846/ Product Manual https://docs.fortinet.com/product/fortigate/6.2 Sourcetypes \u00b6 sourcetype notes fgt_log Catch-all sourcetype; not used by the TA fgt_traffic None fgt_utm None fgt_event None Sourcetype and Index Configuration \u00b6 key sourcetype index notes fortinet_fortios_traffic fgt_traffic netfw none fortinet_fortios_utm fgt_utm netfw none fortinet_fortios_event fgt_event netops none fortinet_fortios_log fgt_log netops none Source Setup and Configuration \u00b6 Refer to the admin manual for specific details of configuration to send Reliable syslog using RFC 3195 format, a typical logging configuration will include the following features. config log memory filter set forward - traffic enable set local - traffic enable set sniffer - traffic disable set anomaly enable set voip disable set multicast - traffic enable set dns enable end config system global set cli - audit - log enable end config log setting set neighbor - event enable end Options \u00b6 Variable default description SC4S_OPTION_FORTINET_SOURCETYPE_PREFIX fgt Notice starting with version 1.6 of the fortinet add-on and app the sourcetype required changes from fgt_* to fortinet_* this is a breaking change to use the new sourcetype set this variable to fortigate in the env_file","title":"Fortios"},{"location":"sources/vendor/Fortinet/fortios/#fortios","text":"","title":"Fortios"},{"location":"sources/vendor/Fortinet/fortios/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Fortinet/fortios/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2846/ Product Manual https://docs.fortinet.com/product/fortigate/6.2","title":"Links"},{"location":"sources/vendor/Fortinet/fortios/#sourcetypes","text":"sourcetype notes fgt_log Catch-all sourcetype; not used by the TA fgt_traffic None fgt_utm None fgt_event None","title":"Sourcetypes"},{"location":"sources/vendor/Fortinet/fortios/#sourcetype-and-index-configuration","text":"key sourcetype index notes fortinet_fortios_traffic fgt_traffic netfw none fortinet_fortios_utm fgt_utm netfw none fortinet_fortios_event fgt_event netops none fortinet_fortios_log fgt_log netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Fortinet/fortios/#source-setup-and-configuration","text":"Refer to the admin manual for specific details of configuration to send Reliable syslog using RFC 3195 format, a typical logging configuration will include the following features. config log memory filter set forward - traffic enable set local - traffic enable set sniffer - traffic disable set anomaly enable set voip disable set multicast - traffic enable set dns enable end config system global set cli - audit - log enable end config log setting set neighbor - event enable end","title":"Source Setup and Configuration"},{"location":"sources/vendor/Fortinet/fortios/#options","text":"Variable default description SC4S_OPTION_FORTINET_SOURCETYPE_PREFIX fgt Notice starting with version 1.6 of the fortinet add-on and app the sourcetype required changes from fgt_* to fortinet_* this is a breaking change to use the new sourcetype set this variable to fortigate in the env_file","title":"Options"},{"location":"sources/vendor/Fortinet/fortiweb/","text":"FortiWeb \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4679/ Product Manual https://docs.fortinet.com/product/fortiweb/6.3 Sourcetypes \u00b6 sourcetype notes fgt_log Catch-all sourcetype; not used by the TA fwb_traffic None fwb_attack None fwb_event None Sourcetype and Index Configuration \u00b6 key sourcetype index notes fortinet_fortiweb_traffic fwb_traffic netfw none fortinet_fortiweb_attack fwb_attack netids none fortinet_fortiweb_event fwb_event netops none fortinet_fortiweb_log fwb_log netops none Source Setup and Configuration \u00b6 Refer to the admin manual for specific details of configuration to send Reliable syslog using RFC 3195 format, a typical logging configuration will include the following features. config log syslog - policy edit splunk config syslog - server - list edit 1 set server x . x . x . x set port 514 ( Example . Should be the same as default or dedicated port selected for sc4s ) end end config log syslogd set policy splunk set status enable end","title":"FortiWeb"},{"location":"sources/vendor/Fortinet/fortiweb/#fortiweb","text":"","title":"FortiWeb"},{"location":"sources/vendor/Fortinet/fortiweb/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Fortinet/fortiweb/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4679/ Product Manual https://docs.fortinet.com/product/fortiweb/6.3","title":"Links"},{"location":"sources/vendor/Fortinet/fortiweb/#sourcetypes","text":"sourcetype notes fgt_log Catch-all sourcetype; not used by the TA fwb_traffic None fwb_attack None fwb_event None","title":"Sourcetypes"},{"location":"sources/vendor/Fortinet/fortiweb/#sourcetype-and-index-configuration","text":"key sourcetype index notes fortinet_fortiweb_traffic fwb_traffic netfw none fortinet_fortiweb_attack fwb_attack netids none fortinet_fortiweb_event fwb_event netops none fortinet_fortiweb_log fwb_log netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Fortinet/fortiweb/#source-setup-and-configuration","text":"Refer to the admin manual for specific details of configuration to send Reliable syslog using RFC 3195 format, a typical logging configuration will include the following features. config log syslog - policy edit splunk config syslog - server - list edit 1 set server x . x . x . x set port 514 ( Example . Should be the same as default or dedicated port selected for sc4s ) end end config log syslogd set policy splunk set status enable end","title":"Source Setup and Configuration"},{"location":"sources/vendor/GitHub/","text":"Enterprise Server \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on Product Manual Sourcetypes \u00b6 sourcetype notes github:enterprise:audit The audit logs of GitHub Enterprise server have information about audites actions performed by github user. Sourcetype and Index Configuration \u00b6 key sourcetype index notes github_ent github:enterprise:audit gitops None","title":"Enterprise Server"},{"location":"sources/vendor/GitHub/#enterprise-server","text":"","title":"Enterprise Server"},{"location":"sources/vendor/GitHub/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/GitHub/#links","text":"Ref Link Splunk Add-on Product Manual","title":"Links"},{"location":"sources/vendor/GitHub/#sourcetypes","text":"sourcetype notes github:enterprise:audit The audit logs of GitHub Enterprise server have information about audites actions performed by github user.","title":"Sourcetypes"},{"location":"sources/vendor/GitHub/#sourcetype-and-index-configuration","text":"key sourcetype index notes github_ent github:enterprise:audit gitops None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/HAProxy/syslog/","text":"HAProxy \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3135/ Sourcetypes \u00b6 sourcetype notes haproxy:tcp Default syslog format haproxy:splunk:http Splunk\u2019s documented custom format. Note: detection is based on client_ip prefix in message Index Configuration \u00b6 key index notes haproxy_syslog netlb none","title":"HAProxy"},{"location":"sources/vendor/HAProxy/syslog/#haproxy","text":"","title":"HAProxy"},{"location":"sources/vendor/HAProxy/syslog/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/HAProxy/syslog/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3135/","title":"Links"},{"location":"sources/vendor/HAProxy/syslog/#sourcetypes","text":"sourcetype notes haproxy:tcp Default syslog format haproxy:splunk:http Splunk\u2019s documented custom format. Note: detection is based on client_ip prefix in message","title":"Sourcetypes"},{"location":"sources/vendor/HAProxy/syslog/#index-configuration","text":"key index notes haproxy_syslog netlb none","title":"Index Configuration"},{"location":"sources/vendor/HPe/ilo/","text":"ILO (4+) \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Sourcetypes \u00b6 sourcetype notes hpe:ilo none Index Configuration \u00b6 key index notes hpe_ilo infraops none","title":"ILO (4+)"},{"location":"sources/vendor/HPe/ilo/#ilo-4","text":"","title":"ILO (4+)"},{"location":"sources/vendor/HPe/ilo/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/HPe/ilo/#links","text":"","title":"Links"},{"location":"sources/vendor/HPe/ilo/#sourcetypes","text":"sourcetype notes hpe:ilo none","title":"Sourcetypes"},{"location":"sources/vendor/HPe/ilo/#index-configuration","text":"key index notes hpe_ilo infraops none","title":"Index Configuration"},{"location":"sources/vendor/HPe/jedirect/","text":"JetDirect \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Sourcetypes \u00b6 sourcetype notes hpe:jetdirect none Index Configuration \u00b6 key index notes hpe_jetdirect print none","title":"Jedirect"},{"location":"sources/vendor/HPe/jedirect/#jetdirect","text":"","title":"JetDirect"},{"location":"sources/vendor/HPe/jedirect/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/HPe/jedirect/#links","text":"Ref Link","title":"Links"},{"location":"sources/vendor/HPe/jedirect/#sourcetypes","text":"sourcetype notes hpe:jetdirect none","title":"Sourcetypes"},{"location":"sources/vendor/HPe/jedirect/#index-configuration","text":"key index notes hpe_jetdirect print none","title":"Index Configuration"},{"location":"sources/vendor/HPe/procurve/","text":"Procurve Switch \u00b6 HP Procurve switches have multiple log formats used. Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Switch https://support.hpe.com/hpesc/public/docDisplay?docId=a00091844en_us Switch (A Series) (Flex) https://techhub.hpe.com/eginfolib/networking/docs/switches/12500/5998-4870_nmm_cg/content/378584395.htm Sourcetypes \u00b6 sourcetype notes hpe:procurve none Index Configuration \u00b6 key index notes hpe_procurve netops none","title":"Procurve Switch"},{"location":"sources/vendor/HPe/procurve/#procurve-switch","text":"HP Procurve switches have multiple log formats used.","title":"Procurve Switch"},{"location":"sources/vendor/HPe/procurve/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/HPe/procurve/#links","text":"Ref Link Switch https://support.hpe.com/hpesc/public/docDisplay?docId=a00091844en_us Switch (A Series) (Flex) https://techhub.hpe.com/eginfolib/networking/docs/switches/12500/5998-4870_nmm_cg/content/378584395.htm","title":"Links"},{"location":"sources/vendor/HPe/procurve/#sourcetypes","text":"sourcetype notes hpe:procurve none","title":"Sourcetypes"},{"location":"sources/vendor/HPe/procurve/#index-configuration","text":"key index notes hpe_procurve netops none","title":"Index Configuration"},{"location":"sources/vendor/IBM/datapower/","text":"Data power \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4662/ Sourcetypes \u00b6 sourcetype notes ibm:datapower:syslog Common sourcetype ibm:datapower:* * is taken from the event sourcetype Index Configuration \u00b6 key source index notes ibm_datapower na inifraops none Parser Configuration \u00b6 Parser configuration is conditional only required if additional events are produced by the device that do not match the default configuration. #/opt/sc4s/local/config/app-parsers/app-vps-ibm_datapower.conf #File name provided is a suggestion it must be globally unique application app - vps - test - ibm_datapower [ sc4s - vps ] { filter { host ( \"^test-ibmdp-\" ) }; parser { p_set_netsource_fields ( vendor ( ' ibm ' ) product ( ' datapower ' ) ); }; };","title":"Data power"},{"location":"sources/vendor/IBM/datapower/#data-power","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4662/","title":"Data power"},{"location":"sources/vendor/IBM/datapower/#sourcetypes","text":"sourcetype notes ibm:datapower:syslog Common sourcetype ibm:datapower:* * is taken from the event sourcetype","title":"Sourcetypes"},{"location":"sources/vendor/IBM/datapower/#index-configuration","text":"key source index notes ibm_datapower na inifraops none","title":"Index Configuration"},{"location":"sources/vendor/IBM/datapower/#parser-configuration","text":"Parser configuration is conditional only required if additional events are produced by the device that do not match the default configuration. #/opt/sc4s/local/config/app-parsers/app-vps-ibm_datapower.conf #File name provided is a suggestion it must be globally unique application app - vps - test - ibm_datapower [ sc4s - vps ] { filter { host ( \"^test-ibmdp-\" ) }; parser { p_set_netsource_fields ( vendor ( ' ibm ' ) product ( ' datapower ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/ISC/bind/","text":"bind \u00b6 This source type is often re-implemented by specific add-ons such as infoblox or bluecat if a more specific source type is desired see that source documentation for instructions Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2876/ Sourcetypes \u00b6 sourcetype notes isc:bind none Index Configuration \u00b6 key index notes isc_bind isc:bind none","title":"bind"},{"location":"sources/vendor/ISC/bind/#bind","text":"This source type is often re-implemented by specific add-ons such as infoblox or bluecat if a more specific source type is desired see that source documentation for instructions","title":"bind"},{"location":"sources/vendor/ISC/bind/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/ISC/bind/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2876/","title":"Links"},{"location":"sources/vendor/ISC/bind/#sourcetypes","text":"sourcetype notes isc:bind none","title":"Sourcetypes"},{"location":"sources/vendor/ISC/bind/#index-configuration","text":"key index notes isc_bind isc:bind none","title":"Index Configuration"},{"location":"sources/vendor/ISC/dhcpd/","text":"dhcpd \u00b6 This source type is often re-implemented by specific add-ons such as infoblox or bluecat if a more specific source type is desired see that source documentation for instructions Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3010/ Sourcetypes \u00b6 sourcetype notes isc:dhcp none Index Configuration \u00b6 key index notes isc_dhcp isc:dhcp none Filter type \u00b6 MSG Parse: This filter parses message content Options \u00b6 None Verification \u00b6 An active site will generate frequent events use the following search to check for new events Verify timestamp, and host values match as expected index=<asconfigured> (sourcetype=isc:dhcp\")","title":"dhcpd"},{"location":"sources/vendor/ISC/dhcpd/#dhcpd","text":"This source type is often re-implemented by specific add-ons such as infoblox or bluecat if a more specific source type is desired see that source documentation for instructions","title":"dhcpd"},{"location":"sources/vendor/ISC/dhcpd/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/ISC/dhcpd/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3010/","title":"Links"},{"location":"sources/vendor/ISC/dhcpd/#sourcetypes","text":"sourcetype notes isc:dhcp none","title":"Sourcetypes"},{"location":"sources/vendor/ISC/dhcpd/#index-configuration","text":"key index notes isc_dhcp isc:dhcp none","title":"Index Configuration"},{"location":"sources/vendor/ISC/dhcpd/#filter-type","text":"MSG Parse: This filter parses message content","title":"Filter type"},{"location":"sources/vendor/ISC/dhcpd/#options","text":"None","title":"Options"},{"location":"sources/vendor/ISC/dhcpd/#verification","text":"An active site will generate frequent events use the following search to check for new events Verify timestamp, and host values match as expected index=<asconfigured> (sourcetype=isc:dhcp\")","title":"Verification"},{"location":"sources/vendor/Imperva/incapusla/","text":"Incapsula \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Splunk Add-on Source Specific https://bitbucket.org/SPLServices/ta-cef-imperva-incapsula/downloads/ Product Manual https://docs.imperva.com/bundle/cloud-application-security/page/more/log-configuration.htm Sourcetypes \u00b6 sourcetype notes cef Common sourcetype Source \u00b6 sourcetype notes Imperva:Incapsula Common sourcetype Index Configuration \u00b6 key source index notes Incapsula_SIEMintegration Imperva:Incapsula netwaf none","title":"Incapsula"},{"location":"sources/vendor/Imperva/incapusla/#incapsula","text":"","title":"Incapsula"},{"location":"sources/vendor/Imperva/incapusla/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Imperva/incapusla/#links","text":"Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Splunk Add-on Source Specific https://bitbucket.org/SPLServices/ta-cef-imperva-incapsula/downloads/ Product Manual https://docs.imperva.com/bundle/cloud-application-security/page/more/log-configuration.htm","title":"Links"},{"location":"sources/vendor/Imperva/incapusla/#sourcetypes","text":"sourcetype notes cef Common sourcetype","title":"Sourcetypes"},{"location":"sources/vendor/Imperva/incapusla/#source","text":"sourcetype notes Imperva:Incapsula Common sourcetype","title":"Source"},{"location":"sources/vendor/Imperva/incapusla/#index-configuration","text":"key source index notes Incapsula_SIEMintegration Imperva:Incapsula netwaf none","title":"Index Configuration"},{"location":"sources/vendor/Imperva/waf/","text":"On-Premises WAF (SecureSphere WAF) \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2874/ Product Manual https://community.microfocus.com/dcvta86296/attachments/dcvta86296/partner-documentation-h-o/22/2/Imperva_SecureSphere_11_5_CEF_Config_Guide_2018.pdf Sourcetypes \u00b6 sourcetype notes imperva:waf none imperva:waf:firewall:cef none imperva:waf:security:cef none Index Configuration \u00b6 key index notes Imperva Inc._SecureSphere netwaf none","title":"On-Premises WAF (SecureSphere WAF)"},{"location":"sources/vendor/Imperva/waf/#on-premises-waf-securesphere-waf","text":"","title":"On-Premises WAF (SecureSphere WAF)"},{"location":"sources/vendor/Imperva/waf/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Imperva/waf/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2874/ Product Manual https://community.microfocus.com/dcvta86296/attachments/dcvta86296/partner-documentation-h-o/22/2/Imperva_SecureSphere_11_5_CEF_Config_Guide_2018.pdf","title":"Links"},{"location":"sources/vendor/Imperva/waf/#sourcetypes","text":"sourcetype notes imperva:waf none imperva:waf:firewall:cef none imperva:waf:security:cef none","title":"Sourcetypes"},{"location":"sources/vendor/Imperva/waf/#index-configuration","text":"key index notes Imperva Inc._SecureSphere netwaf none","title":"Index Configuration"},{"location":"sources/vendor/InfoBlox/","text":"NIOS \u00b6 Warning: Despite the TA indication this data source is CIM compliant all versions of NIOS including the most recent available as of 2019-12-17 do not support the DNS data model correctly. For DNS security use cases use Splunk Stream instead. Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2934/ Product Manual https://docs.infoblox.com/display/ILP/NIOS?preview=/8945695/43728387/NIOS_8.4_Admin_Guide.pdf Sourcetypes \u00b6 sourcetype notes infoblox:dns None infoblox:dhcp None infoblox:threat None nix:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes infoblox_nios_dns infoblox:dns netdns none infoblox_nios_dhcp infoblox:dhcp netipam none infoblox_nios_threat infoblox:threatprotect netids none infoblox_nios_audit infoblox:audit netops none infoblox_nios_fallback infoblox:port netops none Options \u00b6 Variable default description SC4S_LISTEN_INFOBLOX_NIOS_UDP_PORT empty Vendor specific port SC4S_LISTEN_INFOBLOX_NIOS_TCP_PORT empty Vendor specific port Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-infoblox_nios.conf #File name provided is a suggestion it must be globally unique application app - vps - test - infoblox_nios [ sc4s - vps ] { filter { host ( \"infoblox-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' infoblox ' ) product ( ' nios ' ) ); }; };","title":"NIOS"},{"location":"sources/vendor/InfoBlox/#nios","text":"Warning: Despite the TA indication this data source is CIM compliant all versions of NIOS including the most recent available as of 2019-12-17 do not support the DNS data model correctly. For DNS security use cases use Splunk Stream instead.","title":"NIOS"},{"location":"sources/vendor/InfoBlox/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/InfoBlox/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2934/ Product Manual https://docs.infoblox.com/display/ILP/NIOS?preview=/8945695/43728387/NIOS_8.4_Admin_Guide.pdf","title":"Links"},{"location":"sources/vendor/InfoBlox/#sourcetypes","text":"sourcetype notes infoblox:dns None infoblox:dhcp None infoblox:threat None nix:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/InfoBlox/#sourcetype-and-index-configuration","text":"key sourcetype index notes infoblox_nios_dns infoblox:dns netdns none infoblox_nios_dhcp infoblox:dhcp netipam none infoblox_nios_threat infoblox:threatprotect netids none infoblox_nios_audit infoblox:audit netops none infoblox_nios_fallback infoblox:port netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/InfoBlox/#options","text":"Variable default description SC4S_LISTEN_INFOBLOX_NIOS_UDP_PORT empty Vendor specific port SC4S_LISTEN_INFOBLOX_NIOS_TCP_PORT empty Vendor specific port","title":"Options"},{"location":"sources/vendor/InfoBlox/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-infoblox_nios.conf #File name provided is a suggestion it must be globally unique application app - vps - test - infoblox_nios [ sc4s - vps ] { filter { host ( \"infoblox-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' infoblox ' ) product ( ' nios ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Juniper/junos/","text":"JunOS \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2847/ JunOS TechLibrary https://www.juniper.net/documentation/en_US/junos/topics/example/syslog-messages-configuring-qfx-series.html Sourcetypes \u00b6 sourcetype notes juniper:junos:firewall None juniper:junos:firewall:structured None juniper:junos:idp None juniper:junos:idp:structured None juniper:junos:aamw:structured None juniper:junos:secintel:structured None juniper:junos:snmp None Sourcetype and Index Configuration \u00b6 key sourcetype index notes juniper_junos_legacy juniper:legacy netops none juniper_junos_flow juniper:junos:firewall netfw none juniper_junos_utm juniper:junos:firewall netfw none juniper_junos_firewall juniper:junos:firewall netfw none juniper_junos_ids juniper:junos:firewall netids none juniper_junos_idp juniper:junos:idp netids none juniper_junos_snmp juniper:junos:snmp netops none juniper_junos_structured_fw juniper:junos:firewall:structured netfw none juniper_junos_structured_ids juniper:junos:firewall:structured netids none juniper_junos_structured_utm juniper:junos:firewall:structured netfw none juniper_junos_structured_idp juniper:junos:idp:structured netids none juniper_junos_structured_aamw juniper:junos:aamw:structured netfw none juniper_junos_structured_secintel juniper:junos:secintel:structured netfw none","title":"JunOS"},{"location":"sources/vendor/Juniper/junos/#junos","text":"","title":"JunOS"},{"location":"sources/vendor/Juniper/junos/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Juniper/junos/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2847/ JunOS TechLibrary https://www.juniper.net/documentation/en_US/junos/topics/example/syslog-messages-configuring-qfx-series.html","title":"Links"},{"location":"sources/vendor/Juniper/junos/#sourcetypes","text":"sourcetype notes juniper:junos:firewall None juniper:junos:firewall:structured None juniper:junos:idp None juniper:junos:idp:structured None juniper:junos:aamw:structured None juniper:junos:secintel:structured None juniper:junos:snmp None","title":"Sourcetypes"},{"location":"sources/vendor/Juniper/junos/#sourcetype-and-index-configuration","text":"key sourcetype index notes juniper_junos_legacy juniper:legacy netops none juniper_junos_flow juniper:junos:firewall netfw none juniper_junos_utm juniper:junos:firewall netfw none juniper_junos_firewall juniper:junos:firewall netfw none juniper_junos_ids juniper:junos:firewall netids none juniper_junos_idp juniper:junos:idp netids none juniper_junos_snmp juniper:junos:snmp netops none juniper_junos_structured_fw juniper:junos:firewall:structured netfw none juniper_junos_structured_ids juniper:junos:firewall:structured netids none juniper_junos_structured_utm juniper:junos:firewall:structured netfw none juniper_junos_structured_idp juniper:junos:idp:structured netids none juniper_junos_structured_aamw juniper:junos:aamw:structured netfw none juniper_junos_structured_secintel juniper:junos:secintel:structured netfw none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Juniper/netscreen/","text":"Netscreen \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2847/ Netscreen Manual http://kb.juniper.net/InfoCenter/index?page=content&id=KB4759 Sourcetypes \u00b6 sourcetype notes netscreen:firewall None Sourcetype and Index Configuration \u00b6 key sourcetype index notes juniper_netscreen netscreen:firewall netfw none","title":"Netscreen"},{"location":"sources/vendor/Juniper/netscreen/#netscreen","text":"","title":"Netscreen"},{"location":"sources/vendor/Juniper/netscreen/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Juniper/netscreen/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2847/ Netscreen Manual http://kb.juniper.net/InfoCenter/index?page=content&id=KB4759","title":"Links"},{"location":"sources/vendor/Juniper/netscreen/#sourcetypes","text":"sourcetype notes netscreen:firewall None","title":"Sourcetypes"},{"location":"sources/vendor/Juniper/netscreen/#sourcetype-and-index-configuration","text":"key sourcetype index notes juniper_netscreen netscreen:firewall netfw none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Kaspersky/es/","text":"Enterprise Security RFC5424 \u00b6 Key facts \u00b6 MSG Format based filter RFC5424 Links \u00b6 Ref Link Splunk Add-on non Sourcetypes \u00b6 sourcetype notes kaspersky:syslog:es Where PROGRAM starts with KES kaspersky:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes kaspersky_syslog kaspersky:syslog epav none kaspersky_syslog_es kaspersky:syslog:es epav none","title":"Enterprise Security RFC5424"},{"location":"sources/vendor/Kaspersky/es/#enterprise-security-rfc5424","text":"","title":"Enterprise Security RFC5424"},{"location":"sources/vendor/Kaspersky/es/#key-facts","text":"MSG Format based filter RFC5424","title":"Key facts"},{"location":"sources/vendor/Kaspersky/es/#links","text":"Ref Link Splunk Add-on non","title":"Links"},{"location":"sources/vendor/Kaspersky/es/#sourcetypes","text":"sourcetype notes kaspersky:syslog:es Where PROGRAM starts with KES kaspersky:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Kaspersky/es/#sourcetype-and-index-configuration","text":"key sourcetype index notes kaspersky_syslog kaspersky:syslog epav none kaspersky_syslog_es kaspersky:syslog:es epav none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Kaspersky/es_cef/","text":"Enterprise Security CEF \u00b6 The TA link provided has commented out the CEF support as of 2022-03-18 manual edits are required Key facts \u00b6 MSG Format based filter RFC5424 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4656/ Sourcetypes \u00b6 sourcetype notes kaspersky:cef kaspersky:klaud kaspersky:klsrv kaspersky:gnrl kaspersky:klnag kaspersky:klprci kaspersky:klbl Sourcetype and Index Configuration \u00b6 key sourcetype index notes KasperskyLab_SecurityCenter all epav none","title":"Enterprise Security CEF"},{"location":"sources/vendor/Kaspersky/es_cef/#enterprise-security-cef","text":"The TA link provided has commented out the CEF support as of 2022-03-18 manual edits are required","title":"Enterprise Security CEF"},{"location":"sources/vendor/Kaspersky/es_cef/#key-facts","text":"MSG Format based filter RFC5424","title":"Key facts"},{"location":"sources/vendor/Kaspersky/es_cef/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4656/","title":"Links"},{"location":"sources/vendor/Kaspersky/es_cef/#sourcetypes","text":"sourcetype notes kaspersky:cef kaspersky:klaud kaspersky:klsrv kaspersky:gnrl kaspersky:klnag kaspersky:klprci kaspersky:klbl","title":"Sourcetypes"},{"location":"sources/vendor/Kaspersky/es_cef/#sourcetype-and-index-configuration","text":"key sourcetype index notes KasperskyLab_SecurityCenter all epav none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Kaspersky/es_leef/","text":"Enterprise Security Leef \u00b6 Leef format has not been tested samples needed Key facts \u00b6 MSG Format based filter Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4656/ Sourcetypes \u00b6 sourcetype notes kaspersky:cef kaspersky:klaud kaspersky:klsrv kaspersky:gnrl kaspersky:klnag kaspersky:klprci kaspersky:klbl Sourcetype and Index Configuration \u00b6 key sourcetype index notes KasperskyLab_SecurityCenter all epav none","title":"Enterprise Security Leef"},{"location":"sources/vendor/Kaspersky/es_leef/#enterprise-security-leef","text":"Leef format has not been tested samples needed","title":"Enterprise Security Leef"},{"location":"sources/vendor/Kaspersky/es_leef/#key-facts","text":"MSG Format based filter","title":"Key facts"},{"location":"sources/vendor/Kaspersky/es_leef/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4656/","title":"Links"},{"location":"sources/vendor/Kaspersky/es_leef/#sourcetypes","text":"sourcetype notes kaspersky:cef kaspersky:klaud kaspersky:klsrv kaspersky:gnrl kaspersky:klnag kaspersky:klprci kaspersky:klbl","title":"Sourcetypes"},{"location":"sources/vendor/Kaspersky/es_leef/#sourcetype-and-index-configuration","text":"key sourcetype index notes KasperskyLab_SecurityCenter all epav none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Liveaction/liveaction_livenx/","text":"Liveaction - livenx \u00b6 Key facts \u00b6 Default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual None Sourcetypes \u00b6 sourcetype notes liveaction:livenx none Sourcetype and Index Configuration \u00b6 key sourcetype index notes liveaction_livenx liveaction:livenx netops None","title":"Liveaction - livenx"},{"location":"sources/vendor/Liveaction/liveaction_livenx/#liveaction-livenx","text":"","title":"Liveaction - livenx"},{"location":"sources/vendor/Liveaction/liveaction_livenx/#key-facts","text":"Default port 514","title":"Key facts"},{"location":"sources/vendor/Liveaction/liveaction_livenx/#links","text":"Ref Link Splunk Add-on None Product Manual None","title":"Links"},{"location":"sources/vendor/Liveaction/liveaction_livenx/#sourcetypes","text":"sourcetype notes liveaction:livenx none","title":"Sourcetypes"},{"location":"sources/vendor/Liveaction/liveaction_livenx/#sourcetype-and-index-configuration","text":"key sourcetype index notes liveaction_livenx liveaction:livenx netops None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/McAfee/epo/","text":"EPO \u00b6 Key facts \u00b6 MSG Format based filter Source requires use of TLS legacy BSD port 6514 TLS Certificate must be trusted by EPO instance Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/5085/ Product Manual https://kc.mcafee.com/corporate/index?page=content&id=KB87927 Sourcetypes \u00b6 sourcetype notes mcafee:epo:syslog none Source \u00b6 source notes policy_auditor_vulnerability_assessment Policy Auditor Vulnerability Assessment events mcafee_agent McAfee Agent events mcafee_endpoint_security McAfee Endpoint Security events Index Configuration \u00b6 key index notes mcafee_epo epav none Filter type \u00b6 MSG Parse: This filter parses message content Options \u00b6 Variable default description SC4S_LISTEN_MCAFEE_EPO_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_MCAFEE_EPO no Enable archive to disk for this specific source SC4S_DEST_MCAFEE_EPO_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source SC4S_SOURCE_TLS_ENABLE no This must be set to yes so that SC4S listens for encrypted syslog from ePO Additional setup \u00b6 You must create a certificate for the SC4S server to receive encrypted syslog from ePO. A self-signed certificate is fine. Generate a self-signed certificate on the SC4S host: openssl req -newkey rsa:2048 -new -nodes -x509 -days 3650 -keyout /opt/sc4s/tls/server.key -out /opt/sc4s/tls/server.pem Uncomment the following line in /lib/systemd/system/sc4s.service to allow the docker container to use the certificate: Environment=\"SC4S_TLS_DIR=-v :/etc/syslog-ng/tls:z\" Troubleshooting \u00b6 from the command line of the SC4S host, run this: openssl s_client -connect localhost:6514 The message: socket : Bad file descriptor connect : errno = 9 indicates that SC4S is not listening for encrypted syslog. Note that a netstat may show the port open, but it is not accepting encrypted traffic as configured. It may take several minutes for the syslog option to be available in the registered servers dropdown.","title":"EPO"},{"location":"sources/vendor/McAfee/epo/#epo","text":"","title":"EPO"},{"location":"sources/vendor/McAfee/epo/#key-facts","text":"MSG Format based filter Source requires use of TLS legacy BSD port 6514 TLS Certificate must be trusted by EPO instance","title":"Key facts"},{"location":"sources/vendor/McAfee/epo/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/5085/ Product Manual https://kc.mcafee.com/corporate/index?page=content&id=KB87927","title":"Links"},{"location":"sources/vendor/McAfee/epo/#sourcetypes","text":"sourcetype notes mcafee:epo:syslog none","title":"Sourcetypes"},{"location":"sources/vendor/McAfee/epo/#source","text":"source notes policy_auditor_vulnerability_assessment Policy Auditor Vulnerability Assessment events mcafee_agent McAfee Agent events mcafee_endpoint_security McAfee Endpoint Security events","title":"Source"},{"location":"sources/vendor/McAfee/epo/#index-configuration","text":"key index notes mcafee_epo epav none","title":"Index Configuration"},{"location":"sources/vendor/McAfee/epo/#filter-type","text":"MSG Parse: This filter parses message content","title":"Filter type"},{"location":"sources/vendor/McAfee/epo/#options","text":"Variable default description SC4S_LISTEN_MCAFEE_EPO_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_MCAFEE_EPO no Enable archive to disk for this specific source SC4S_DEST_MCAFEE_EPO_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source SC4S_SOURCE_TLS_ENABLE no This must be set to yes so that SC4S listens for encrypted syslog from ePO","title":"Options"},{"location":"sources/vendor/McAfee/epo/#additional-setup","text":"You must create a certificate for the SC4S server to receive encrypted syslog from ePO. A self-signed certificate is fine. Generate a self-signed certificate on the SC4S host: openssl req -newkey rsa:2048 -new -nodes -x509 -days 3650 -keyout /opt/sc4s/tls/server.key -out /opt/sc4s/tls/server.pem Uncomment the following line in /lib/systemd/system/sc4s.service to allow the docker container to use the certificate: Environment=\"SC4S_TLS_DIR=-v :/etc/syslog-ng/tls:z\"","title":"Additional setup"},{"location":"sources/vendor/McAfee/epo/#troubleshooting","text":"from the command line of the SC4S host, run this: openssl s_client -connect localhost:6514 The message: socket : Bad file descriptor connect : errno = 9 indicates that SC4S is not listening for encrypted syslog. Note that a netstat may show the port open, but it is not accepting encrypted traffic as configured. It may take several minutes for the syslog option to be available in the registered servers dropdown.","title":"Troubleshooting"},{"location":"sources/vendor/McAfee/nsp/","text":"Network Security Platform \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Product Manual https://docs.mcafee.com/bundle/network-security-platform-10.1.x-product-guide/page/GUID-373C1CA6-EC0E-49E1-8858-749D1AA2716A.html Sourcetypes \u00b6 sourcetype notes mcafee:nsp none Source \u00b6 source notes mcafee:nsp:alert Alert/Attack Events mcafee:nsp:audit Audit Event or User Activity Events mcafee:nsp:fault Fault Events mcafee:nsp:firewall Firewall Events Index Configuration \u00b6 key index notes mcafee_nsp netids none","title":"Network Security Platform"},{"location":"sources/vendor/McAfee/nsp/#network-security-platform","text":"","title":"Network Security Platform"},{"location":"sources/vendor/McAfee/nsp/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/McAfee/nsp/#links","text":"Ref Link Product Manual https://docs.mcafee.com/bundle/network-security-platform-10.1.x-product-guide/page/GUID-373C1CA6-EC0E-49E1-8858-749D1AA2716A.html","title":"Links"},{"location":"sources/vendor/McAfee/nsp/#sourcetypes","text":"sourcetype notes mcafee:nsp none","title":"Sourcetypes"},{"location":"sources/vendor/McAfee/nsp/#source","text":"source notes mcafee:nsp:alert Alert/Attack Events mcafee:nsp:audit Audit Event or User Activity Events mcafee:nsp:fault Fault Events mcafee:nsp:firewall Firewall Events","title":"Source"},{"location":"sources/vendor/McAfee/nsp/#index-configuration","text":"key index notes mcafee_nsp netids none","title":"Index Configuration"},{"location":"sources/vendor/McAfee/wg/","text":"Web Gateway \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3009/ Product Manual https://kc.mcafee.com/corporate/index?page=content&id=KB77988&actp=RSS Sourcetypes \u00b6 sourcetype notes mcafee:wg:kv none Index Configuration \u00b6 key index notes mcafee_wg netproxy none","title":"Wg"},{"location":"sources/vendor/McAfee/wg/#web-gateway","text":"","title":"Web Gateway"},{"location":"sources/vendor/McAfee/wg/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/McAfee/wg/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3009/ Product Manual https://kc.mcafee.com/corporate/index?page=content&id=KB77988&actp=RSS","title":"Links"},{"location":"sources/vendor/McAfee/wg/#sourcetypes","text":"sourcetype notes mcafee:wg:kv none","title":"Sourcetypes"},{"location":"sources/vendor/McAfee/wg/#index-configuration","text":"key index notes mcafee_wg netproxy none","title":"Index Configuration"},{"location":"sources/vendor/Microfocus/arcsight/","text":"Arcsight Internal Agent \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on CEF https://github.com/splunk/splunk-add-on-for-cef/downloads/ Sourcetypes \u00b6 sourcetype notes cef Common sourcetype Source \u00b6 source notes ArcSight:ArcSight Internal logs Index Configuration \u00b6 key source index notes ArcSight_ArcSight ArcSight:ArcSight main none","title":"Arcsight Internal Agent"},{"location":"sources/vendor/Microfocus/arcsight/#arcsight-internal-agent","text":"","title":"Arcsight Internal Agent"},{"location":"sources/vendor/Microfocus/arcsight/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Microfocus/arcsight/#links","text":"Ref Link Splunk Add-on CEF https://github.com/splunk/splunk-add-on-for-cef/downloads/","title":"Links"},{"location":"sources/vendor/Microfocus/arcsight/#sourcetypes","text":"sourcetype notes cef Common sourcetype","title":"Sourcetypes"},{"location":"sources/vendor/Microfocus/arcsight/#source","text":"source notes ArcSight:ArcSight Internal logs","title":"Source"},{"location":"sources/vendor/Microfocus/arcsight/#index-configuration","text":"key source index notes ArcSight_ArcSight ArcSight:ArcSight main none","title":"Index Configuration"},{"location":"sources/vendor/Microfocus/windows/","text":"Arcsight Microsoft Windows (CEF) \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-microsoft-windows-for-splunk/downloads/ Product Manual https://docs.imperva.com/bundle/cloud-application-security/page/more/log-configuration.htm Sourcetypes \u00b6 sourcetype notes cef Common sourcetype Source \u00b6 source notes CEFEventLog:System or Application Event Windows Application and System Event Logs CEFEventLog:Microsoft Windows Windows Security Event Logs Index Configuration \u00b6 key source index notes Microsoft_System or Application Event CEFEventLog:System or Application Event oswin none Microsoft_Microsoft Windows CEFEventLog:Microsoft Windows oswinsec none","title":"Arcsight Microsoft Windows (CEF)"},{"location":"sources/vendor/Microfocus/windows/#arcsight-microsoft-windows-cef","text":"","title":"Arcsight Microsoft Windows (CEF)"},{"location":"sources/vendor/Microfocus/windows/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Microfocus/windows/#links","text":"Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-microsoft-windows-for-splunk/downloads/ Product Manual https://docs.imperva.com/bundle/cloud-application-security/page/more/log-configuration.htm","title":"Links"},{"location":"sources/vendor/Microfocus/windows/#sourcetypes","text":"sourcetype notes cef Common sourcetype","title":"Sourcetypes"},{"location":"sources/vendor/Microfocus/windows/#source","text":"source notes CEFEventLog:System or Application Event Windows Application and System Event Logs CEFEventLog:Microsoft Windows Windows Security Event Logs","title":"Source"},{"location":"sources/vendor/Microfocus/windows/#index-configuration","text":"key source index notes Microsoft_System or Application Event CEFEventLog:System or Application Event oswin none Microsoft_Microsoft Windows CEFEventLog:Microsoft Windows oswinsec none","title":"Index Configuration"},{"location":"sources/vendor/Microsoft/","text":"Cloud App Security (MCAS) \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Splunk Add-on Source Specific none Product Manual https://docs.microsoft.com/en-us/cloud-app-security/siem Sourcetypes \u00b6 sourcetype notes cef Common sourcetype Source \u00b6 source notes microsoft:cas Common sourcetype Index Configuration \u00b6 key source index notes MCAS_SIEM_Agent microsoft:cas main none","title":"Cloud App Security (MCAS)"},{"location":"sources/vendor/Microsoft/#cloud-app-security-mcas","text":"","title":"Cloud App Security (MCAS)"},{"location":"sources/vendor/Microsoft/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Microsoft/#links","text":"Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Splunk Add-on Source Specific none Product Manual https://docs.microsoft.com/en-us/cloud-app-security/siem","title":"Links"},{"location":"sources/vendor/Microsoft/#sourcetypes","text":"sourcetype notes cef Common sourcetype","title":"Sourcetypes"},{"location":"sources/vendor/Microsoft/#source","text":"source notes microsoft:cas Common sourcetype","title":"Source"},{"location":"sources/vendor/Microsoft/#index-configuration","text":"key source index notes MCAS_SIEM_Agent microsoft:cas main none","title":"Index Configuration"},{"location":"sources/vendor/Mikrotik/routeros/","text":"RouterOS \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 RouterOS will send ISC Bind and ISC DHCPD events Links \u00b6 Sourcetypes \u00b6 sourcetype notes routeros none Index Configuration \u00b6 key index notes mikrotik_routeros netops none mikrotik_routeros_fw netfw Used for events with forward: Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-mikrotik_routeros.conf #File name provided is a suggestion it must be globally unique application app - vps - test - mikrotik_routeros [ sc4s - vps ] { filter { host ( \"test-mrtros-\" type ( string ) flags ( prefix )) }; parser { p_set_netsource_fields ( vendor ( ' mikrotik ' ) product ( ' routeros ' ) ); }; };","title":"RouterOS"},{"location":"sources/vendor/Mikrotik/routeros/#routeros","text":"","title":"RouterOS"},{"location":"sources/vendor/Mikrotik/routeros/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514 RouterOS will send ISC Bind and ISC DHCPD events","title":"Key facts"},{"location":"sources/vendor/Mikrotik/routeros/#links","text":"","title":"Links"},{"location":"sources/vendor/Mikrotik/routeros/#sourcetypes","text":"sourcetype notes routeros none","title":"Sourcetypes"},{"location":"sources/vendor/Mikrotik/routeros/#index-configuration","text":"key index notes mikrotik_routeros netops none mikrotik_routeros_fw netfw Used for events with forward:","title":"Index Configuration"},{"location":"sources/vendor/Mikrotik/routeros/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-mikrotik_routeros.conf #File name provided is a suggestion it must be globally unique application app - vps - test - mikrotik_routeros [ sc4s - vps ] { filter { host ( \"test-mrtros-\" type ( string ) flags ( prefix )) }; parser { p_set_netsource_fields ( vendor ( ' mikrotik ' ) product ( ' routeros ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/NetApp/ontap/","text":"OnTap \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3418/ Product Manual unknown Sourcetypes \u00b6 sourcetype notes netapp:ems None Sourcetype and Index Configuration \u00b6 key sourcetype index notes netapp_ontap netapp:ems infraops none","title":"OnTap"},{"location":"sources/vendor/NetApp/ontap/#ontap","text":"","title":"OnTap"},{"location":"sources/vendor/NetApp/ontap/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/NetApp/ontap/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3418/ Product Manual unknown","title":"Links"},{"location":"sources/vendor/NetApp/ontap/#sourcetypes","text":"sourcetype notes netapp:ems None","title":"Sourcetypes"},{"location":"sources/vendor/NetApp/ontap/#sourcetype-and-index-configuration","text":"key sourcetype index notes netapp_ontap netapp:ems infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/NetApp/storage-grid/","text":"StorageGRID \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Community requested parser Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3895/ Product Manual unknown Sourcetypes \u00b6 sourcetype notes grid:auditlog None grid:rest:api None Sourcetype and Index Configuration \u00b6 key sourcetype index notes netapp_grid grid:auditlog infraops none netapp_grid grid:rest:api infraops none","title":"StorageGRID"},{"location":"sources/vendor/NetApp/storage-grid/#storagegrid","text":"","title":"StorageGRID"},{"location":"sources/vendor/NetApp/storage-grid/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514 Community requested parser","title":"Key facts"},{"location":"sources/vendor/NetApp/storage-grid/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3895/ Product Manual unknown","title":"Links"},{"location":"sources/vendor/NetApp/storage-grid/#sourcetypes","text":"sourcetype notes grid:auditlog None grid:rest:api None","title":"Sourcetypes"},{"location":"sources/vendor/NetApp/storage-grid/#sourcetype-and-index-configuration","text":"key sourcetype index notes netapp_grid grid:auditlog infraops none netapp_grid grid:rest:api infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/NetScout/arbor_edge/","text":"DatAdvantage \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link TA https://github.com/arbor/TA_netscout_aed Sourcetypes \u00b6 sourcetype notes netscout:aed Index Configuration \u00b6 key sourcetype index notes NETSCOUT_Arbor Edge Defense netscout:aed netids NETSCOUT_Arbor Networks APS netscout:aed netids","title":"DatAdvantage"},{"location":"sources/vendor/NetScout/arbor_edge/#datadvantage","text":"","title":"DatAdvantage"},{"location":"sources/vendor/NetScout/arbor_edge/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/NetScout/arbor_edge/#links","text":"Ref Link TA https://github.com/arbor/TA_netscout_aed","title":"Links"},{"location":"sources/vendor/NetScout/arbor_edge/#sourcetypes","text":"sourcetype notes netscout:aed","title":"Sourcetypes"},{"location":"sources/vendor/NetScout/arbor_edge/#index-configuration","text":"key sourcetype index notes NETSCOUT_Arbor Edge Defense netscout:aed netids NETSCOUT_Arbor Networks APS netscout:aed netids","title":"Index Configuration"},{"location":"sources/vendor/Netmotion/mobilityserver/","text":"Mobility Server \u00b6 Key facts \u00b6 MSG Format based filter Links \u00b6 Ref Link Splunk Add-on none Product Manual unknown Sourcetypes \u00b6 sourcetype notes netmotion:mobilityserver:* The third segment of the source type is constructed from the sdid field of the syslog sdata Sourcetype and Index Configuration \u00b6 key sourcetype index notes netmotion_mobility-server_* netmotion:mobilityserver:* netops none","title":"Mobility Server"},{"location":"sources/vendor/Netmotion/mobilityserver/#mobility-server","text":"","title":"Mobility Server"},{"location":"sources/vendor/Netmotion/mobilityserver/#key-facts","text":"MSG Format based filter","title":"Key facts"},{"location":"sources/vendor/Netmotion/mobilityserver/#links","text":"Ref Link Splunk Add-on none Product Manual unknown","title":"Links"},{"location":"sources/vendor/Netmotion/mobilityserver/#sourcetypes","text":"sourcetype notes netmotion:mobilityserver:* The third segment of the source type is constructed from the sdid field of the syslog sdata","title":"Sourcetypes"},{"location":"sources/vendor/Netmotion/mobilityserver/#sourcetype-and-index-configuration","text":"key sourcetype index notes netmotion_mobility-server_* netmotion:mobilityserver:* netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Netmotion/reporting/","text":"Reporting \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on none Product Manual unknown Sourcetypes \u00b6 sourcetype notes netmotion:reporting None Sourcetype and Index Configuration \u00b6 key sourcetype index notes netmotion_reporting netmotion:reporting netops none","title":"Reporting"},{"location":"sources/vendor/Netmotion/reporting/#reporting","text":"","title":"Reporting"},{"location":"sources/vendor/Netmotion/reporting/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Netmotion/reporting/#links","text":"Ref Link Splunk Add-on none Product Manual unknown","title":"Links"},{"location":"sources/vendor/Netmotion/reporting/#sourcetypes","text":"sourcetype notes netmotion:reporting None","title":"Sourcetypes"},{"location":"sources/vendor/Netmotion/reporting/#sourcetype-and-index-configuration","text":"key sourcetype index notes netmotion_reporting netmotion:reporting netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Novell/netiq/","text":"NetIQ \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes novell:netiq none Sourcetype and Index Configuration \u00b6 key sourcetype index notes novell_netiq novell_netiq netauth None","title":"NetIQ"},{"location":"sources/vendor/Novell/netiq/#netiq","text":"","title":"NetIQ"},{"location":"sources/vendor/Novell/netiq/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Novell/netiq/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Novell/netiq/#sourcetypes","text":"sourcetype notes novell:netiq none","title":"Sourcetypes"},{"location":"sources/vendor/Novell/netiq/#sourcetype-and-index-configuration","text":"key sourcetype index notes novell_netiq novell_netiq netauth None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Nutanix/cvm/","text":"Nutanix_CVM_Audit \u00b6 Key facts \u00b6 MSG Format based filter Community requested filter Only CVM log supported Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes nutanix:syslog CVM logs nutanix:syslog:audit CVM system audit logs Considering the message host format is default ntnx-xxxx-cvm Sourcetype and Index Configuration \u00b6 key sourcetype index notes nutanix_syslog nutanix:syslog infraops none nutanix_syslog_audit nutanix:syslog:audit infraops none","title":"Nutanix_CVM_Audit"},{"location":"sources/vendor/Nutanix/cvm/#nutanix_cvm_audit","text":"","title":"Nutanix_CVM_Audit"},{"location":"sources/vendor/Nutanix/cvm/#key-facts","text":"MSG Format based filter Community requested filter Only CVM log supported","title":"Key facts"},{"location":"sources/vendor/Nutanix/cvm/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Nutanix/cvm/#sourcetypes","text":"sourcetype notes nutanix:syslog CVM logs nutanix:syslog:audit CVM system audit logs Considering the message host format is default ntnx-xxxx-cvm","title":"Sourcetypes"},{"location":"sources/vendor/Nutanix/cvm/#sourcetype-and-index-configuration","text":"key sourcetype index notes nutanix_syslog nutanix:syslog infraops none nutanix_syslog_audit nutanix:syslog:audit infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Ossec/ossec/","text":"Ossec \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2808/ Product Manual https://www.ossec.net/docs/index.html Sourcetypes \u00b6 sourcetype notes ossec The add-on supports data from the following sources: File Integrity Management (FIM) data, FTP data, su data, ssh data, Windows data, including audit and logon information Sourcetype and Index Configuration \u00b6 key sourcetype index notes ossec_ossec ossec main None","title":"Ossec"},{"location":"sources/vendor/Ossec/ossec/#ossec","text":"","title":"Ossec"},{"location":"sources/vendor/Ossec/ossec/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Ossec/ossec/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2808/ Product Manual https://www.ossec.net/docs/index.html","title":"Links"},{"location":"sources/vendor/Ossec/ossec/#sourcetypes","text":"sourcetype notes ossec The add-on supports data from the following sources: File Integrity Management (FIM) data, FTP data, su data, ssh data, Windows data, including audit and logon information","title":"Sourcetypes"},{"location":"sources/vendor/Ossec/ossec/#sourcetype-and-index-configuration","text":"key sourcetype index notes ossec_ossec ossec main None","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/PaloaltoNetworks/cortexxdr/","text":"Cortext \u00b6 Key facts \u00b6 MSG Format based filter Cortex requires TLS and uses IETF Framed SYSLOG default port is 6587 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2757/ Sourcetypes \u00b6 sourcetype notes pan:* pan:xsoar none Index Configuration \u00b6 key index notes Palo Alto Networks_Palo Alto Networks Cortex XSOAR epintel none","title":"Cortext"},{"location":"sources/vendor/PaloaltoNetworks/cortexxdr/#cortext","text":"","title":"Cortext"},{"location":"sources/vendor/PaloaltoNetworks/cortexxdr/#key-facts","text":"MSG Format based filter Cortex requires TLS and uses IETF Framed SYSLOG default port is 6587 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2757/","title":"Key facts"},{"location":"sources/vendor/PaloaltoNetworks/cortexxdr/#sourcetypes","text":"sourcetype notes pan:* pan:xsoar none","title":"Sourcetypes"},{"location":"sources/vendor/PaloaltoNetworks/cortexxdr/#index-configuration","text":"key index notes Palo Alto Networks_Palo Alto Networks Cortex XSOAR epintel none","title":"Index Configuration"},{"location":"sources/vendor/PaloaltoNetworks/panos/","text":"panos \u00b6 Key facts \u00b6 MSG Format based filter from NGFW, PANORAMA OR CORTEX data lake Legacy BSD Format default port 514 used by default. \u201cDefault TCP/UDP\u201d is 30% slower than preferred IETF Framed IMPORTANT IETF Framed syslog must use port 601 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2757/ Product Manual https://docs.paloaltonetworks.com/pan-os/9-0/pan-os-admin/monitoring/use-syslog-for-monitoring/configure-syslog-monitoring.html Sourcetypes \u00b6 sourcetype notes pan:log None pan:pan_globalprotect none pan:traffic None pan:threat None pan:system None pan:config None pan:hipmatch None pan:correlation None Sourcetype and Index Configuration \u00b6 key sourcetype index notes pan_panos_log pan:log netops none pan_panos_globalprotect pan:pan_globalprotect netfw none pan_panos_traffic pan:traffic netfw none pan_panos_threat pan:threat netproxy none pan_panos_system pan:system netops none pan_panos_config pan:config netops none pan_panos_hipmatch pan:hipmatch netops none pan_panos_correlation pan:correlation netops none Filter type \u00b6 MSG Parse: This filter parses message content Setup and Configuration \u00b6 Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source. Refer to the admin manual for specific details of configuration Select TCP or SSL transport option Select IETF Format Ensure the format of the event is not customized Options \u00b6 Variable default description SC4S_LISTEN_PULSE_PAN_PANOS_RFC6587_PORT empty string Enable a TCP using IETF Framing (RFC6587) port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_PAN_PANOS_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_PAN_PANOS no Enable archive to disk for this specific source SC4S_DEST_PAN_PANOS_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source Verification \u00b6 An active firewall will generate frequent events. Use the following search to validate events are present per source device index=<asconfigured> sourcetype=pan:*| stats count by host","title":"panos"},{"location":"sources/vendor/PaloaltoNetworks/panos/#panos","text":"","title":"panos"},{"location":"sources/vendor/PaloaltoNetworks/panos/#key-facts","text":"MSG Format based filter from NGFW, PANORAMA OR CORTEX data lake Legacy BSD Format default port 514 used by default. \u201cDefault TCP/UDP\u201d is 30% slower than preferred IETF Framed IMPORTANT IETF Framed syslog must use port 601","title":"Key facts"},{"location":"sources/vendor/PaloaltoNetworks/panos/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2757/ Product Manual https://docs.paloaltonetworks.com/pan-os/9-0/pan-os-admin/monitoring/use-syslog-for-monitoring/configure-syslog-monitoring.html","title":"Links"},{"location":"sources/vendor/PaloaltoNetworks/panos/#sourcetypes","text":"sourcetype notes pan:log None pan:pan_globalprotect none pan:traffic None pan:threat None pan:system None pan:config None pan:hipmatch None pan:correlation None","title":"Sourcetypes"},{"location":"sources/vendor/PaloaltoNetworks/panos/#sourcetype-and-index-configuration","text":"key sourcetype index notes pan_panos_log pan:log netops none pan_panos_globalprotect pan:pan_globalprotect netfw none pan_panos_traffic pan:traffic netfw none pan_panos_threat pan:threat netproxy none pan_panos_system pan:system netops none pan_panos_config pan:config netops none pan_panos_hipmatch pan:hipmatch netops none pan_panos_correlation pan:correlation netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/PaloaltoNetworks/panos/#filter-type","text":"MSG Parse: This filter parses message content","title":"Filter type"},{"location":"sources/vendor/PaloaltoNetworks/panos/#setup-and-configuration","text":"Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source. Refer to the admin manual for specific details of configuration Select TCP or SSL transport option Select IETF Format Ensure the format of the event is not customized","title":"Setup and Configuration"},{"location":"sources/vendor/PaloaltoNetworks/panos/#options","text":"Variable default description SC4S_LISTEN_PULSE_PAN_PANOS_RFC6587_PORT empty string Enable a TCP using IETF Framing (RFC6587) port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_PAN_PANOS_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_PAN_PANOS no Enable archive to disk for this specific source SC4S_DEST_PAN_PANOS_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source","title":"Options"},{"location":"sources/vendor/PaloaltoNetworks/panos/#verification","text":"An active firewall will generate frequent events. Use the following search to validate events are present per source device index=<asconfigured> sourcetype=pan:*| stats count by host","title":"Verification"},{"location":"sources/vendor/PaloaltoNetworks/traps/","text":"TRAPS \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2757/ Sourcetypes \u00b6 sourcetype notes pan:traps4 none Index Configuration \u00b6 key index notes Palo Alto Networks_Traps Agent epintel none","title":"Traps"},{"location":"sources/vendor/PaloaltoNetworks/traps/#traps","text":"","title":"TRAPS"},{"location":"sources/vendor/PaloaltoNetworks/traps/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/PaloaltoNetworks/traps/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2757/","title":"Links"},{"location":"sources/vendor/PaloaltoNetworks/traps/#sourcetypes","text":"sourcetype notes pan:traps4 none","title":"Sourcetypes"},{"location":"sources/vendor/PaloaltoNetworks/traps/#index-configuration","text":"key index notes Palo Alto Networks_Traps Agent epintel none","title":"Index Configuration"},{"location":"sources/vendor/Pfsense/firewall/","text":"Firewall \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1527/ Product Manual https://docs.netgate.com/pfsense/en/latest/monitoring/copying-logs-to-a-remote-host-with-syslog.html?highlight=syslog Sourcetypes \u00b6 sourcetype notes pfsense:filterlog None pfsense:* All programs other than filterlog Sourcetype and Index Configuration \u00b6 key sourcetype index notes pfsense pfsense netops none pfsense_filterlog pfsense:filterlog netfw none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-pfsense_firewall.conf #File name provided is a suggestion it must be globally unique application app - vps - test - pfsense_firewall [ sc4s - vps ] { filter { \"${HOST}\" eq \"pfsense_firewall\" }; parser { p_set_netsource_fields ( vendor ( ' pfsense ' ) product ( ' firewall ' ) ); }; };","title":"Firewall"},{"location":"sources/vendor/Pfsense/firewall/#firewall","text":"","title":"Firewall"},{"location":"sources/vendor/Pfsense/firewall/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Pfsense/firewall/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1527/ Product Manual https://docs.netgate.com/pfsense/en/latest/monitoring/copying-logs-to-a-remote-host-with-syslog.html?highlight=syslog","title":"Links"},{"location":"sources/vendor/Pfsense/firewall/#sourcetypes","text":"sourcetype notes pfsense:filterlog None pfsense:* All programs other than filterlog","title":"Sourcetypes"},{"location":"sources/vendor/Pfsense/firewall/#sourcetype-and-index-configuration","text":"key sourcetype index notes pfsense pfsense netops none pfsense_filterlog pfsense:filterlog netfw none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Pfsense/firewall/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-pfsense_firewall.conf #File name provided is a suggestion it must be globally unique application app - vps - test - pfsense_firewall [ sc4s - vps ] { filter { \"${HOST}\" eq \"pfsense_firewall\" }; parser { p_set_netsource_fields ( vendor ( ' pfsense ' ) product ( ' firewall ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Polycom/rprm/","text":"RPRM \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on none Product Manual unknown Sourcetypes \u00b6 sourcetype notes polycom:rprm:syslog Sourcetype and Index Configuration \u00b6 key sourcetype index notes polycom_rprm polycom:rprm:syslog netops none","title":"RPRM"},{"location":"sources/vendor/Polycom/rprm/#rprm","text":"","title":"RPRM"},{"location":"sources/vendor/Polycom/rprm/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Polycom/rprm/#links","text":"Ref Link Splunk Add-on none Product Manual unknown","title":"Links"},{"location":"sources/vendor/Polycom/rprm/#sourcetypes","text":"sourcetype notes polycom:rprm:syslog","title":"Sourcetypes"},{"location":"sources/vendor/Polycom/rprm/#sourcetype-and-index-configuration","text":"key sourcetype index notes polycom_rprm polycom:rprm:syslog netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Proofpoint/","text":"Proofpoint Protection Server \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 NOTE: This filter will simply parse the syslog message itself, and will not perform the (required) re-assembly of related messages to create meaningful final output. This will require follow-on processing in Splunk. Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3080/ Product Manual https://proofpointcommunities.force.com/community/s/article/Remote-Syslog-Forwarding Sourcetypes \u00b6 sourcetype notes pps_filter_log pps_mail_log This sourcetype will conflict with sendmail itself, so will require that the PPS send syslog on a dedicated port or be uniquely identifiable with a hostname glob or CIDR block if this sourcetype is desired for PPS. Sourcetype and Index Configuration \u00b6 key sourcetype index notes proofpoint_pps_filter pps_filter_log email none proofpoint_pps_sendmail pps_mail_log email none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-proofpoint_pps.conf #File name provided is a suggestion it must be globally unique application app - vps - test - proofpoint_pps [ sc4s - vps ] { filter { host ( \"pps-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' proofpoint ' ) product ( ' pps ' ) ); }; };","title":"Proofpoint Protection Server"},{"location":"sources/vendor/Proofpoint/#proofpoint-protection-server","text":"","title":"Proofpoint Protection Server"},{"location":"sources/vendor/Proofpoint/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514 NOTE: This filter will simply parse the syslog message itself, and will not perform the (required) re-assembly of related messages to create meaningful final output. This will require follow-on processing in Splunk.","title":"Key facts"},{"location":"sources/vendor/Proofpoint/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3080/ Product Manual https://proofpointcommunities.force.com/community/s/article/Remote-Syslog-Forwarding","title":"Links"},{"location":"sources/vendor/Proofpoint/#sourcetypes","text":"sourcetype notes pps_filter_log pps_mail_log This sourcetype will conflict with sendmail itself, so will require that the PPS send syslog on a dedicated port or be uniquely identifiable with a hostname glob or CIDR block if this sourcetype is desired for PPS.","title":"Sourcetypes"},{"location":"sources/vendor/Proofpoint/#sourcetype-and-index-configuration","text":"key sourcetype index notes proofpoint_pps_filter pps_filter_log email none proofpoint_pps_sendmail pps_mail_log email none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Proofpoint/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-proofpoint_pps.conf #File name provided is a suggestion it must be globally unique application app - vps - test - proofpoint_pps [ sc4s - vps ] { filter { host ( \"pps-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' proofpoint ' ) product ( ' pps ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Pulse/connectsecure/","text":"Pulse \u00b6 Key facts \u00b6 Requires vendor product by source configuration IETF Frames use port 601/tcp or 6587/TLS Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3852/ JunOS TechLibrary https://docs.pulsesecure.net/WebHelp/Content/PCS/PCS_AdminGuide_8.2/Configuring%20Syslog.htm Sourcetypes \u00b6 sourcetype notes pulse:connectsecure None pulse:connectsecure:web None Sourcetype and Index Configuration \u00b6 key sourcetype index notes pulse_connect_secure pulse:connectsecure netfw none pulse_connect_secure_web pulse:connectsecure:web netproxy none","title":"Pulse"},{"location":"sources/vendor/Pulse/connectsecure/#pulse","text":"","title":"Pulse"},{"location":"sources/vendor/Pulse/connectsecure/#key-facts","text":"Requires vendor product by source configuration IETF Frames use port 601/tcp or 6587/TLS","title":"Key facts"},{"location":"sources/vendor/Pulse/connectsecure/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3852/ JunOS TechLibrary https://docs.pulsesecure.net/WebHelp/Content/PCS/PCS_AdminGuide_8.2/Configuring%20Syslog.htm","title":"Links"},{"location":"sources/vendor/Pulse/connectsecure/#sourcetypes","text":"sourcetype notes pulse:connectsecure None pulse:connectsecure:web None","title":"Sourcetypes"},{"location":"sources/vendor/Pulse/connectsecure/#sourcetype-and-index-configuration","text":"key sourcetype index notes pulse_connect_secure pulse:connectsecure netfw none pulse_connect_secure_web pulse:connectsecure:web netproxy none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/PureStorage/array/","text":"Array \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None note TA published on Splunk base does not include syslog extractions Product Manual Sourcetypes \u00b6 sourcetype notes purestorage:array purestorage:array:${class} This type is generated from the message Sourcetype and Index Configuration \u00b6 key sourcetype index notes purestorage_array purestorage:array infraops None purestorage_array_${class} purestorage:array:class infraops class is extracted as the string following \u201cpurity.\u201d","title":"Array"},{"location":"sources/vendor/PureStorage/array/#array","text":"","title":"Array"},{"location":"sources/vendor/PureStorage/array/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/PureStorage/array/#links","text":"Ref Link Splunk Add-on None note TA published on Splunk base does not include syslog extractions Product Manual","title":"Links"},{"location":"sources/vendor/PureStorage/array/#sourcetypes","text":"sourcetype notes purestorage:array purestorage:array:${class} This type is generated from the message","title":"Sourcetypes"},{"location":"sources/vendor/PureStorage/array/#sourcetype-and-index-configuration","text":"key sourcetype index notes purestorage_array purestorage:array infraops None purestorage_array_${class} purestorage:array:class infraops class is extracted as the string following \u201cpurity.\u201d","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Qumulo/storage/","text":"Storage \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on none Sourcetypes \u00b6 sourcetype notes qumulo:storage None Sourcetype and Index Configuration \u00b6 key sourcetype index notes qumulo_storage qumulo:storage infraops none","title":"Storage"},{"location":"sources/vendor/Qumulo/storage/#storage","text":"","title":"Storage"},{"location":"sources/vendor/Qumulo/storage/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Qumulo/storage/#links","text":"Ref Link Splunk Add-on none","title":"Links"},{"location":"sources/vendor/Qumulo/storage/#sourcetypes","text":"sourcetype notes qumulo:storage None","title":"Sourcetypes"},{"location":"sources/vendor/Qumulo/storage/#sourcetype-and-index-configuration","text":"key sourcetype index notes qumulo_storage qumulo:storage infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Radware/defensepro/","text":"DefensePro \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on Note this add-on does not provide functional extractions https://splunkbase.splunk.com/app/4480/ Product Manual https://www.radware.com/products/defensepro/ Sourcetypes \u00b6 sourcetype notes radware:defensepro Note some events do not contain host Sourcetype and Index Configuration \u00b6 key sourcetype index notes radware_defensepro radware:defensepro netops none","title":"DefensePro"},{"location":"sources/vendor/Radware/defensepro/#defensepro","text":"","title":"DefensePro"},{"location":"sources/vendor/Radware/defensepro/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Radware/defensepro/#links","text":"Ref Link Splunk Add-on Note this add-on does not provide functional extractions https://splunkbase.splunk.com/app/4480/ Product Manual https://www.radware.com/products/defensepro/","title":"Links"},{"location":"sources/vendor/Radware/defensepro/#sourcetypes","text":"sourcetype notes radware:defensepro Note some events do not contain host","title":"Sourcetypes"},{"location":"sources/vendor/Radware/defensepro/#sourcetype-and-index-configuration","text":"key sourcetype index notes radware_defensepro radware:defensepro netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Raritan/dsx/","text":"DSX \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on none Product Manual https://www.raritan.com/products/kvm-serial/serial-console-servers/serial-over-ip-console-server Sourcetypes \u00b6 sourcetype notes raritan:dsx Note events do not contain host Sourcetype and Index Configuration \u00b6 key sourcetype index notes raritan_dsx raritan:dsx infraops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-raritan_dsx.conf #File name provided is a suggestion it must be globally unique application app - vps - test - raritan_dsx [ sc4s - vps ] { filter { host ( \"raritan_dsx*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' raritan ' ) product ( ' dsx ' ) ); }; };","title":"DSX"},{"location":"sources/vendor/Raritan/dsx/#dsx","text":"","title":"DSX"},{"location":"sources/vendor/Raritan/dsx/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Raritan/dsx/#links","text":"Ref Link Splunk Add-on none Product Manual https://www.raritan.com/products/kvm-serial/serial-console-servers/serial-over-ip-console-server","title":"Links"},{"location":"sources/vendor/Raritan/dsx/#sourcetypes","text":"sourcetype notes raritan:dsx Note events do not contain host","title":"Sourcetypes"},{"location":"sources/vendor/Raritan/dsx/#sourcetype-and-index-configuration","text":"key sourcetype index notes raritan_dsx raritan:dsx infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Raritan/dsx/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-raritan_dsx.conf #File name provided is a suggestion it must be globally unique application app - vps - test - raritan_dsx [ sc4s - vps ] { filter { host ( \"raritan_dsx*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' raritan ' ) product ( ' dsx ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Ricoh/mfp/","text":"MFP \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes ricoh:mfp None Sourcetype and Index Configuration \u00b6 key sourcetype index notes ricoh_syslog ricoh:mfp printer none SC4S Options \u00b6 Variable default description SC4S_SOURCE_RICOH_SYSLOG_FIXHOST yes Current firmware incorrectly sends the value of HOST in the program field if this is ever corrected this value will need to be set back to no we suggest using yes","title":"MFP"},{"location":"sources/vendor/Ricoh/mfp/#mfp","text":"","title":"MFP"},{"location":"sources/vendor/Ricoh/mfp/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Ricoh/mfp/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Ricoh/mfp/#sourcetypes","text":"sourcetype notes ricoh:mfp None","title":"Sourcetypes"},{"location":"sources/vendor/Ricoh/mfp/#sourcetype-and-index-configuration","text":"key sourcetype index notes ricoh_syslog ricoh:mfp printer none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Ricoh/mfp/#sc4s-options","text":"Variable default description SC4S_SOURCE_RICOH_SYSLOG_FIXHOST yes Current firmware incorrectly sends the value of HOST in the program field if this is ever corrected this value will need to be set back to no we suggest using yes","title":"SC4S Options"},{"location":"sources/vendor/Riverbed/","text":"Syslog \u00b6 Used when more specific steelhead or steelconnect can not be identified Key facts \u00b6 MSG Format based filter RFC5424 or Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes riverbed:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes riverbed_syslog riverbed:syslog netops none riverbed_syslog_nix_syslog nix:syslog osnix none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-riverbed_syslog.conf #File name provided is a suggestion it must be globally unique application app - vps - riverbed_syslog [ sc4s - vps ] { filter { host (....) }; parser { p_set_netsource_fields ( vendor ( ' riverbed ' ) product ( ' syslog ' ) ); }; };","title":"Syslog"},{"location":"sources/vendor/Riverbed/#syslog","text":"Used when more specific steelhead or steelconnect can not be identified","title":"Syslog"},{"location":"sources/vendor/Riverbed/#key-facts","text":"MSG Format based filter RFC5424 or Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Riverbed/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Riverbed/#sourcetypes","text":"sourcetype notes riverbed:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Riverbed/#sourcetype-and-index-configuration","text":"key sourcetype index notes riverbed_syslog riverbed:syslog netops none riverbed_syslog_nix_syslog nix:syslog osnix none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Riverbed/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-riverbed_syslog.conf #File name provided is a suggestion it must be globally unique application app - vps - riverbed_syslog [ sc4s - vps ] { filter { host (....) }; parser { p_set_netsource_fields ( vendor ( ' riverbed ' ) product ( ' syslog ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Riverbed/steelconnect/","text":"Steelconnect \u00b6 Key facts \u00b6 MSG Format based filter RFC5424 or Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes riverbed:steelconnect None Sourcetype and Index Configuration \u00b6 key sourcetype index notes riverbed_syslog_steelconnect riverbed:steelconnect netops none","title":"Steelconnect"},{"location":"sources/vendor/Riverbed/steelconnect/#steelconnect","text":"","title":"Steelconnect"},{"location":"sources/vendor/Riverbed/steelconnect/#key-facts","text":"MSG Format based filter RFC5424 or Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Riverbed/steelconnect/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Riverbed/steelconnect/#sourcetypes","text":"sourcetype notes riverbed:steelconnect None","title":"Sourcetypes"},{"location":"sources/vendor/Riverbed/steelconnect/#sourcetype-and-index-configuration","text":"key sourcetype index notes riverbed_syslog_steelconnect riverbed:steelconnect netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Riverbed/steelhead/","text":"SteelHead \u00b6 Key facts \u00b6 Partial MSG Format based filter RFC5424 or Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes riverbed:steelhead None Sourcetype and Index Configuration \u00b6 key sourcetype index notes riverbed_syslog_steelhead riverbed:steelhead netops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-riverbed_syslog.conf #File name provided is a suggestion it must be globally unique application app - vps - riverbed_syslog [ sc4s - vps ] { filter { host (....) }; parser { p_set_netsource_fields ( vendor ( ' riverbed ' ) product ( ' syslog ' ) ); }; };","title":"SteelHead"},{"location":"sources/vendor/Riverbed/steelhead/#steelhead","text":"","title":"SteelHead"},{"location":"sources/vendor/Riverbed/steelhead/#key-facts","text":"Partial MSG Format based filter RFC5424 or Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Riverbed/steelhead/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Riverbed/steelhead/#sourcetypes","text":"sourcetype notes riverbed:steelhead None","title":"Sourcetypes"},{"location":"sources/vendor/Riverbed/steelhead/#sourcetype-and-index-configuration","text":"key sourcetype index notes riverbed_syslog_steelhead riverbed:steelhead netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Riverbed/steelhead/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-riverbed_syslog.conf #File name provided is a suggestion it must be globally unique application app - vps - riverbed_syslog [ sc4s - vps ] { filter { host (....) }; parser { p_set_netsource_fields ( vendor ( ' riverbed ' ) product ( ' syslog ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Ruckus/SmartZone/","text":"Smart Zone \u00b6 Some events may not match the source format please report issues if found Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes ruckus:smartzone None Sourcetype and Index Configuration \u00b6 key sourcetype index notes ruckus_smartzone ruckus:smartzone netops none","title":"Smart Zone"},{"location":"sources/vendor/Ruckus/SmartZone/#smart-zone","text":"Some events may not match the source format please report issues if found","title":"Smart Zone"},{"location":"sources/vendor/Ruckus/SmartZone/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Ruckus/SmartZone/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Ruckus/SmartZone/#sourcetypes","text":"sourcetype notes ruckus:smartzone None","title":"Sourcetypes"},{"location":"sources/vendor/Ruckus/SmartZone/#sourcetype-and-index-configuration","text":"key sourcetype index notes ruckus_smartzone ruckus:smartzone netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Schneider/apc/","text":"APC Power systems \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on none Product Manual multiple Sourcetypes \u00b6 sourcetype notes apc:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes schneider_apc apc:syslog main none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-schneider_apc.conf #File name provided is a suggestion it must be globally unique application app - vps - test - schneider_apc [ sc4s - vps ] { filter { host ( \"test_apc-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' schneider ' ) product ( ' apc ' ) ); }; };","title":"APC Power systems"},{"location":"sources/vendor/Schneider/apc/#apc-power-systems","text":"","title":"APC Power systems"},{"location":"sources/vendor/Schneider/apc/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Schneider/apc/#links","text":"Ref Link Splunk Add-on none Product Manual multiple","title":"Links"},{"location":"sources/vendor/Schneider/apc/#sourcetypes","text":"sourcetype notes apc:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Schneider/apc/#sourcetype-and-index-configuration","text":"key sourcetype index notes schneider_apc apc:syslog main none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Schneider/apc/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-schneider_apc.conf #File name provided is a suggestion it must be globally unique application app - vps - test - schneider_apc [ sc4s - vps ] { filter { host ( \"test_apc-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' schneider ' ) product ( ' apc ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Solace/evenbroker/","text":"EventBroker \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes solace:eventbroker None Sourcetype and Index Configuration \u00b6 key sourcetype index notes solace_eventbroker solace:eventbroker main none","title":"EventBroker"},{"location":"sources/vendor/Solace/evenbroker/#eventbroker","text":"","title":"EventBroker"},{"location":"sources/vendor/Solace/evenbroker/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Solace/evenbroker/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Solace/evenbroker/#sourcetypes","text":"sourcetype notes solace:eventbroker None","title":"Sourcetypes"},{"location":"sources/vendor/Solace/evenbroker/#sourcetype-and-index-configuration","text":"key sourcetype index notes solace_eventbroker solace:eventbroker main none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Sophos/Firewall/","text":"Web Appliance \u00b6 Key facts \u00b6 Community requested filter Default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/6187/ Product Manual unknown Sourcetypes \u00b6 sourcetype notes sophos:xg:atp None sophos:xg:anti_spam None sophos:xg:anti_virus None sophos:xg:content_filtering None sophos:xg:event None sophos:xg:firewall None sophos:xg:ssl None sophos:xg:sandbox None sophos:xg:system_health None sophos:xg:heartbeat None sophos:xg:waf None sophos:xg:wireless_protection None sophos:xg:idp None Sourcetype and Index Configuration \u00b6 key sourcetype index notes sophos_xg_atp sophos:xg:atp netdlp none sophos_xg_anti_spam sophos:xg:anti_spam netdlp none sophos_xg_anti_virus sophos:xg:anti_virus netdlp none sophos_xg_content_filtering sophos:xg:content_filtering netdlp none sophos_xg_event sophos:xg:event netdlp none sophos_xg_firewall sophos:xg:firewall netdlp none sophos_xg_ssl sophos:xg:ssl netdlp none sophos_xg_sandbox sophos:xg:sandbox netdlp none sophos_xg_system_health sophos:xg:system_health netdlp none sophos_xg_heartbeat sophos:xg:heartbeat netdlp none sophos_xg_waf sophos:xg:waf netdlp none sophos_xg_wireless_protection sophos:xg:wireless_protection netdlp none sophos_xg_idp sophos:xg:idp netdlp none","title":"Web Appliance"},{"location":"sources/vendor/Sophos/Firewall/#web-appliance","text":"","title":"Web Appliance"},{"location":"sources/vendor/Sophos/Firewall/#key-facts","text":"Community requested filter Default port 514","title":"Key facts"},{"location":"sources/vendor/Sophos/Firewall/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/6187/ Product Manual unknown","title":"Links"},{"location":"sources/vendor/Sophos/Firewall/#sourcetypes","text":"sourcetype notes sophos:xg:atp None sophos:xg:anti_spam None sophos:xg:anti_virus None sophos:xg:content_filtering None sophos:xg:event None sophos:xg:firewall None sophos:xg:ssl None sophos:xg:sandbox None sophos:xg:system_health None sophos:xg:heartbeat None sophos:xg:waf None sophos:xg:wireless_protection None sophos:xg:idp None","title":"Sourcetypes"},{"location":"sources/vendor/Sophos/Firewall/#sourcetype-and-index-configuration","text":"key sourcetype index notes sophos_xg_atp sophos:xg:atp netdlp none sophos_xg_anti_spam sophos:xg:anti_spam netdlp none sophos_xg_anti_virus sophos:xg:anti_virus netdlp none sophos_xg_content_filtering sophos:xg:content_filtering netdlp none sophos_xg_event sophos:xg:event netdlp none sophos_xg_firewall sophos:xg:firewall netdlp none sophos_xg_ssl sophos:xg:ssl netdlp none sophos_xg_sandbox sophos:xg:sandbox netdlp none sophos_xg_system_health sophos:xg:system_health netdlp none sophos_xg_heartbeat sophos:xg:heartbeat netdlp none sophos_xg_waf sophos:xg:waf netdlp none sophos_xg_wireless_protection sophos:xg:wireless_protection netdlp none sophos_xg_idp sophos:xg:idp netdlp none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Sophos/webappliance/","text":"Web Appliance \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes sophos:webappliance None Sourcetype and Index Configuration \u00b6 key sourcetype index notes sophos_webappliance sophos:webappliance netproxy none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-sophos_webappliance.conf #File name provided is a suggestion it must be globally unique application app - vps - test - sophos_webappliance [ sc4s - vps ] { filter { host ( \"test-sophos-webapp-\" type ( string ) flags ( prefix )) }; parser { p_set_netsource_fields ( vendor ( ' sophos ' ) product ( ' webappliance ' ) ); }; };","title":"Web Appliance"},{"location":"sources/vendor/Sophos/webappliance/#web-appliance","text":"","title":"Web Appliance"},{"location":"sources/vendor/Sophos/webappliance/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Sophos/webappliance/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Sophos/webappliance/#sourcetypes","text":"sourcetype notes sophos:webappliance None","title":"Sourcetypes"},{"location":"sources/vendor/Sophos/webappliance/#sourcetype-and-index-configuration","text":"key sourcetype index notes sophos_webappliance sophos:webappliance netproxy none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Sophos/webappliance/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-sophos_webappliance.conf #File name provided is a suggestion it must be globally unique application app - vps - test - sophos_webappliance [ sc4s - vps ] { filter { host ( \"test-sophos-webapp-\" type ( string ) flags ( prefix )) }; parser { p_set_netsource_fields ( vendor ( ' sophos ' ) product ( ' webappliance ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Spectracom/","text":"NTP Appliance \u00b6 Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes spectracom:ntp None nix:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes spectracom_ntp spectracom:ntp netops none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-spectracom_ntp.conf #File name provided is a suggestion it must be globally unique application app - vps - test - spectracom_ntp [ sc4s - vps ] { filter { netmask ( 169.254.100.1 / 24 ) }; parser { p_set_netsource_fields ( vendor ( ' spectracom ' ) product ( ' ntp ' ) ); }; };","title":"NTP Appliance"},{"location":"sources/vendor/Spectracom/#ntp-appliance","text":"","title":"NTP Appliance"},{"location":"sources/vendor/Spectracom/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Spectracom/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Spectracom/#sourcetypes","text":"sourcetype notes spectracom:ntp None nix:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Spectracom/#sourcetype-and-index-configuration","text":"key sourcetype index notes spectracom_ntp spectracom:ntp netops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Spectracom/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-spectracom_ntp.conf #File name provided is a suggestion it must be globally unique application app - vps - test - spectracom_ntp [ sc4s - vps ] { filter { netmask ( 169.254.100.1 / 24 ) }; parser { p_set_netsource_fields ( vendor ( ' spectracom ' ) product ( ' ntp ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/Splunk/heavyforwarder/","text":"Splunk Heavy Forwarder \u00b6 In certain network architectures such as those using data diodes or those networks requiring \u201cin the clear\u201d inspection at network egress SC4S can be used to accept specially formatted output from Splunk as RFC5424 syslog. Key facts \u00b6 RFC 5424 using port 601 (Framed) Links \u00b6 Ref Link Splunk Add-on None Product Manual unknown Sourcetypes \u00b6 sourcetype notes spectracom:ntp None nix:syslog None Sourcetype and Index Configuration \u00b6 Index Source and Sourcetype will be used as determined by the Source/HWF Splunk Configuration \u00b6 Splunk MUST have props and transforms applied (Typically via add-ons) This configuration will consume all output presuming no S2S is allowed no Splunk destination will be used outputs.conf \u00b6 #Because audit trail is protected and we can't transform it we can not use default we must use tcp_routing [tcpout] defaultGroup = NoForwarding [tcpout:nexthop] server = localhost:9000 sendCookedData = false props.conf \u00b6 [default] ADD_EXTRA_TIME_FIELDS = none ANNOTATE_PUNCT = false SHOULD_LINEMERGE = false TRANSFORMS-zza-syslog = syslog_canforward, metadata_meta, metadata_source, metadata_sourcetype, metadata_index, metadata_host, metadata_subsecond, metadata_time, syslog_prefix, syslog_drop_zero # The following applies for TCP destinations where the IETF frame is required TRANSFORMS-zzz-syslog = syslog_octal, syslog_octal_append # Comment out the above and uncomment the following for udp #TRANSFORMS-zzz-syslog-udp = syslog_octal, syslog_octal_append, syslog_drop_zero [audittrail] # We can't transform this source type its protected TRANSFORMS-zza-syslog = TRANSFORMS-zzz-syslog = transforms.conf \u00b6 syslog_canforward] REGEX = ^.(?!audit) DEST_KEY = _TCP_ROUTING FORMAT = nexthop [metadata_meta] SOURCE_KEY = _meta REGEX = (?ims)(.*) FORMAT = ~~~SM~~~$1~~~EM~~~$0 DEST_KEY = _raw [metadata_source] SOURCE_KEY = MetaData:Source REGEX = ^source::(.*)$ FORMAT = s=\"$1\"] $0 DEST_KEY = _raw [metadata_sourcetype] SOURCE_KEY = MetaData:Sourcetype REGEX = ^sourcetype::(.*)$ FORMAT = st=\"$1\" $0 DEST_KEY = _raw [metadata_index] SOURCE_KEY = _MetaData:Index REGEX = (.*) FORMAT = i=\"$1\" $0 DEST_KEY = _raw [metadata_host] SOURCE_KEY = MetaData:Host REGEX = ^host::(.*)$ FORMAT = \" h=\"$1\" $0 DEST_KEY = _raw [syslog_prefix] SOURCE_KEY = _time REGEX = (.*) FORMAT = <1>1 - - SPLUNK - COOKED [fields@274489 $0 DEST_KEY = _raw [metadata_time] SOURCE_KEY = _time REGEX = (.*) FORMAT = t=\"$1$0 DEST_KEY = _raw [metadata_subsecond] SOURCE_KEY = _meta REGEX = \\_subsecond\\:\\:(\\.\\d+) FORMAT = $1 $0 DEST_KEY = _raw [syslog_octal] INGEST_EVAL = mlen=length(_raw)+1 [syslog_octal_append] INGEST_EVAL = _raw=mlen + \" \" + _raw [syslog_drop_zero] INGEST_EVAL = queue=if(mlen<10,\"nullQueue\",queue)","title":"Splunk Heavy Forwarder"},{"location":"sources/vendor/Splunk/heavyforwarder/#splunk-heavy-forwarder","text":"In certain network architectures such as those using data diodes or those networks requiring \u201cin the clear\u201d inspection at network egress SC4S can be used to accept specially formatted output from Splunk as RFC5424 syslog.","title":"Splunk Heavy Forwarder"},{"location":"sources/vendor/Splunk/heavyforwarder/#key-facts","text":"RFC 5424 using port 601 (Framed)","title":"Key facts"},{"location":"sources/vendor/Splunk/heavyforwarder/#links","text":"Ref Link Splunk Add-on None Product Manual unknown","title":"Links"},{"location":"sources/vendor/Splunk/heavyforwarder/#sourcetypes","text":"sourcetype notes spectracom:ntp None nix:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Splunk/heavyforwarder/#sourcetype-and-index-configuration","text":"Index Source and Sourcetype will be used as determined by the Source/HWF","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Splunk/heavyforwarder/#splunk-configuration","text":"Splunk MUST have props and transforms applied (Typically via add-ons) This configuration will consume all output presuming no S2S is allowed no Splunk destination will be used","title":"Splunk Configuration"},{"location":"sources/vendor/Splunk/heavyforwarder/#outputsconf","text":"#Because audit trail is protected and we can't transform it we can not use default we must use tcp_routing [tcpout] defaultGroup = NoForwarding [tcpout:nexthop] server = localhost:9000 sendCookedData = false","title":"outputs.conf"},{"location":"sources/vendor/Splunk/heavyforwarder/#propsconf","text":"[default] ADD_EXTRA_TIME_FIELDS = none ANNOTATE_PUNCT = false SHOULD_LINEMERGE = false TRANSFORMS-zza-syslog = syslog_canforward, metadata_meta, metadata_source, metadata_sourcetype, metadata_index, metadata_host, metadata_subsecond, metadata_time, syslog_prefix, syslog_drop_zero # The following applies for TCP destinations where the IETF frame is required TRANSFORMS-zzz-syslog = syslog_octal, syslog_octal_append # Comment out the above and uncomment the following for udp #TRANSFORMS-zzz-syslog-udp = syslog_octal, syslog_octal_append, syslog_drop_zero [audittrail] # We can't transform this source type its protected TRANSFORMS-zza-syslog = TRANSFORMS-zzz-syslog =","title":"props.conf"},{"location":"sources/vendor/Splunk/heavyforwarder/#transformsconf","text":"syslog_canforward] REGEX = ^.(?!audit) DEST_KEY = _TCP_ROUTING FORMAT = nexthop [metadata_meta] SOURCE_KEY = _meta REGEX = (?ims)(.*) FORMAT = ~~~SM~~~$1~~~EM~~~$0 DEST_KEY = _raw [metadata_source] SOURCE_KEY = MetaData:Source REGEX = ^source::(.*)$ FORMAT = s=\"$1\"] $0 DEST_KEY = _raw [metadata_sourcetype] SOURCE_KEY = MetaData:Sourcetype REGEX = ^sourcetype::(.*)$ FORMAT = st=\"$1\" $0 DEST_KEY = _raw [metadata_index] SOURCE_KEY = _MetaData:Index REGEX = (.*) FORMAT = i=\"$1\" $0 DEST_KEY = _raw [metadata_host] SOURCE_KEY = MetaData:Host REGEX = ^host::(.*)$ FORMAT = \" h=\"$1\" $0 DEST_KEY = _raw [syslog_prefix] SOURCE_KEY = _time REGEX = (.*) FORMAT = <1>1 - - SPLUNK - COOKED [fields@274489 $0 DEST_KEY = _raw [metadata_time] SOURCE_KEY = _time REGEX = (.*) FORMAT = t=\"$1$0 DEST_KEY = _raw [metadata_subsecond] SOURCE_KEY = _meta REGEX = \\_subsecond\\:\\:(\\.\\d+) FORMAT = $1 $0 DEST_KEY = _raw [syslog_octal] INGEST_EVAL = mlen=length(_raw)+1 [syslog_octal_append] INGEST_EVAL = _raw=mlen + \" \" + _raw [syslog_drop_zero] INGEST_EVAL = queue=if(mlen<10,\"nullQueue\",queue)","title":"transforms.conf"},{"location":"sources/vendor/Splunk/sc4s/","text":"Splunk Connect for Syslog (SC4S) \u00b6 Key facts \u00b6 Internal events Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4740/ Product Manual https://splunk-connect-for-syslog.readthedocs.io/en/latest/ Sourcetypes \u00b6 sourcetype notes sc4s:events Internal events from the SC4S container and underlying syslog-ng process sc4s:metrics syslog-ng operational metrics that will be delivered directly to a metrics index in Splunk Sourcetype and Index Configuration \u00b6 key sourcetype index notes splunk_sc4s_events all main none splunk_sc4s_metrics all em_metrics none splunk_sc4s_fallback all main none Filter type \u00b6 SC4S events and metrics are generated automatically and no specific ports or filters need to be configured for the collection of this data. Setup and Configuration \u00b6 The default index used for sc4s metrics will be \u201c_metrics\u201d Metrics data is collected by default as traditional events; use of Splunk Metrics is enabled by an opt-in set by the variable SC4S_DEST_SPLUNK_SC4S_METRICS_HEC . See the \u201cOptions\u201d section below for details. Options \u00b6 Variable default description SC4S_DEST_SPLUNK_SC4S_METRICS_HEC multi2 event produce metrics as plain text events; single produce metrics using Splunk Enterprise 7.3 single metrics format; multi produce metrics using Splunk Enterprise >8.1 multi metric format multi2 produces improved (reduced resource consumption) multi metric format SC4S_SOURCE_MARK_MESSAGE_NULLQUEUE yes (yes Verification \u00b6 SC4S will generate versioning events at startup. These startup events can be used to validate HEC is set up properly on the Splunk side. index=<asconfigured> sourcetype=sc4s:events | stats count by host Metrics can be observed via the \u201cAnalytics\u2013>Metrics\u201d navigation in the Search and Reporting app in Splunk. NOTE: The presentation of metrics is undergoing active development; the delivery of metrics is currently considered an experimental feature.","title":"Splunk Connect for Syslog (SC4S)"},{"location":"sources/vendor/Splunk/sc4s/#splunk-connect-for-syslog-sc4s","text":"","title":"Splunk Connect for Syslog (SC4S)"},{"location":"sources/vendor/Splunk/sc4s/#key-facts","text":"Internal events","title":"Key facts"},{"location":"sources/vendor/Splunk/sc4s/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4740/ Product Manual https://splunk-connect-for-syslog.readthedocs.io/en/latest/","title":"Links"},{"location":"sources/vendor/Splunk/sc4s/#sourcetypes","text":"sourcetype notes sc4s:events Internal events from the SC4S container and underlying syslog-ng process sc4s:metrics syslog-ng operational metrics that will be delivered directly to a metrics index in Splunk","title":"Sourcetypes"},{"location":"sources/vendor/Splunk/sc4s/#sourcetype-and-index-configuration","text":"key sourcetype index notes splunk_sc4s_events all main none splunk_sc4s_metrics all em_metrics none splunk_sc4s_fallback all main none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Splunk/sc4s/#filter-type","text":"SC4S events and metrics are generated automatically and no specific ports or filters need to be configured for the collection of this data.","title":"Filter type"},{"location":"sources/vendor/Splunk/sc4s/#setup-and-configuration","text":"The default index used for sc4s metrics will be \u201c_metrics\u201d Metrics data is collected by default as traditional events; use of Splunk Metrics is enabled by an opt-in set by the variable SC4S_DEST_SPLUNK_SC4S_METRICS_HEC . See the \u201cOptions\u201d section below for details.","title":"Setup and Configuration"},{"location":"sources/vendor/Splunk/sc4s/#options","text":"Variable default description SC4S_DEST_SPLUNK_SC4S_METRICS_HEC multi2 event produce metrics as plain text events; single produce metrics using Splunk Enterprise 7.3 single metrics format; multi produce metrics using Splunk Enterprise >8.1 multi metric format multi2 produces improved (reduced resource consumption) multi metric format SC4S_SOURCE_MARK_MESSAGE_NULLQUEUE yes (yes","title":"Options"},{"location":"sources/vendor/Splunk/sc4s/#verification","text":"SC4S will generate versioning events at startup. These startup events can be used to validate HEC is set up properly on the Splunk side. index=<asconfigured> sourcetype=sc4s:events | stats count by host Metrics can be observed via the \u201cAnalytics\u2013>Metrics\u201d navigation in the Search and Reporting app in Splunk. NOTE: The presentation of metrics is undergoing active development; the delivery of metrics is currently considered an experimental feature.","title":"Verification"},{"location":"sources/vendor/StealthWatch/StealthIntercept/","text":"Stealth Intercept \u00b6 Key facts \u00b6 Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4609/ Product Manual unknown Sourcetypes \u00b6 sourcetype notes StealthINTERCEPT None StealthINTERCEPT:alerts SC4S Format Shifts to JSON override template to t_msg_hdr for original raw Sourcetype and Index Configuration \u00b6 key sourcetype index notes stealthbits_stealthintercept StealthINTERCEPT netids none stealthbits_stealthintercept_alerts StealthINTERCEPT:alerts netids Note TA does not support this source type","title":"Stealth Intercept"},{"location":"sources/vendor/StealthWatch/StealthIntercept/#stealth-intercept","text":"","title":"Stealth Intercept"},{"location":"sources/vendor/StealthWatch/StealthIntercept/#key-facts","text":"Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/StealthWatch/StealthIntercept/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4609/ Product Manual unknown","title":"Links"},{"location":"sources/vendor/StealthWatch/StealthIntercept/#sourcetypes","text":"sourcetype notes StealthINTERCEPT None StealthINTERCEPT:alerts SC4S Format Shifts to JSON override template to t_msg_hdr for original raw","title":"Sourcetypes"},{"location":"sources/vendor/StealthWatch/StealthIntercept/#sourcetype-and-index-configuration","text":"key sourcetype index notes stealthbits_stealthintercept StealthINTERCEPT netids none stealthbits_stealthintercept_alerts StealthINTERCEPT:alerts netids Note TA does not support this source type","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Tanium/platform/","text":"Platform \u00b6 This source requires a TLS connection; in most cases enabling TLS and using the default port 6514 is adequate. The source is understood to require a valid certificate. Key facts \u00b6 MSG Format based filter Requires TLS and uses IETF Frames use port 6587 after TLS Configuration Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4439/ Sourcetypes \u00b6 sourcetype notes tanium none Index Configuration \u00b6 key index notes tanium_syslog epintel none","title":"Platform"},{"location":"sources/vendor/Tanium/platform/#platform","text":"This source requires a TLS connection; in most cases enabling TLS and using the default port 6514 is adequate. The source is understood to require a valid certificate.","title":"Platform"},{"location":"sources/vendor/Tanium/platform/#key-facts","text":"MSG Format based filter Requires TLS and uses IETF Frames use port 6587 after TLS Configuration","title":"Key facts"},{"location":"sources/vendor/Tanium/platform/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4439/","title":"Links"},{"location":"sources/vendor/Tanium/platform/#sourcetypes","text":"sourcetype notes tanium none","title":"Sourcetypes"},{"location":"sources/vendor/Tanium/platform/#index-configuration","text":"key index notes tanium_syslog epintel none","title":"Index Configuration"},{"location":"sources/vendor/Tenable/ad/","text":"ad \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4060/ Product Manual Sourcetypes \u00b6 sourcetype notes tenable:ad:alerts None Sourcetype and Index Configuration \u00b6 key sourcetype index notes tenable_ad tenable:ad:alerts oswinsec none","title":"ad"},{"location":"sources/vendor/Tenable/ad/#ad","text":"","title":"ad"},{"location":"sources/vendor/Tenable/ad/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Tenable/ad/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4060/ Product Manual","title":"Links"},{"location":"sources/vendor/Tenable/ad/#sourcetypes","text":"sourcetype notes tenable:ad:alerts None","title":"Sourcetypes"},{"location":"sources/vendor/Tenable/ad/#sourcetype-and-index-configuration","text":"key sourcetype index notes tenable_ad tenable:ad:alerts oswinsec none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Tenable/nnm/","text":"nnm \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4060/ Product Manual https://docs.tenable.com/integrations/Splunk/Content/Splunk2/ProcessWorkflow.htm Sourcetypes \u00b6 sourcetype notes tenable:nnm:vuln None Sourcetype and Index Configuration \u00b6 key sourcetype index notes tenable_nnm tenable:nnm:vuln netfw none","title":"nnm"},{"location":"sources/vendor/Tenable/nnm/#nnm","text":"","title":"nnm"},{"location":"sources/vendor/Tenable/nnm/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Tenable/nnm/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4060/ Product Manual https://docs.tenable.com/integrations/Splunk/Content/Splunk2/ProcessWorkflow.htm","title":"Links"},{"location":"sources/vendor/Tenable/nnm/#sourcetypes","text":"sourcetype notes tenable:nnm:vuln None","title":"Sourcetypes"},{"location":"sources/vendor/Tenable/nnm/#sourcetype-and-index-configuration","text":"key sourcetype index notes tenable_nnm tenable:nnm:vuln netfw none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Thycotic/secretserver/","text":"Secret Server \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4060/ Product Manual Sourcetypes \u00b6 sourcetype notes thycotic:syslog None Sourcetype and Index Configuration \u00b6 key sourcetype index notes Thycotic Software_Secret Server thycotic:syslog netauth none","title":"Secret Server"},{"location":"sources/vendor/Thycotic/secretserver/#secret-server","text":"","title":"Secret Server"},{"location":"sources/vendor/Thycotic/secretserver/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Thycotic/secretserver/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4060/ Product Manual","title":"Links"},{"location":"sources/vendor/Thycotic/secretserver/#sourcetypes","text":"sourcetype notes thycotic:syslog None","title":"Sourcetypes"},{"location":"sources/vendor/Thycotic/secretserver/#sourcetype-and-index-configuration","text":"key sourcetype index notes Thycotic Software_Secret Server thycotic:syslog netauth none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Tintri/syslog/","text":"Syslog \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Sourcetypes \u00b6 sourcetype notes tintri none Index Configuration \u00b6 key index notes tintri_syslog infraops none","title":"Syslog"},{"location":"sources/vendor/Tintri/syslog/#syslog","text":"","title":"Syslog"},{"location":"sources/vendor/Tintri/syslog/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Tintri/syslog/#links","text":"Ref Link Splunk Add-on None","title":"Links"},{"location":"sources/vendor/Tintri/syslog/#sourcetypes","text":"sourcetype notes tintri none","title":"Sourcetypes"},{"location":"sources/vendor/Tintri/syslog/#index-configuration","text":"key index notes tintri_syslog infraops none","title":"Index Configuration"},{"location":"sources/vendor/Trend/deepsecurity/","text":"Deep Security \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on CEF https://splunkbase.splunk.com/app/1936/ Sourcetypes \u00b6 sourcetype notes deepsecurity-system_events deepsecurity-intrusion_prevention deepsecurity-integrity_monitoring deepsecurity-log_inspection deepsecurity-web_reputation deepsecurity-firewall deepsecurity-antimalware deepsecurity-app_control Index Configuration \u00b6 key sourcetype index notes Trend Micro_Deep Security Agent deepsecurity epintel Used only if a correct source type is not matched Trend Micro_Deep Security Agent_intrusion prevention deepsecurity-intrusion_prevention epintel Trend Micro_Deep Security Agent_integrity monitoring deepsecurity-integrity_monitoring epintel Trend Micro_Deep Security Agent_log inspection deepsecurity-log_inspection epintel Trend Micro_Deep Security Agent_web reputation deepsecurity-web_reputation epintel Trend Micro_Deep Security Agent_firewall deepsecurity-firewall epintel Trend Micro_Deep Security Agent_antimalware deepsecurity-antimalware epintel Trend Micro_Deep Security Agent_app control deepsecurity-app_control epintel Trend Micro_Deep Security Manager deepsecurity-system_events epintel","title":"Deep Security"},{"location":"sources/vendor/Trend/deepsecurity/#deep-security","text":"","title":"Deep Security"},{"location":"sources/vendor/Trend/deepsecurity/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Trend/deepsecurity/#links","text":"Ref Link Splunk Add-on CEF https://splunkbase.splunk.com/app/1936/","title":"Links"},{"location":"sources/vendor/Trend/deepsecurity/#sourcetypes","text":"sourcetype notes deepsecurity-system_events deepsecurity-intrusion_prevention deepsecurity-integrity_monitoring deepsecurity-log_inspection deepsecurity-web_reputation deepsecurity-firewall deepsecurity-antimalware deepsecurity-app_control","title":"Sourcetypes"},{"location":"sources/vendor/Trend/deepsecurity/#index-configuration","text":"key sourcetype index notes Trend Micro_Deep Security Agent deepsecurity epintel Used only if a correct source type is not matched Trend Micro_Deep Security Agent_intrusion prevention deepsecurity-intrusion_prevention epintel Trend Micro_Deep Security Agent_integrity monitoring deepsecurity-integrity_monitoring epintel Trend Micro_Deep Security Agent_log inspection deepsecurity-log_inspection epintel Trend Micro_Deep Security Agent_web reputation deepsecurity-web_reputation epintel Trend Micro_Deep Security Agent_firewall deepsecurity-firewall epintel Trend Micro_Deep Security Agent_antimalware deepsecurity-antimalware epintel Trend Micro_Deep Security Agent_app control deepsecurity-app_control epintel Trend Micro_Deep Security Manager deepsecurity-system_events epintel","title":"Index Configuration"},{"location":"sources/vendor/Ubiquiti/unifi/","text":"Unifi \u00b6 All Ubiquity Unfi firewalls, switches, and access points share a common syslog configuration via the NMS. Login to NMS Navigate to settings Navigate to Site Enable Remote syslog server Enter hostname and port Key facts \u00b6 Requires vendor product by source configuration Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4107/ Product Manual https://https://help.ubnt.com/ Sourcetypes \u00b6 sourcetype notes ubnt Used when no sub source type is required by add on ubnt:fw USG events ubnt:threat USG IDS events ubnt:switch Unifi Switches ubnt:wireless Access Point logs Sourcetype and Index Configuration \u00b6 key sourcetype index notes ubiquiti_unifi ubnt netops none ubiquiti_unifi_fw ubnt:fw netfw none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-ubiquiti_unifi_fw.conf #File name provided is a suggestion it must be globally unique application app - vps - test - ubiquiti_unifi_fw [ sc4s - vps ] { filter { host ( \"usg-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' ubiquiti ' ) product ( ' unifi ' ) ); }; };","title":"Unifi"},{"location":"sources/vendor/Ubiquiti/unifi/#unifi","text":"All Ubiquity Unfi firewalls, switches, and access points share a common syslog configuration via the NMS. Login to NMS Navigate to settings Navigate to Site Enable Remote syslog server Enter hostname and port","title":"Unifi"},{"location":"sources/vendor/Ubiquiti/unifi/#key-facts","text":"Requires vendor product by source configuration Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Ubiquiti/unifi/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4107/ Product Manual https://https://help.ubnt.com/","title":"Links"},{"location":"sources/vendor/Ubiquiti/unifi/#sourcetypes","text":"sourcetype notes ubnt Used when no sub source type is required by add on ubnt:fw USG events ubnt:threat USG IDS events ubnt:switch Unifi Switches ubnt:wireless Access Point logs","title":"Sourcetypes"},{"location":"sources/vendor/Ubiquiti/unifi/#sourcetype-and-index-configuration","text":"key sourcetype index notes ubiquiti_unifi ubnt netops none ubiquiti_unifi_fw ubnt:fw netfw none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Ubiquiti/unifi/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-ubiquiti_unifi_fw.conf #File name provided is a suggestion it must be globally unique application app - vps - test - ubiquiti_unifi_fw [ sc4s - vps ] { filter { host ( \"usg-*\" type ( glob )) }; parser { p_set_netsource_fields ( vendor ( ' ubiquiti ' ) product ( ' unifi ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/VMWare/airwatch/","text":"Airwatch \u00b6 AirWatch is a product used for enterprise mobility management (EMM) software and standalone management systems for content, applications and email. Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Product Manual https://docs.vmware.com/en/VMware-Workspace-ONE/index.html Sourcetypes \u00b6 sourcetype notes vmware:airwatch None Index Configuration \u00b6 key index notes vmware_airwatch epintel none","title":"Airwatch"},{"location":"sources/vendor/VMWare/airwatch/#airwatch","text":"AirWatch is a product used for enterprise mobility management (EMM) software and standalone management systems for content, applications and email.","title":"Airwatch"},{"location":"sources/vendor/VMWare/airwatch/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/VMWare/airwatch/#links","text":"Ref Link Product Manual https://docs.vmware.com/en/VMware-Workspace-ONE/index.html","title":"Links"},{"location":"sources/vendor/VMWare/airwatch/#sourcetypes","text":"sourcetype notes vmware:airwatch None","title":"Sourcetypes"},{"location":"sources/vendor/VMWare/airwatch/#index-configuration","text":"key index notes vmware_airwatch epintel none","title":"Index Configuration"},{"location":"sources/vendor/VMWare/carbonblack/","text":"Carbon Black Protection \u00b6 RFC 5424 Format \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on none Sourcetypes \u00b6 sourcetype notes vmware:cb:protect Common sourcetype Source \u00b6 source notes carbonblack:protection:cef Note this method of onboarding is not recommended for a more complete experience utilize the json format supported by he product with hec or s3 Index Configuration \u00b6 key source index notes vmware_cb-protect carbonblack:protection:cef epintel none Legacy CEF Format \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on none Sourcetypes \u00b6 sourcetype notes cef Common sourcetype Source \u00b6 source notes carbonblack:protection:cef Note this method of onboarding is not recommended for a more complete experience utilize the json format supported by he product with hec or s3 Index Configuration \u00b6 key source index notes Carbon Black_Protection carbonblack:protection:cef epintel none","title":"Carbon Black Protection"},{"location":"sources/vendor/VMWare/carbonblack/#carbon-black-protection","text":"","title":"Carbon Black Protection"},{"location":"sources/vendor/VMWare/carbonblack/#rfc-5424-format","text":"","title":"RFC 5424 Format"},{"location":"sources/vendor/VMWare/carbonblack/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/VMWare/carbonblack/#links","text":"Ref Link Splunk Add-on none","title":"Links"},{"location":"sources/vendor/VMWare/carbonblack/#sourcetypes","text":"sourcetype notes vmware:cb:protect Common sourcetype","title":"Sourcetypes"},{"location":"sources/vendor/VMWare/carbonblack/#source","text":"source notes carbonblack:protection:cef Note this method of onboarding is not recommended for a more complete experience utilize the json format supported by he product with hec or s3","title":"Source"},{"location":"sources/vendor/VMWare/carbonblack/#index-configuration","text":"key source index notes vmware_cb-protect carbonblack:protection:cef epintel none","title":"Index Configuration"},{"location":"sources/vendor/VMWare/carbonblack/#legacy-cef-format","text":"","title":"Legacy CEF Format"},{"location":"sources/vendor/VMWare/carbonblack/#key-facts_1","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/VMWare/carbonblack/#links_1","text":"Ref Link Splunk Add-on none","title":"Links"},{"location":"sources/vendor/VMWare/carbonblack/#sourcetypes_1","text":"sourcetype notes cef Common sourcetype","title":"Sourcetypes"},{"location":"sources/vendor/VMWare/carbonblack/#source_1","text":"source notes carbonblack:protection:cef Note this method of onboarding is not recommended for a more complete experience utilize the json format supported by he product with hec or s3","title":"Source"},{"location":"sources/vendor/VMWare/carbonblack/#index-configuration_1","text":"key source index notes Carbon Black_Protection carbonblack:protection:cef epintel none","title":"Index Configuration"},{"location":"sources/vendor/VMWare/horizonview/","text":"Horizon View \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on None Manual unknown Sourcetypes \u00b6 sourcetype notes vmware:horizon None nix:syslog When used with a default port this will follow the generic NIX configuration when using a dedicated port, IP or host rules events will follow the index configuration for vmware nsx Sourcetype and Index Configuration \u00b6 key sourcetype index notes vmware_horizon vmware:horizon main none","title":"Horizon View"},{"location":"sources/vendor/VMWare/horizonview/#horizon-view","text":"","title":"Horizon View"},{"location":"sources/vendor/VMWare/horizonview/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/VMWare/horizonview/#links","text":"Ref Link Splunk Add-on None Manual unknown","title":"Links"},{"location":"sources/vendor/VMWare/horizonview/#sourcetypes","text":"sourcetype notes vmware:horizon None nix:syslog When used with a default port this will follow the generic NIX configuration when using a dedicated port, IP or host rules events will follow the index configuration for vmware nsx","title":"Sourcetypes"},{"location":"sources/vendor/VMWare/horizonview/#sourcetype-and-index-configuration","text":"key sourcetype index notes vmware_horizon vmware:horizon main none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/VMWare/vsphere/","text":"Product - vSphere - ESX NSX (Controller, Manager, Edge) \u00b6 Vmware vsphere product line has multiple old and known issues in syslog output. GUID values sent in place of time stamp Improper time stamp in all RFC5424 events No PRI No syslog header for some split events mismatch syslog header for some split events (segment 1 contains header remaining segments contain no header) WARNING use of a load balancer with udp will cause \u201ccorrupt\u201d event behavior due to out of order message processing caused by the load balancer Ref Link Splunk Add-on ESX https://splunkbase.splunk.com/app/5603/ Splunk Add-on Vcenter https://splunkbase.splunk.com/app/5601/ Splunk Add-on nxs none Splunk Add-on vsan none Sourcetypes \u00b6 sourcetype notes vmware:esxlog:${PROGRAM} None vmware:nsxlog:${PROGRAM} None vmware:vclog:${PROGRAM} None nix:syslog When used with a default port, this will follow the generic NIX configuration. When using a dedicated port, IP or host rules events will follow the index configuration for vmware nsx Sourcetype and Index Configuration \u00b6 key sourcetype index notes vmware_vsphere_esx vmware:esxlog:${PROGRAM} infraops none vmware_vsphere_nsx vmware:nxlog:${PROGRAM} infraops none vmware_vsphere_nsxfw vmware:nxlog:dfwpktlogs infraops none vmware_vsphere_vc vmware:vclog:${PROGRAM} infraops none Filter type \u00b6 MSG Parse: This filter parses message content when using the default configuration. SC4S will normalize the structure of vmware events from multiple incorrectly formed varients to rfc5424 format to improve parsing Setup and Configuration \u00b6 Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source. Refer to the Splunk TA documentation for the specific customer format required for proxy configuration Select TCP or SSL transport option Ensure the format of the event is customized per Splunk documentation Options \u00b6 Variable default description SC4S_LISTEN_VMWARE_VSPHERE_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_VMWARE_VSPHERE_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_VMWARE_VSPHERE_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_SOURCE_VMWARE_VSPHERE_GROUPMSG empty string empty/yes groups known instances of improperly split events set \u201cyes\u201d to return to enable Verification \u00b6 An active proxy will generate frequent events. Use the following search to validate events are present per source device index=<asconfigured> sourcetype=\"vmware:vsphere:*\" | stats count by host Automatic Parser Configuration \u00b6 Enable the following options in the env_file #Do not enable with a SNAT load balancer SC4S_USE_NAME_CACHE = yes #Combine known split events into a single event for Splunk SC4S_SOURCE_VMWARE_VSPHERE_GROUPMSG = yes #Learn vendor product from recognized events and apply to generic events #for example after the first vpxd event sshd will utilize vps \"vmware_vsphere_nix_syslog\" rather than \"nix_syslog\" SC4S_USE_VPS_CACHE = yes Manual Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-vmware_vsphere.conf #File name provided is a suggestion it must be globally unique application app - vps - test - vmware_vsphere [ sc4s - vps ] { filter { #netmask(169.254.100.1/24) #host(\"-esx-\") }; parser { p_set_netsource_fields ( vendor ( ' vmware ' ) product ( ' vsphere ' ) ); }; };","title":"Vsphere"},{"location":"sources/vendor/VMWare/vsphere/#product-vsphere-esx-nsx-controller-manager-edge","text":"Vmware vsphere product line has multiple old and known issues in syslog output. GUID values sent in place of time stamp Improper time stamp in all RFC5424 events No PRI No syslog header for some split events mismatch syslog header for some split events (segment 1 contains header remaining segments contain no header) WARNING use of a load balancer with udp will cause \u201ccorrupt\u201d event behavior due to out of order message processing caused by the load balancer Ref Link Splunk Add-on ESX https://splunkbase.splunk.com/app/5603/ Splunk Add-on Vcenter https://splunkbase.splunk.com/app/5601/ Splunk Add-on nxs none Splunk Add-on vsan none","title":"Product - vSphere - ESX NSX (Controller, Manager, Edge)"},{"location":"sources/vendor/VMWare/vsphere/#sourcetypes","text":"sourcetype notes vmware:esxlog:${PROGRAM} None vmware:nsxlog:${PROGRAM} None vmware:vclog:${PROGRAM} None nix:syslog When used with a default port, this will follow the generic NIX configuration. When using a dedicated port, IP or host rules events will follow the index configuration for vmware nsx","title":"Sourcetypes"},{"location":"sources/vendor/VMWare/vsphere/#sourcetype-and-index-configuration","text":"key sourcetype index notes vmware_vsphere_esx vmware:esxlog:${PROGRAM} infraops none vmware_vsphere_nsx vmware:nxlog:${PROGRAM} infraops none vmware_vsphere_nsxfw vmware:nxlog:dfwpktlogs infraops none vmware_vsphere_vc vmware:vclog:${PROGRAM} infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/VMWare/vsphere/#filter-type","text":"MSG Parse: This filter parses message content when using the default configuration. SC4S will normalize the structure of vmware events from multiple incorrectly formed varients to rfc5424 format to improve parsing","title":"Filter type"},{"location":"sources/vendor/VMWare/vsphere/#setup-and-configuration","text":"Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source. Refer to the Splunk TA documentation for the specific customer format required for proxy configuration Select TCP or SSL transport option Ensure the format of the event is customized per Splunk documentation","title":"Setup and Configuration"},{"location":"sources/vendor/VMWare/vsphere/#options","text":"Variable default description SC4S_LISTEN_VMWARE_VSPHERE_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_VMWARE_VSPHERE_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_VMWARE_VSPHERE_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_SOURCE_VMWARE_VSPHERE_GROUPMSG empty string empty/yes groups known instances of improperly split events set \u201cyes\u201d to return to enable","title":"Options"},{"location":"sources/vendor/VMWare/vsphere/#verification","text":"An active proxy will generate frequent events. Use the following search to validate events are present per source device index=<asconfigured> sourcetype=\"vmware:vsphere:*\" | stats count by host","title":"Verification"},{"location":"sources/vendor/VMWare/vsphere/#automatic-parser-configuration","text":"Enable the following options in the env_file #Do not enable with a SNAT load balancer SC4S_USE_NAME_CACHE = yes #Combine known split events into a single event for Splunk SC4S_SOURCE_VMWARE_VSPHERE_GROUPMSG = yes #Learn vendor product from recognized events and apply to generic events #for example after the first vpxd event sshd will utilize vps \"vmware_vsphere_nix_syslog\" rather than \"nix_syslog\" SC4S_USE_VPS_CACHE = yes","title":"Automatic Parser Configuration"},{"location":"sources/vendor/VMWare/vsphere/#manual-parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-vmware_vsphere.conf #File name provided is a suggestion it must be globally unique application app - vps - test - vmware_vsphere [ sc4s - vps ] { filter { #netmask(169.254.100.1/24) #host(\"-esx-\") }; parser { p_set_netsource_fields ( vendor ( ' vmware ' ) product ( ' vsphere ' ) ); }; };","title":"Manual Parser Configuration"},{"location":"sources/vendor/Varonis/datadvantage/","text":"DatAdvantage \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Technology Add-On for Varonis https://splunkbase.splunk.com/app/4256/ Sourcetypes \u00b6 sourcetype notes varonis:ta Index Configuration \u00b6 key sourcetype index notes Varonis Inc._DatAdvantage varonis:ta main","title":"DatAdvantage"},{"location":"sources/vendor/Varonis/datadvantage/#datadvantage","text":"","title":"DatAdvantage"},{"location":"sources/vendor/Varonis/datadvantage/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Varonis/datadvantage/#links","text":"Ref Link Technology Add-On for Varonis https://splunkbase.splunk.com/app/4256/","title":"Links"},{"location":"sources/vendor/Varonis/datadvantage/#sourcetypes","text":"sourcetype notes varonis:ta","title":"Sourcetypes"},{"location":"sources/vendor/Varonis/datadvantage/#index-configuration","text":"key sourcetype index notes Varonis Inc._DatAdvantage varonis:ta main","title":"Index Configuration"},{"location":"sources/vendor/Vectra/cognito/","text":"Cognito \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Technology Add-On for Vectra Cognito https://splunkbase.splunk.com/app/4408/ Sourcetypes \u00b6 sourcetype notes vectra:cognito:detect vectra:cognito:accountdetect vectra:cognito:accountscoring vectra:cognito:audit vectra:cognito:campaigns vectra:cognito:health vectra:cognito:hostscoring vectra:cognito:accountlockdown Index Configuration \u00b6 key sourcetype index notes Vectra Networks_X Series vectra:cognito:detect main Vectra Networks_X Series_accountdetect vectra:cognito:accountdetect main Vectra Networks_X Series_asc vectra:cognito:accountscoring main Vectra Networks_X Series_audit vectra:cognito:audit main Vectra Networks_X Series_campaigns vectra:cognito:campaigns main Vectra Networks_X Series_health vectra:cognito:health main Vectra Networks_X Series_hsc vectra:cognito:hostscoring main Vectra Networks_X Series_lockdown vectra:cognito:accountlockdown main","title":"Cognito"},{"location":"sources/vendor/Vectra/cognito/#cognito","text":"","title":"Cognito"},{"location":"sources/vendor/Vectra/cognito/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Vectra/cognito/#links","text":"Ref Link Technology Add-On for Vectra Cognito https://splunkbase.splunk.com/app/4408/","title":"Links"},{"location":"sources/vendor/Vectra/cognito/#sourcetypes","text":"sourcetype notes vectra:cognito:detect vectra:cognito:accountdetect vectra:cognito:accountscoring vectra:cognito:audit vectra:cognito:campaigns vectra:cognito:health vectra:cognito:hostscoring vectra:cognito:accountlockdown","title":"Sourcetypes"},{"location":"sources/vendor/Vectra/cognito/#index-configuration","text":"key sourcetype index notes Vectra Networks_X Series vectra:cognito:detect main Vectra Networks_X Series_accountdetect vectra:cognito:accountdetect main Vectra Networks_X Series_asc vectra:cognito:accountscoring main Vectra Networks_X Series_audit vectra:cognito:audit main Vectra Networks_X Series_campaigns vectra:cognito:campaigns main Vectra Networks_X Series_health vectra:cognito:health main Vectra Networks_X Series_hsc vectra:cognito:hostscoring main Vectra Networks_X Series_lockdown vectra:cognito:accountlockdown main","title":"Index Configuration"},{"location":"sources/vendor/Wallix/bastion/","text":"Bastion \u00b6 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3661/ Sourcetypes \u00b6 sourcetype notes WB:syslog note this sourcetype includes program:rdproxy all other data will be treated as nix Sourcetype and Index Configuration \u00b6 key sourcetype index notes wallix_bastion infraops main none Parser Configuration \u00b6 #/opt/sc4s/local/config/app-parsers/app-vps-wallix_bastion.conf #File name provided is a suggestion it must be globally unique application app - vps - test - wallix_bastion [ sc4s - vps ] { filter { host ( ' ^ wasb ' ) }; parser { p_set_netsource_fields ( vendor ( ' wallix ' ) product ( ' bastion ' ) ); }; };","title":"Bastion"},{"location":"sources/vendor/Wallix/bastion/#bastion","text":"","title":"Bastion"},{"location":"sources/vendor/Wallix/bastion/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Wallix/bastion/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3661/","title":"Links"},{"location":"sources/vendor/Wallix/bastion/#sourcetypes","text":"sourcetype notes WB:syslog note this sourcetype includes program:rdproxy all other data will be treated as nix","title":"Sourcetypes"},{"location":"sources/vendor/Wallix/bastion/#sourcetype-and-index-configuration","text":"key sourcetype index notes wallix_bastion infraops main none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Wallix/bastion/#parser-configuration","text":"#/opt/sc4s/local/config/app-parsers/app-vps-wallix_bastion.conf #File name provided is a suggestion it must be globally unique application app - vps - test - wallix_bastion [ sc4s - vps ] { filter { host ( ' ^ wasb ' ) }; parser { p_set_netsource_fields ( vendor ( ' wallix ' ) product ( ' bastion ' ) ); }; };","title":"Parser Configuration"},{"location":"sources/vendor/XYPro/mergedaudit/","text":"Merged Audit \u00b6 XY Pro merged audit also called XYGate or XMA is the defacto solution for syslog from HP Nonstop Server (Tandem) Key facts \u00b6 Legacy BSD Format default port 514 CEF Format Links \u00b6 Ref Link Splunk Add-on None Product Manual https://xypro.com/products/hpe-software-from-xypro/ Sourcetypes \u00b6 sourcetype notes cef None Sourcetype and Index Configuration \u00b6 key sourcetype index notes XYPRO_NONSTOP cef infraops none","title":"Merged Audit"},{"location":"sources/vendor/XYPro/mergedaudit/#merged-audit","text":"XY Pro merged audit also called XYGate or XMA is the defacto solution for syslog from HP Nonstop Server (Tandem)","title":"Merged Audit"},{"location":"sources/vendor/XYPro/mergedaudit/#key-facts","text":"Legacy BSD Format default port 514 CEF Format","title":"Key facts"},{"location":"sources/vendor/XYPro/mergedaudit/#links","text":"Ref Link Splunk Add-on None Product Manual https://xypro.com/products/hpe-software-from-xypro/","title":"Links"},{"location":"sources/vendor/XYPro/mergedaudit/#sourcetypes","text":"sourcetype notes cef None","title":"Sourcetypes"},{"location":"sources/vendor/XYPro/mergedaudit/#sourcetype-and-index-configuration","text":"key sourcetype index notes XYPRO_NONSTOP cef infraops none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Zscaler/lss/","text":"LSS \u00b6 The ZScaler product manual includes and extensive section of configuration for multiple Splunk TCP input ports around page 26. When using SC4S these ports are not required and should not be used. Simply configure all outputs from the LSS to utilize the IP or host name of the SC4S instance and port 514 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3865/ Product Manual https://community.zscaler.com/t/zscaler-splunk-app-design-and-installation-documentation/4728 Sourcetypes \u00b6 sourcetype notes zscaler_lss-app None zscaler_lss-auth None zscaler_lss-bba None zscaler_lss-connector None Sourcetype and Index Configuration \u00b6 key sourcetype index notes zscaler_lss zscalerlss_zpa-app netproxy none zscaler_lss zscalerlss_zpa_auth netproxy none zscaler_lss zscalerlss_zpa_auth netproxy none zscaler_lss zscalerlss_zpa_connector netproxy none","title":"LSS"},{"location":"sources/vendor/Zscaler/lss/#lss","text":"The ZScaler product manual includes and extensive section of configuration for multiple Splunk TCP input ports around page 26. When using SC4S these ports are not required and should not be used. Simply configure all outputs from the LSS to utilize the IP or host name of the SC4S instance and port 514","title":"LSS"},{"location":"sources/vendor/Zscaler/lss/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Zscaler/lss/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3865/ Product Manual https://community.zscaler.com/t/zscaler-splunk-app-design-and-installation-documentation/4728","title":"Links"},{"location":"sources/vendor/Zscaler/lss/#sourcetypes","text":"sourcetype notes zscaler_lss-app None zscaler_lss-auth None zscaler_lss-bba None zscaler_lss-connector None","title":"Sourcetypes"},{"location":"sources/vendor/Zscaler/lss/#sourcetype-and-index-configuration","text":"key sourcetype index notes zscaler_lss zscalerlss_zpa-app netproxy none zscaler_lss zscalerlss_zpa_auth netproxy none zscaler_lss zscalerlss_zpa_auth netproxy none zscaler_lss zscalerlss_zpa_connector netproxy none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Zscaler/nss/","text":"NSS \u00b6 The ZScaler product manual includes and extensive section of configuration for multiple Splunk TCP input ports around page 26. When using SC4S these ports are not required and should not be used. Simply configure all outputs from the NSS to utilize the IP or host name of the SC4S instance and port 514 Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3865/ Product Manual https://community.zscaler.com/t/zscaler-splunk-app-design-and-installation-documentation/4728 Sourcetypes \u00b6 sourcetype notes zscaler_nss_alerts Requires format customization add \\tvendor=Zscaler\\tproduct=alerts immediately prior to the \\n in the NSS Alert Web format. See Zscaler manual for more info. zscaler_nss_dns Requires format customization add \\tvendor=Zscaler\\tproduct=dns immediately prior to the \\n in the NSS DNS format. See Zscaler manual for more info. zscaler_nss_web None zscaler_nss_fw Requires format customization add \\tvendor=Zscaler\\tproduct=fw immediately prior to the \\n in the Firewall format. See Zscaler manual for more info. Sourcetype and Index Configuration \u00b6 key sourcetype index notes zscaler_nss_alerts zscalernss-alerts main none zscaler_nss_dns zscalernss-dns netdns none zscaler_nss_fw zscalernss-fw netfw none zscaler_nss_web zscalernss-web netproxy none zscaler_nss_tunnel zscalernss-tunnel netops none zscaler_zia_audit zscalernss-zia-audit netops none zscaler_zia_sandbox zscalernss-zia-sandbox main none Filter type \u00b6 MSG Parse: This filter parses message content Setup and Configuration \u00b6 Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source. Refer to the Splunk TA documentation for the specific customer format required for proxy configuration Select TCP or SSL transport option Ensure the format of the event is customized per Splunk documentation","title":"NSS"},{"location":"sources/vendor/Zscaler/nss/#nss","text":"The ZScaler product manual includes and extensive section of configuration for multiple Splunk TCP input ports around page 26. When using SC4S these ports are not required and should not be used. Simply configure all outputs from the NSS to utilize the IP or host name of the SC4S instance and port 514","title":"NSS"},{"location":"sources/vendor/Zscaler/nss/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/Zscaler/nss/#links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3865/ Product Manual https://community.zscaler.com/t/zscaler-splunk-app-design-and-installation-documentation/4728","title":"Links"},{"location":"sources/vendor/Zscaler/nss/#sourcetypes","text":"sourcetype notes zscaler_nss_alerts Requires format customization add \\tvendor=Zscaler\\tproduct=alerts immediately prior to the \\n in the NSS Alert Web format. See Zscaler manual for more info. zscaler_nss_dns Requires format customization add \\tvendor=Zscaler\\tproduct=dns immediately prior to the \\n in the NSS DNS format. See Zscaler manual for more info. zscaler_nss_web None zscaler_nss_fw Requires format customization add \\tvendor=Zscaler\\tproduct=fw immediately prior to the \\n in the Firewall format. See Zscaler manual for more info.","title":"Sourcetypes"},{"location":"sources/vendor/Zscaler/nss/#sourcetype-and-index-configuration","text":"key sourcetype index notes zscaler_nss_alerts zscalernss-alerts main none zscaler_nss_dns zscalernss-dns netdns none zscaler_nss_fw zscalernss-fw netfw none zscaler_nss_web zscalernss-web netproxy none zscaler_nss_tunnel zscalernss-tunnel netops none zscaler_zia_audit zscalernss-zia-audit netops none zscaler_zia_sandbox zscalernss-zia-sandbox main none","title":"Sourcetype and Index Configuration"},{"location":"sources/vendor/Zscaler/nss/#filter-type","text":"MSG Parse: This filter parses message content","title":"Filter type"},{"location":"sources/vendor/Zscaler/nss/#setup-and-configuration","text":"Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer. Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source. Refer to the Splunk TA documentation for the specific customer format required for proxy configuration Select TCP or SSL transport option Ensure the format of the event is customized per Splunk documentation","title":"Setup and Configuration"},{"location":"sources/vendor/syslog-ng/loggen/","text":"loggen \u00b6 Loggen is a tool used to load test syslog implementations. Key facts \u00b6 MSG Format based filter Legacy BSD Format default port 514 Links \u00b6 Ref Link Product Manual https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.26/administration-guide/96#loggen.1 Sourcetypes \u00b6 sourcetype notes syslogng:loggen By default, loggen uses the legacy BSD-syslog message format. BSD example: loggen --inet --dgram --number 1 <ip> <port> RFC5424 example: loggen --inet --dgram -PF --number 1 <ip> <port> Refer to above manual link for more examples. Index Configuration \u00b6 key index notes syslogng_loggen main none","title":"loggen"},{"location":"sources/vendor/syslog-ng/loggen/#loggen","text":"Loggen is a tool used to load test syslog implementations.","title":"loggen"},{"location":"sources/vendor/syslog-ng/loggen/#key-facts","text":"MSG Format based filter Legacy BSD Format default port 514","title":"Key facts"},{"location":"sources/vendor/syslog-ng/loggen/#links","text":"Ref Link Product Manual https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.26/administration-guide/96#loggen.1","title":"Links"},{"location":"sources/vendor/syslog-ng/loggen/#sourcetypes","text":"sourcetype notes syslogng:loggen By default, loggen uses the legacy BSD-syslog message format. BSD example: loggen --inet --dgram --number 1 <ip> <port> RFC5424 example: loggen --inet --dgram -PF --number 1 <ip> <port> Refer to above manual link for more examples.","title":"Sourcetypes"},{"location":"sources/vendor/syslog-ng/loggen/#index-configuration","text":"key index notes syslogng_loggen main none","title":"Index Configuration"},{"location":"troubleshooting/troubleshoot_SC4S_server/","text":"SC4S Server Startup and Operational Validation \u00b6 The following sections will guide the administrator to the most commons solutions to startup and operational issues with SC4S. In general, if you are just starting out with SC4S and wish to simply run with the \u201cstock\u201d configuration, startup out of systemd is recommended. If, on the other hand, you are in the depths of a custom configuration of SC4S with significant modifications (such as multiple unique ports for sources, hostname/CIDR block configuration for sources, new log paths, etc.) then it is best to start SC4S with the container runtime command ( podman or docker ) directly from the command line (below). When you are satisfied with the operation, a transition to systemd can then be made. systemd Errors During SC4S Startup \u00b6 Most issues that occur with startup and operation of sc4s typically involve syntax errors or duplicate listening ports. If you are running out of systemd, you may see this at startup: [ root@sc4s syslog-ng ] # systemctl start sc4s Job for sc4s.service failed because the control process exited with error code. See \"systemctl status sc4s.service\" and \"journalctl -xe\" for details. Follow the checks below to resolve the issue: Is the SC4S container running? \u00b6 There may be nothing untoward after starting with systemd, but the container is not running at all after checking with podman logs SC4S or podman ps . A more informative command than journalctl -xe is the following, journalctl -b -u sc4s | tail -100 which will print the last 100 lines of the system journal in far more detail, which should be sufficient to see the specific failure (syntax or runtime) and guide you in troubleshooting why the container exited unexpectedly. Does the SC4S container start (and run) properly outside of the systemd service environment? \u00b6 As an alternative to launching via systemd during the initial installation phase, you may wish to test the container startup outside of the systemd startup environment. This alternative should be considered required when undergoing heavy troubleshooting or log path development (e.g. when SC4S_DEBUG_CONTAINER is set to \u201cyes\u201d). The following command will launch the container directly from the CLI. This command assumes the local mounted directories are set up as shown in the \u201cgetting started\u201d examples; adjust for your local requirements: /usr/bin/podman run \\ -v splunk-sc4s-var:/var/lib/syslog-ng \\ -v /opt/sc4s/local:/etc/syslog-ng/conf.d/local:z \\ -v /opt/sc4s/archive:/var/lib/syslog-ng/archive:z \\ -v /opt/sc4s/tls:/etc/syslog-ng/tls:z \\ --env-file = /opt/sc4s/env_file \\ --network host \\ --name SC4S \\ --rm splunk/scs:latest If you are using docker, substitute \u201cdocker\u201d for \u201cpodman\u201d for the container runtime command above. Is the container still running (when systemd thinks it\u2019s not)? \u00b6 In some instances, (particularly when SC4S_DEBUG_CONTAINER=yes ) an SC4S container might not shut down completely when starting/stopping out of systemd, and systemd will attempt to start a new container when one is already running with the SC4S name. You will see this type of output when viewing the journal after a failed start caused by this condition, or a similar message when the container is run directly from the CLI: Jul 15 18:45:20 sra-sc4s-alln01-02 podman[11187]: Error: error creating container storage: the container name \"SC4S\" is already in use by \"894357502b2a7142d097ea3ca1468d1cb4fbc69959a9817a1bbe145a09d37fb9\". You have to remove that container... Jul 15 18:45:20 sra-sc4s-alln01-02 systemd[1]: sc4s.service: Main process exited, code=exited, status=125/n/a To rectify this, simply execute podman rm -f SC4S SC4S should then start normally. NOTE: This symptom will recur if SC4S_DEBUG_CONTAINER is set to \u201cyes\u201d. Do not attempt to use systemd when this variable is set; use the CLI podman or docker commands directly to start/stop SC4S. HEC/token connection errors (AKA \u201cNo data in Splunk\u201d) \u00b6 SC4S performs basic HEC connectivity and index checks at startup. These indicate general connection issues and indexes that may not be accessible and/or configured on the Splunk side. To check the container logs which contain the results of these tests, run: /usr/bin/<podman | docker> logs SC4S and note the output. You will see entries similar to these: SC4S_ENV_CHECK_HEC : Splunk HEC connection test successful ; checking indexes ... SC4S_ENV_CHECK_INDEX : Checking email { \"text\" : \"Incorrect index\" , \"code\" : 7 , \"invalid-event-number\" : 1 } SC4S_ENV_CHECK_INDEX : Checking epav { \"text\" : \"Incorrect index\" , \"code\" : 7 , \"invalid-event-number\" : 1 } SC4S_ENV_CHECK_INDEX : Checking main { \"text\" : \"Success\" , \"code\" : 0 } Note the specifics of the indexes that are not configured correctly, and rectify in the Splunk configuration. If this is not addressed properly, you may see output similar to the below when data flows into sc4s: Mar 16 19 : 00 : 06 b817af4e89da syslog-ng [ 1 ] : Server returned with a 4XX ( client errors ) status code , which means we are not authorized or the URL is not found .; url = 'https://splunk-instance.com:8088/services/collector/event' , status_code = '400' , driver = 'd_hec#0' , location = '/opt/syslog-ng/etc/conf.d/destinations/splunk_hec.conf:2:5' Mar 16 19 : 00 : 06 b817af4e89da syslog-ng [ 1 ] : Server disconnected while preparing messages for sending , trying again ; driver = 'd_hec#0' , location = '/opt/syslog-ng/etc/conf.d/destinations/splunk_hec.conf:2:5' , worker_index = '4' , time_reopen = '10' , batch_size = '1000' This is an indication that the standard d_hec destination in syslog-ng (which is the route to Splunk) is being rejected by the HEC endpoint. A 400 error (not 404) is normally caused by an index that has not been created on the Splunk side. This can present a serious problem, as just one bad index will \u201ctaint\u201d the entire batch (in this case, 1000 events) and prevent any of them from being sent to Splunk. It is imperative that the container logs be free of these kinds of errors in production. You can use the alternate HEC debug destination (below) to help debug this condition by sending direct \u201ccurl\u201d commands to the HEC endpoint outside of the SC4S setting. SC4S Local Disk Resource Considerations \u00b6 Check the HEC connection to Splunk. If the connection is down for a long period of time, the local disk buffer used for backup will exhaust local disk resources. The size of the local disk buffer is configured in the env_file: Disk buffer configuration Check the env_file to see if SC4S_DEST_GLOBAL_ALTERNATES is set to d_hec_debug , d_archive or other file-based destination; all of these will consume significant local disk space. d_hec_debug and d_archive are organized by sourcetype; the du -sh * command can be used in each subdirectory to find the culprit. Try rebuilding sc4s volume podman volume rm splunk - sc4s - var podman volume create splunk - sc4s - var Try pruning containers podman system prune [--all] SC4S/kernel UDP Input Buffer Settings \u00b6 SC4S has a setting that requests a certain buffer size when configuring the UDP sockets. The kernel must have its parameters set to at least the same size (or greater) than the syslog-ng config is requesting, or the following will occur in the SC4S logs: /usr/bin/<podman | docker> logs SC4S Note the output. The following warning message is not a failure condition unless we are reaching the upper limit of hardware performance. The kernel refused to set the receive buffer (SO_RCVBUF) to the requested size, you probably need to adjust buffer related kernel parameters; so_rcvbuf='1703936', so_rcvbuf_set='425984' Make changes to /etc/sysctl.conf. Changing receive buffer values here to 16 MB: net.core.rmem_default = 17039360 net.core.rmem_max = 17039360 Run following commands for changes to be affected. sysctl -p restart SC4S SC4S TLS Listener Validation \u00b6 To verify the correct configuration of the TLS server use the following command. Replace the IP, FQDN, and port as appropriate: <podman | docker> run -ti drwetter/testssl.sh --severity MEDIUM --ip 127 .0.0.1 selfsigned.example.com:6510 Timezone mismatch in events \u00b6 By default, SC4S resolves the timezone to GMT. If customer have a preference to use local TZ then set the user TZ preference in Splunk during search time rather than at index time. Timezone config documentation Dealing with non RFC-5424 compliant sources \u00b6 If a data source you are trying to ingest claims it is RFC-5424 compliant but you are getting an \u201cError processing log message:\u201d from SC4S, the message violates the standard in some way. Unfortunately multiple vendors claim RFC-5424 compliance without fully testing that they are. In this case, the underlying syslog-ng process will send an error event, with the location of the error in the original event highlighted with >@< to indicate where the error occurred. Here is an example error message: { [ - ] ISODATE : 2020 -05-04 T21 : 21 : 59.001 + 00 : 00 MESSAGE : Error processing log message : < 14 > 1 2020 -05-04 T21 : 21 : 58.117351 + 00 : 00 arcata - pks - cluster -1 pod . log / cf - workloads / logspinner - testing -6446 b8ef - - [ kubernetes @47450 cloudfoundry . org / process_type = \"web\" cloudfoundry . org / rootfs - version = \"v75.0.0\" cloudfoundry . org / version = \"eae53cc3-148d-4395-985c-8fef0606b9e3\" controller - revision - hash = \"logspinner-testing-6446b8ef05-7db777754c\" cloudfoundry . org / app_guid = \"f71634fe-34a4-4f89-adac-3e523f61a401\" cloudfoundry . org / source_type = \"APP\" security . istio . io / tlsMode = \"istio\" statefulset . kubernetes . io / pod - n > @ < ame = \"logspinner-testing-6446b8ef05-0\" cloudfoundry . org / guid = \"f71634fe-34a4-4f89-adac-3e523f61a401\" namespace_name = \"cf-workloads\" object_name = \"logspinner-testing-6446b8ef05-0\" container_name = \"opi\" vm_id = \"vm-e34452a3-771e-4994-666e-bfbc7eb77489\" ] Duration 10.00299412 s TotalSent 10 Rate 0.999701 PID : 33 PRI : < 43 > PROGRAM : syslog - ng } In this example the error can be seen in the snippet statefulset.kubernetes.io/pod-n>@<ame . Looking at the spec for RFC5424, it states that the \u201cSD-NAME\u201d (the left-hand side of the name=value pairs) cannot be longer than 32 printable ASCII characters. In this message, the indicated name exceeds that. Unfortunately, this is a spec violation on the part of the vendor. Ideally the vendor would address this violation so their logs would be RFC-5424 compliant. Alternatively, an exception could be added to the SC4S filter log path (or an alternative (workaround) log path created) for the data source if the vendor can\u2019t/won\u2019t fix the defect. In this example, the reason RAWMSG is not shown in the fields above is because this error message is coming from syslog-ng itself \u2013 not the filter/log path. In messages of the type Error processing log message: where the PROGRAM is shown as syslog-ng , that is the clue your incoming message is not RFC-5424 compliant (though it\u2019s often close, as is the case here). In BYOE the metrics/internal processing message are abusing the terminal , how to fix this? \u00b6 In BYOE, when we try to start sc4s service , the terminal is getting abused from the internal and metrics log Example of the issue is Github Terminal abuse issue To rectify this, Please set following property in env_file SC4S_SEND_METRICS_TERMINAL = no Restart SC4S and it will not send any more metrics data to Terminal. NOTE: This symptom will recur if SC4S_DEBUG_CONTAINER is set to \u201cyes\u201d. Do not attempt to use systemd when this variable is set; use the CLI podman or docker commands directly to start/stop SC4S. SC4S dropping invalid events \u00b6 Sometimes you notice you are missing some CEF logs which are not RFC compliant but logs are important, how to fix this? \u00b6 To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_CEF=yes Restart SC4S and it will not drop any invalid CEF format. If you notice you are missing some VMWARE CB-PROTECT logs which are not RFC compliant but logs are important, how to fix this? \u00b6 To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_VMWARE_CB_PROTECT=yes Restart SC4S and it will not drop any invalid VMWARE CB-PROTECT format. If you notice you are missing some CISCO IOS logs which are not RFC compliant but logs are important, how to fix this? \u00b6 To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_CISCO=yes Restart SC4S and it will not drop any invalid CISCO IOS format. If you notice you are missing some VMWARE VSPHERE logs which are not RFC compliant but logs are important, how to fix this? \u00b6 To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_VMWARE_VSPHERE=yes Restart SC4S and it will not drop any invalid VMWARE VSPHERE format. If you notice you are missing some RAW BSD logs which are not RFC compliant but logs are important, how to fix this? \u00b6 To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_RAW_BSD=yes Restart SC4S and it will not drop any invalid RAW BSD format. If you notice you are missing some RAW XML logs which are not RFC compliant but logs are important, how to fix this? \u00b6 To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_XML=yes Restart SC4S and it will not drop any invalid RAW XML format. If you notice you are missing some HPE JETDIRECT logs which are not RFC compliant but logs are important, how to fix this? \u00b6 To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_HPE=yes Restart SC4S and it will not drop any invalid HPE JETDIRECT format. NOTE: Please use only in this case of exception and this is splunk-unsupported feature. Also this setting might impact SC4S performance.","title":"SC4S Startup and Validation"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sc4s-server-startup-and-operational-validation","text":"The following sections will guide the administrator to the most commons solutions to startup and operational issues with SC4S. In general, if you are just starting out with SC4S and wish to simply run with the \u201cstock\u201d configuration, startup out of systemd is recommended. If, on the other hand, you are in the depths of a custom configuration of SC4S with significant modifications (such as multiple unique ports for sources, hostname/CIDR block configuration for sources, new log paths, etc.) then it is best to start SC4S with the container runtime command ( podman or docker ) directly from the command line (below). When you are satisfied with the operation, a transition to systemd can then be made.","title":"SC4S Server Startup and Operational Validation"},{"location":"troubleshooting/troubleshoot_SC4S_server/#systemd-errors-during-sc4s-startup","text":"Most issues that occur with startup and operation of sc4s typically involve syntax errors or duplicate listening ports. If you are running out of systemd, you may see this at startup: [ root@sc4s syslog-ng ] # systemctl start sc4s Job for sc4s.service failed because the control process exited with error code. See \"systemctl status sc4s.service\" and \"journalctl -xe\" for details. Follow the checks below to resolve the issue:","title":"systemd Errors During SC4S Startup"},{"location":"troubleshooting/troubleshoot_SC4S_server/#is-the-sc4s-container-running","text":"There may be nothing untoward after starting with systemd, but the container is not running at all after checking with podman logs SC4S or podman ps . A more informative command than journalctl -xe is the following, journalctl -b -u sc4s | tail -100 which will print the last 100 lines of the system journal in far more detail, which should be sufficient to see the specific failure (syntax or runtime) and guide you in troubleshooting why the container exited unexpectedly.","title":"Is the SC4S container running?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#does-the-sc4s-container-start-and-run-properly-outside-of-the-systemd-service-environment","text":"As an alternative to launching via systemd during the initial installation phase, you may wish to test the container startup outside of the systemd startup environment. This alternative should be considered required when undergoing heavy troubleshooting or log path development (e.g. when SC4S_DEBUG_CONTAINER is set to \u201cyes\u201d). The following command will launch the container directly from the CLI. This command assumes the local mounted directories are set up as shown in the \u201cgetting started\u201d examples; adjust for your local requirements: /usr/bin/podman run \\ -v splunk-sc4s-var:/var/lib/syslog-ng \\ -v /opt/sc4s/local:/etc/syslog-ng/conf.d/local:z \\ -v /opt/sc4s/archive:/var/lib/syslog-ng/archive:z \\ -v /opt/sc4s/tls:/etc/syslog-ng/tls:z \\ --env-file = /opt/sc4s/env_file \\ --network host \\ --name SC4S \\ --rm splunk/scs:latest If you are using docker, substitute \u201cdocker\u201d for \u201cpodman\u201d for the container runtime command above.","title":"Does the SC4S container start (and run) properly outside of the systemd service environment?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#is-the-container-still-running-when-systemd-thinks-its-not","text":"In some instances, (particularly when SC4S_DEBUG_CONTAINER=yes ) an SC4S container might not shut down completely when starting/stopping out of systemd, and systemd will attempt to start a new container when one is already running with the SC4S name. You will see this type of output when viewing the journal after a failed start caused by this condition, or a similar message when the container is run directly from the CLI: Jul 15 18:45:20 sra-sc4s-alln01-02 podman[11187]: Error: error creating container storage: the container name \"SC4S\" is already in use by \"894357502b2a7142d097ea3ca1468d1cb4fbc69959a9817a1bbe145a09d37fb9\". You have to remove that container... Jul 15 18:45:20 sra-sc4s-alln01-02 systemd[1]: sc4s.service: Main process exited, code=exited, status=125/n/a To rectify this, simply execute podman rm -f SC4S SC4S should then start normally. NOTE: This symptom will recur if SC4S_DEBUG_CONTAINER is set to \u201cyes\u201d. Do not attempt to use systemd when this variable is set; use the CLI podman or docker commands directly to start/stop SC4S.","title":"Is the container still running (when systemd thinks it's not)?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#hectoken-connection-errors-aka-no-data-in-splunk","text":"SC4S performs basic HEC connectivity and index checks at startup. These indicate general connection issues and indexes that may not be accessible and/or configured on the Splunk side. To check the container logs which contain the results of these tests, run: /usr/bin/<podman | docker> logs SC4S and note the output. You will see entries similar to these: SC4S_ENV_CHECK_HEC : Splunk HEC connection test successful ; checking indexes ... SC4S_ENV_CHECK_INDEX : Checking email { \"text\" : \"Incorrect index\" , \"code\" : 7 , \"invalid-event-number\" : 1 } SC4S_ENV_CHECK_INDEX : Checking epav { \"text\" : \"Incorrect index\" , \"code\" : 7 , \"invalid-event-number\" : 1 } SC4S_ENV_CHECK_INDEX : Checking main { \"text\" : \"Success\" , \"code\" : 0 } Note the specifics of the indexes that are not configured correctly, and rectify in the Splunk configuration. If this is not addressed properly, you may see output similar to the below when data flows into sc4s: Mar 16 19 : 00 : 06 b817af4e89da syslog-ng [ 1 ] : Server returned with a 4XX ( client errors ) status code , which means we are not authorized or the URL is not found .; url = 'https://splunk-instance.com:8088/services/collector/event' , status_code = '400' , driver = 'd_hec#0' , location = '/opt/syslog-ng/etc/conf.d/destinations/splunk_hec.conf:2:5' Mar 16 19 : 00 : 06 b817af4e89da syslog-ng [ 1 ] : Server disconnected while preparing messages for sending , trying again ; driver = 'd_hec#0' , location = '/opt/syslog-ng/etc/conf.d/destinations/splunk_hec.conf:2:5' , worker_index = '4' , time_reopen = '10' , batch_size = '1000' This is an indication that the standard d_hec destination in syslog-ng (which is the route to Splunk) is being rejected by the HEC endpoint. A 400 error (not 404) is normally caused by an index that has not been created on the Splunk side. This can present a serious problem, as just one bad index will \u201ctaint\u201d the entire batch (in this case, 1000 events) and prevent any of them from being sent to Splunk. It is imperative that the container logs be free of these kinds of errors in production. You can use the alternate HEC debug destination (below) to help debug this condition by sending direct \u201ccurl\u201d commands to the HEC endpoint outside of the SC4S setting.","title":"HEC/token connection errors (AKA \u201cNo data in Splunk\u201d)"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sc4s-local-disk-resource-considerations","text":"Check the HEC connection to Splunk. If the connection is down for a long period of time, the local disk buffer used for backup will exhaust local disk resources. The size of the local disk buffer is configured in the env_file: Disk buffer configuration Check the env_file to see if SC4S_DEST_GLOBAL_ALTERNATES is set to d_hec_debug , d_archive or other file-based destination; all of these will consume significant local disk space. d_hec_debug and d_archive are organized by sourcetype; the du -sh * command can be used in each subdirectory to find the culprit. Try rebuilding sc4s volume podman volume rm splunk - sc4s - var podman volume create splunk - sc4s - var Try pruning containers podman system prune [--all]","title":"SC4S Local Disk Resource Considerations"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sc4skernel-udp-input-buffer-settings","text":"SC4S has a setting that requests a certain buffer size when configuring the UDP sockets. The kernel must have its parameters set to at least the same size (or greater) than the syslog-ng config is requesting, or the following will occur in the SC4S logs: /usr/bin/<podman | docker> logs SC4S Note the output. The following warning message is not a failure condition unless we are reaching the upper limit of hardware performance. The kernel refused to set the receive buffer (SO_RCVBUF) to the requested size, you probably need to adjust buffer related kernel parameters; so_rcvbuf='1703936', so_rcvbuf_set='425984' Make changes to /etc/sysctl.conf. Changing receive buffer values here to 16 MB: net.core.rmem_default = 17039360 net.core.rmem_max = 17039360 Run following commands for changes to be affected. sysctl -p restart SC4S","title":"SC4S/kernel UDP Input Buffer Settings"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sc4s-tls-listener-validation","text":"To verify the correct configuration of the TLS server use the following command. Replace the IP, FQDN, and port as appropriate: <podman | docker> run -ti drwetter/testssl.sh --severity MEDIUM --ip 127 .0.0.1 selfsigned.example.com:6510","title":"SC4S TLS Listener Validation"},{"location":"troubleshooting/troubleshoot_SC4S_server/#timezone-mismatch-in-events","text":"By default, SC4S resolves the timezone to GMT. If customer have a preference to use local TZ then set the user TZ preference in Splunk during search time rather than at index time. Timezone config documentation","title":"Timezone mismatch in events"},{"location":"troubleshooting/troubleshoot_SC4S_server/#dealing-with-non-rfc-5424-compliant-sources","text":"If a data source you are trying to ingest claims it is RFC-5424 compliant but you are getting an \u201cError processing log message:\u201d from SC4S, the message violates the standard in some way. Unfortunately multiple vendors claim RFC-5424 compliance without fully testing that they are. In this case, the underlying syslog-ng process will send an error event, with the location of the error in the original event highlighted with >@< to indicate where the error occurred. Here is an example error message: { [ - ] ISODATE : 2020 -05-04 T21 : 21 : 59.001 + 00 : 00 MESSAGE : Error processing log message : < 14 > 1 2020 -05-04 T21 : 21 : 58.117351 + 00 : 00 arcata - pks - cluster -1 pod . log / cf - workloads / logspinner - testing -6446 b8ef - - [ kubernetes @47450 cloudfoundry . org / process_type = \"web\" cloudfoundry . org / rootfs - version = \"v75.0.0\" cloudfoundry . org / version = \"eae53cc3-148d-4395-985c-8fef0606b9e3\" controller - revision - hash = \"logspinner-testing-6446b8ef05-7db777754c\" cloudfoundry . org / app_guid = \"f71634fe-34a4-4f89-adac-3e523f61a401\" cloudfoundry . org / source_type = \"APP\" security . istio . io / tlsMode = \"istio\" statefulset . kubernetes . io / pod - n > @ < ame = \"logspinner-testing-6446b8ef05-0\" cloudfoundry . org / guid = \"f71634fe-34a4-4f89-adac-3e523f61a401\" namespace_name = \"cf-workloads\" object_name = \"logspinner-testing-6446b8ef05-0\" container_name = \"opi\" vm_id = \"vm-e34452a3-771e-4994-666e-bfbc7eb77489\" ] Duration 10.00299412 s TotalSent 10 Rate 0.999701 PID : 33 PRI : < 43 > PROGRAM : syslog - ng } In this example the error can be seen in the snippet statefulset.kubernetes.io/pod-n>@<ame . Looking at the spec for RFC5424, it states that the \u201cSD-NAME\u201d (the left-hand side of the name=value pairs) cannot be longer than 32 printable ASCII characters. In this message, the indicated name exceeds that. Unfortunately, this is a spec violation on the part of the vendor. Ideally the vendor would address this violation so their logs would be RFC-5424 compliant. Alternatively, an exception could be added to the SC4S filter log path (or an alternative (workaround) log path created) for the data source if the vendor can\u2019t/won\u2019t fix the defect. In this example, the reason RAWMSG is not shown in the fields above is because this error message is coming from syslog-ng itself \u2013 not the filter/log path. In messages of the type Error processing log message: where the PROGRAM is shown as syslog-ng , that is the clue your incoming message is not RFC-5424 compliant (though it\u2019s often close, as is the case here).","title":"Dealing with non RFC-5424 compliant sources"},{"location":"troubleshooting/troubleshoot_SC4S_server/#in-byoe-the-metricsinternal-processing-message-are-abusing-the-terminal-how-to-fix-this","text":"In BYOE, when we try to start sc4s service , the terminal is getting abused from the internal and metrics log Example of the issue is Github Terminal abuse issue To rectify this, Please set following property in env_file SC4S_SEND_METRICS_TERMINAL = no Restart SC4S and it will not send any more metrics data to Terminal. NOTE: This symptom will recur if SC4S_DEBUG_CONTAINER is set to \u201cyes\u201d. Do not attempt to use systemd when this variable is set; use the CLI podman or docker commands directly to start/stop SC4S.","title":"In BYOE the metrics/internal processing message are abusing the terminal , how to fix this?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sc4s-dropping-invalid-events","text":"","title":"SC4S dropping invalid events"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sometimes-you-notice-you-are-missing-some-cef-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","text":"To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_CEF=yes Restart SC4S and it will not drop any invalid CEF format.","title":"Sometimes you notice you are missing some CEF logs which are not RFC compliant but logs are important, how to fix this?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-vmware-cb-protect-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","text":"To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_VMWARE_CB_PROTECT=yes Restart SC4S and it will not drop any invalid VMWARE CB-PROTECT format.","title":"If you notice you are missing some VMWARE CB-PROTECT logs which are not RFC compliant but logs are important, how to fix this?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-cisco-ios-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","text":"To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_CISCO=yes Restart SC4S and it will not drop any invalid CISCO IOS format.","title":"If you notice you are missing some CISCO IOS logs which are not RFC compliant but logs are important, how to fix this?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-vmware-vsphere-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","text":"To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_VMWARE_VSPHERE=yes Restart SC4S and it will not drop any invalid VMWARE VSPHERE format.","title":"If you notice you are missing some VMWARE VSPHERE logs which are not RFC compliant but logs are important, how to fix this?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-raw-bsd-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","text":"To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_RAW_BSD=yes Restart SC4S and it will not drop any invalid RAW BSD format.","title":"If you notice you are missing some RAW BSD logs which are not RFC compliant but logs are important, how to fix this?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-raw-xml-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","text":"To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_XML=yes Restart SC4S and it will not drop any invalid RAW XML format.","title":"If you notice you are missing some RAW XML logs which are not RFC compliant but logs are important, how to fix this?"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-hpe-jetdirect-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","text":"To rectify this, Please set following property in env_file SC4S_DISABLE_DROP_INVALID_HPE=yes Restart SC4S and it will not drop any invalid HPE JETDIRECT format. NOTE: Please use only in this case of exception and this is splunk-unsupported feature. Also this setting might impact SC4S performance.","title":"If you notice you are missing some HPE JETDIRECT logs which are not RFC compliant but logs are important, how to fix this?"},{"location":"troubleshooting/troubleshoot_resources/","text":"SC4S Logging and Troubleshooting Resources \u00b6 Helpful Linux and Container Commands \u00b6 Linux service (systemd) commands \u00b6 Check service status systemctl status sc4s Start service systemctl start service Stop service systemctl stop service Restart service systemctl restart service Enabling service at boot systemctl enable sc4s Query the system journal journalctl -b -u sc4s Container Commands \u00b6 NOTE: All container commands below can be run with either runtime ( podman or docker ). Container logs sudo podman logs SC4S Exec into SC4S container podman exec -it SC4S bash Rebuilding SC4S volume podman volume rm splunk - sc4s - var podman volume create splunk - sc4s - var Pull an image or a repository from a registry podman pull splunk:scs:latest Remove unused data podman system prune Load an image from a tar archive or STDIN podman load <tar> Test Commands \u00b6 Checking SC4S port using \u201cnc\u201d. Run this command where SC4S is hosted and check for data in Splunk for success and failure echo '<raw_sample>' |nc <host> <port> Obtaining \u201cOn-the-wire\u201d Raw Events \u00b6 In almost all cases during development or troubleshooting, you will need to obtain samples of the messages exactly as they are received by SC4S. These \u201craw\u201d events contain the full syslog message (including the <PRI> preamble) and differs from those that appear in Splunk after processing by sc4s and/or Splunk. This is the only way to determine if SC4S parsers and filters are operating correctly, as raw messages are needed for \u201cplayback\u201d when testing. In addition, the community supporting SC4S will always first ask for raw samples (kind of like the way Splunk support always asks for \u201cdiags\u201d) before any development or troubleshooting exercise. Here are some options for obtaining raw logs for one or more sourcetypes: Run tcpdump on the collection interface and display the results in ASCII. You will see events of the form < 165 > 1 2007 -02 -15 T09 : 17 : 15.719 Z router1 mgd 3046 UI_DBASE_LOGOUT_EVENT [ junos @2636.1.1.1.2.18 username = \"user\" ] User ' user ' exiting configuration mode buried in the packet contents. Set the variable SC4S_SOURCE_STORE_RAWMSG=yes in env_file and restart sc4s. This will store the raw message in a syslog-ng macro called RAWMSG and will be displayed in Splunk for all fallback messages. For most other sourcetypes, the RAWMSG is not displayed, but can be surfaced by changing the output template to one of the JSON variants (t_JSON_3164 or t_JSON_5424 depending on RFC message type). See SC4S metadata configuration for more details. ** IMPORTANT! Be sure to turn off the RAWMSG variable when you are finished, as it doubles the memory and disk requirements of sc4s. Do not use RAWMSG in production! Lastly, you can enable the alternate destination d_rawmsg for one or more sourcetypes. This destination will write the raw messages to the container directory /var/syslog-ng/archive/rawmsg/<sourcetype> (which is typically mapped locally to /opt/sc4s/archive ). Within this directory, the logs are organized by host and time. This method can be useful when raw samples are needed for events that partially parse (or parse into the wrong sourcetype) and the output template is not JSON (see above). \u201cexec\u201d into the container (advanced) \u00b6 You can confirm how the templating process created the actual syslog-ng config files that are in use by \u201cexec\u2019ing in\u201d to the container and navigating the syslog-ng config filesystem directly. To do this, run /usr/bin/podman exec -it SC4S /bin/bash and navigate to /opt/syslog-ng/etc/ to see the actual config files in use. If you are adept with container operations and syslog-ng itself, you can modify files directly and reload syslog-ng with the command kill -1 1 in the container. You can also run the /entrypoint.sh script by hand (or a subset of it, such as everything but syslog-ng) and have complete control over the templating and underlying syslog-ng process. This is an advanced topic and further help can be obtained via the github issue tracker and Slack channels. Keeping a failed container running (even more advanced) \u00b6 When debugging a configuration syntax issue at startup, it is often helpful to keep the container running after a syslog-ng startup failure. In order to facilitate troubleshooting and make \u201con the fly\u201d syslog-ng configuration changes from within a running container, the container can be forced to remain running when syslog-ng fails to start (which normally terminates the container). This can be enabled by adding SC4S_DEBUG_CONTAINER=yes to the env_file . Use this capability in conjunction with \u201cexec-ing\u201d into the container described above. NOTE: Do not attempt to enable the debug container mode while running out of systemd. Run the container manually from the CLI, as podman or docker commands will be required to start, stop, and optionally clean up cruft left behind by the debug process. Only when SC4S_DEBUG_CONTAINER is set to \u201cno\u201d (or completely unset) should systemd startup processing resume. Fix timezone \u00b6 Mismatch in TZ can occur if SC4S and logHost are not in same TZ. This is commonly occurring problem. To fix it one must create a filter using sc4s-lp-dest-format-d_hec_fmt . Example: #filename: /opt/sc4s/local/config/app_parsers/rewriters/app-dest-rewrite-fix_tz_something.conf block parser app - dest - rewrite - checkpoint_drop - d_fmt_hec_default () { channel { rewrite { fix - time - zone ( \"EST5EDT\" ); }; }; }; application app - dest - rewrite - fix_tz_something - d_fmt_hec_default [ sc4s - lp - dest - format - d_hec_fmt ] { filter { match ( ' checkpoint ' value ( ' fields . sc4s_vendor ' ) type ( string )) <- this has to be customized and match ( ' syslog ' value ( ' fields . sc4s_product ' ) type ( string )) <- this has to be customized and match ( ' Drop ' value ( ' . SDATA . sc4s @2620. action ' ) type ( string )) <- this has to be customized and match ( ' 12. ' value ( ' . SDATA . sc4s @2620. src ' ) type ( string ) flags ( prefix ) ); <- this has to be customized }; parser { app - dest - rewrite - fix_tz_something - d_fmt_hec_default (); }; }; Or create a post-filter if destport, container, proto are not available in indexed fields: #filename: /opt/sc4s/local/config/app_parsers/rewriters/app-dest-rewrite-fix_tz_something.conf block parser app - dest - rewrite - fortinet_fortios - d_fmt_hec_default () { channel { rewrite { fix - time - zone ( \"EST5EDT\" ); }; }; }; application app - dest - rewrite - device - d_fmt_hec_default [ sc4s - postfilter ] { filter { match ( \"xxxx\" , value ( \"fields.sc4s_destport\" ) type ( glob )); <- this has to be customized }; parser { app - dest - rewrite - fortinet_fortios - d_fmt_hec_default (); }; }; Note that filter match statement should be aligned to your data! Parser accepts timezone in formats: \u201cAmerica/New York\u201d or \u201cEST5EDT\u201d (though not short form like \u201cEST\u201d only). Cyberark logs known issue \u00b6 When the data is received on the indexers all the events are merged together into one. Please check the below link for configuration on cyberark side https://cyberark-customers.force.com/s/article/00004289 SC4S events dropping issue when another interface used to receive logs \u00b6 When second / another interface used to receive syslog traffic, RPF (Reverse Path Forwarding filtering) in RHEL (configured as default configuration) was dropping the events. Need to add static route for source device to point back dedicated syslog interface. Reference: https://access.redhat.com/solutions/53031","title":"SC4S Logging and Troubleshooting Resources"},{"location":"troubleshooting/troubleshoot_resources/#sc4s-logging-and-troubleshooting-resources","text":"","title":"SC4S Logging and Troubleshooting Resources"},{"location":"troubleshooting/troubleshoot_resources/#helpful-linux-and-container-commands","text":"","title":"Helpful Linux and Container Commands"},{"location":"troubleshooting/troubleshoot_resources/#linux-service-systemd-commands","text":"Check service status systemctl status sc4s Start service systemctl start service Stop service systemctl stop service Restart service systemctl restart service Enabling service at boot systemctl enable sc4s Query the system journal journalctl -b -u sc4s","title":"Linux service (systemd) commands"},{"location":"troubleshooting/troubleshoot_resources/#container-commands","text":"NOTE: All container commands below can be run with either runtime ( podman or docker ). Container logs sudo podman logs SC4S Exec into SC4S container podman exec -it SC4S bash Rebuilding SC4S volume podman volume rm splunk - sc4s - var podman volume create splunk - sc4s - var Pull an image or a repository from a registry podman pull splunk:scs:latest Remove unused data podman system prune Load an image from a tar archive or STDIN podman load <tar>","title":"Container Commands"},{"location":"troubleshooting/troubleshoot_resources/#test-commands","text":"Checking SC4S port using \u201cnc\u201d. Run this command where SC4S is hosted and check for data in Splunk for success and failure echo '<raw_sample>' |nc <host> <port>","title":"Test Commands"},{"location":"troubleshooting/troubleshoot_resources/#obtaining-on-the-wire-raw-events","text":"In almost all cases during development or troubleshooting, you will need to obtain samples of the messages exactly as they are received by SC4S. These \u201craw\u201d events contain the full syslog message (including the <PRI> preamble) and differs from those that appear in Splunk after processing by sc4s and/or Splunk. This is the only way to determine if SC4S parsers and filters are operating correctly, as raw messages are needed for \u201cplayback\u201d when testing. In addition, the community supporting SC4S will always first ask for raw samples (kind of like the way Splunk support always asks for \u201cdiags\u201d) before any development or troubleshooting exercise. Here are some options for obtaining raw logs for one or more sourcetypes: Run tcpdump on the collection interface and display the results in ASCII. You will see events of the form < 165 > 1 2007 -02 -15 T09 : 17 : 15.719 Z router1 mgd 3046 UI_DBASE_LOGOUT_EVENT [ junos @2636.1.1.1.2.18 username = \"user\" ] User ' user ' exiting configuration mode buried in the packet contents. Set the variable SC4S_SOURCE_STORE_RAWMSG=yes in env_file and restart sc4s. This will store the raw message in a syslog-ng macro called RAWMSG and will be displayed in Splunk for all fallback messages. For most other sourcetypes, the RAWMSG is not displayed, but can be surfaced by changing the output template to one of the JSON variants (t_JSON_3164 or t_JSON_5424 depending on RFC message type). See SC4S metadata configuration for more details. ** IMPORTANT! Be sure to turn off the RAWMSG variable when you are finished, as it doubles the memory and disk requirements of sc4s. Do not use RAWMSG in production! Lastly, you can enable the alternate destination d_rawmsg for one or more sourcetypes. This destination will write the raw messages to the container directory /var/syslog-ng/archive/rawmsg/<sourcetype> (which is typically mapped locally to /opt/sc4s/archive ). Within this directory, the logs are organized by host and time. This method can be useful when raw samples are needed for events that partially parse (or parse into the wrong sourcetype) and the output template is not JSON (see above).","title":"Obtaining \"On-the-wire\" Raw Events"},{"location":"troubleshooting/troubleshoot_resources/#exec-into-the-container-advanced","text":"You can confirm how the templating process created the actual syslog-ng config files that are in use by \u201cexec\u2019ing in\u201d to the container and navigating the syslog-ng config filesystem directly. To do this, run /usr/bin/podman exec -it SC4S /bin/bash and navigate to /opt/syslog-ng/etc/ to see the actual config files in use. If you are adept with container operations and syslog-ng itself, you can modify files directly and reload syslog-ng with the command kill -1 1 in the container. You can also run the /entrypoint.sh script by hand (or a subset of it, such as everything but syslog-ng) and have complete control over the templating and underlying syslog-ng process. This is an advanced topic and further help can be obtained via the github issue tracker and Slack channels.","title":"\"exec\" into the container (advanced)"},{"location":"troubleshooting/troubleshoot_resources/#keeping-a-failed-container-running-even-more-advanced","text":"When debugging a configuration syntax issue at startup, it is often helpful to keep the container running after a syslog-ng startup failure. In order to facilitate troubleshooting and make \u201con the fly\u201d syslog-ng configuration changes from within a running container, the container can be forced to remain running when syslog-ng fails to start (which normally terminates the container). This can be enabled by adding SC4S_DEBUG_CONTAINER=yes to the env_file . Use this capability in conjunction with \u201cexec-ing\u201d into the container described above. NOTE: Do not attempt to enable the debug container mode while running out of systemd. Run the container manually from the CLI, as podman or docker commands will be required to start, stop, and optionally clean up cruft left behind by the debug process. Only when SC4S_DEBUG_CONTAINER is set to \u201cno\u201d (or completely unset) should systemd startup processing resume.","title":"Keeping a failed container running (even more advanced)"},{"location":"troubleshooting/troubleshoot_resources/#fix-timezone","text":"Mismatch in TZ can occur if SC4S and logHost are not in same TZ. This is commonly occurring problem. To fix it one must create a filter using sc4s-lp-dest-format-d_hec_fmt . Example: #filename: /opt/sc4s/local/config/app_parsers/rewriters/app-dest-rewrite-fix_tz_something.conf block parser app - dest - rewrite - checkpoint_drop - d_fmt_hec_default () { channel { rewrite { fix - time - zone ( \"EST5EDT\" ); }; }; }; application app - dest - rewrite - fix_tz_something - d_fmt_hec_default [ sc4s - lp - dest - format - d_hec_fmt ] { filter { match ( ' checkpoint ' value ( ' fields . sc4s_vendor ' ) type ( string )) <- this has to be customized and match ( ' syslog ' value ( ' fields . sc4s_product ' ) type ( string )) <- this has to be customized and match ( ' Drop ' value ( ' . SDATA . sc4s @2620. action ' ) type ( string )) <- this has to be customized and match ( ' 12. ' value ( ' . SDATA . sc4s @2620. src ' ) type ( string ) flags ( prefix ) ); <- this has to be customized }; parser { app - dest - rewrite - fix_tz_something - d_fmt_hec_default (); }; }; Or create a post-filter if destport, container, proto are not available in indexed fields: #filename: /opt/sc4s/local/config/app_parsers/rewriters/app-dest-rewrite-fix_tz_something.conf block parser app - dest - rewrite - fortinet_fortios - d_fmt_hec_default () { channel { rewrite { fix - time - zone ( \"EST5EDT\" ); }; }; }; application app - dest - rewrite - device - d_fmt_hec_default [ sc4s - postfilter ] { filter { match ( \"xxxx\" , value ( \"fields.sc4s_destport\" ) type ( glob )); <- this has to be customized }; parser { app - dest - rewrite - fortinet_fortios - d_fmt_hec_default (); }; }; Note that filter match statement should be aligned to your data! Parser accepts timezone in formats: \u201cAmerica/New York\u201d or \u201cEST5EDT\u201d (though not short form like \u201cEST\u201d only).","title":"Fix timezone"},{"location":"troubleshooting/troubleshoot_resources/#cyberark-logs-known-issue","text":"When the data is received on the indexers all the events are merged together into one. Please check the below link for configuration on cyberark side https://cyberark-customers.force.com/s/article/00004289","title":"Cyberark logs known issue"},{"location":"troubleshooting/troubleshoot_resources/#sc4s-events-dropping-issue-when-another-interface-used-to-receive-logs","text":"When second / another interface used to receive syslog traffic, RPF (Reverse Path Forwarding filtering) in RHEL (configured as default configuration) was dropping the events. Need to add static route for source device to point back dedicated syslog interface. Reference: https://access.redhat.com/solutions/53031","title":"SC4S events dropping issue when another interface used to receive logs"}]}